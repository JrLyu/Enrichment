\include{preamble}

\title{Emory University\\\textbf{MATH 347 Non Linear Optimization}\\ Learning Notes}
\author{Jiuru Lyu}
\date{\today}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Math Preliminaries}
\subsection{Introduction to Optimization}
\begin{df}{Optimization Problem}
	The main optimization problem can be stated as follows \begin{equation}\label{eq1}\min_{x\in S}f(x),\end{equation} where 
	\begin{itemize}
		\item $x$ is the \textit{optimization variable},
		\item $S$ is the \textit{feasible set}, and 
		\item $f$ is the \textit{objective function}.
	\end{itemize}
\end{df}
\begin{rmk}
	$\dsst\max_{x\in S}f(x)=-\min_{x\in S}-f(x)$. Hence, we will only study minimization problems.
\end{rmk}
\begin{thm}{Solving an Optimization Problem}
	\begin{itemize}
		\item Theoretical Analysis: analytic solution
		\item Numerical solution/optimization
	\end{itemize}	
\end{thm}
\begin{df}{Solution Methods depend on the type of $x$, $S$, and $f$}
	\begin{itemize}
		\item When $x$ is continuous (e.g., $\R$, $\R^n$, $\R^{m\times n}$, $\dots$), then the optimization problem stated in Eq. (\ref{eq1}) is a \textit{continuous optimization problem}. $_\textit{It will also be the focus of this class.}$ \par Opposite to continuous optimization problems, we have \textit{discrete optimization problem} if $x$ is discrete. \par If $x$ has both types of components, then we call the problem \textit{mixed}.
		\item Depending on $S$, we can have 
		\begin{itemize}
			\item \textit{Unconstrained problems}: where $S=\R^n$, $S=\R^{m\times n}$, $\dots$ ($m,n$ are fixed).
			\item \textit{Constrained problems}: where $S\subsetneq\R^n$, $S\subsetneq\R^{m\times n}$, $\dots$. \par $_\textit{Both types of problems will be studied.}$
		\end{itemize}
		\item Depending on $f$, we have 
		\begin{itemize}
			\item \textit{Smooth optimization problems}: $f$ has first and/or second order derivatives. \par $_\textit{Only smooth optimization problems will be studied.}$
			\item \textit{Non-smooth optimization problems}: $f$ is not differentiable. 
		\end{itemize}
	\end{itemize}	
\end{df}

\begin{df}{Linear Optimization/Program}
	If $f$ is linear and $S$ consists of linear constrains, then the optimization problem is called a \textit{linear problem/program}.	
\end{df}
\begin{eg}{Classification of Optimization Problems}
	\begin{enumerate}
		\item Consider the following problem \[\min_{x_1,x_2,x_3}x_1^2-4x_1x_2+3x_2x_3+\sin{x_3}\]
		\begin{sol}
			\begin{itemize}
				\item Optimization variable: $x=(x_1,x_2,x_3)\in\R^3$. $\longrightarrow$ continuous.
				\item Feasible set: $S=\R^3$. $\longrightarrow$ unconstrained.
				\item Objective function: $f(x_1,x_2,x_3)=x_1^2-4x_1x_2+3x_2x_3+\sin{x_3}$. $\longrightarrow$ smooth but non-linear. 
			\end{itemize}	
		\end{sol}
		\item Consider the following problem \[\max_{\substack{4x_1+7x_2+3x_3\leq1 \\ x_1,x_2,x_3\geq0}}x_1+2x_2+3x_3\]
		\begin{sol}
			\begin{itemize}
				\item Optimization variable: $x=(x_1,x_2,x_3)\in\R^3$. $\longrightarrow$ continuous.
				\item Feasible set: $S=\qty{(x_1,x_2,x_3):x_1,x_2,x_3\geq0, 4x_1+7x_2+3x_3\leq 1}\subsetneq\R^3$. $\qquad\longrightarrow$ constrained.
				\item Objective function: $f(x_1,x_2,x_3)=x_1+2x_2+3x_3$. $\longrightarrow$ smooth and linear. 
			\end{itemize}	
		\end{sol}
		\begin{rmk}
			This problem can be considered as the budget constrained optimization problem in Economics. 	
		\end{rmk}

		\item Consider the following problem \[\min_{x_1,x_2\geq0}4x_1-3\qty|x_2|+\sin(x_1^2-2x_2)\]
		\begin{sol}
			\begin{itemize}
				\item Optimization variable: $x=(x_1,x_2)\in\R^2$. $\longrightarrow$ continuous. 
				\item Feasible set: $S=\qty{(x_1,x_2):x_1,x_2\geq0}\subsetneq\R62$. $\longrightarrow$ constrained.
				\item Objective function: $f(x_1,x_2)=4x_1-3\qty|x_2|+\sin(x_1^2-2x_2)$. $\longrightarrow$ non-smooth and non-linear.
			\end{itemize}	
		\end{sol}
		\begin{rmk}
			In this particular problem, $x_2\geq0$, and so $f(x_1,x_2)=4x_1-3x_2+\sin\qty(x_1^2-2x_2)$ on the feasible set. Hence, this problem can be equivalently written as \[\min_{x_1,x_2\geq0}4x_1-3x_2+\sin\qty(x_1^2-2x_2),\] which is a smooth optimization problem. 
		\end{rmk}
	\end{enumerate}
\end{eg}

\subsection{Linear Algebra Review}
\begin{eg}{Why linear algebra for optimization?}
	Consider $\dsst\min_{x\in\R}f(x)$, where $f(x)=c+bx+ax^2,\ a,b,c\in\R$. 
	\begin{itemize}
		\item $a>0$: $x^*=-\dfrac{b}{2a}$ is a global minimum and $f(x^*)=c-\dfrac{b^2}{4a}$.
		\item $a<0$: no minimum exists.
		\item $a=0$: $f(x)=c+bx$.
		\begin{itemize}
			\item $b\neq0$: no minimum exists.
			\item $b=0$: $f(x)=c$, and every $x$ is a minimum point.
		\end{itemize}
	\end{itemize}
	We can approximate any smoothing function using Taylor's approximation and make them simple into the case discussed above. 
\end{eg}
\begin{thm}{Taylor's Approximation}
	\[f(x)=\underbrace{f(x_0)+f'(x_0)(x-x_0)+\dfrac{f''(x_0)}{2}(x-x_0)^2}_{q(x)}+\underbrace{\epsilon(x-x_0)(x-x_0)^2}_\text{error},\] where $\dsst\lim_{x\to x_0}\epsilon(x-x_0).$
\end{thm}
\begin{rmk}
	The hope is that the quadratic approximation will inform us on the behavior of $f$ near $x_0$	and be useful for instance in referring $x_0$ on the subject of optimality. 
\end{rmk}
\begin{df}{Quadratic Approximation in Higher Dimensions}
	When $d>1$, we consider $\dsst\min_{x\in\R^d}f(x)$. Then, the \textit{quadratic approximation} of $f$ is defined as \[q(x)\coloneqq c+\langle b,x\rangle+\langle x,Ax\rangle,\] where $c\in\R$, $b\in\R^d$, $A\in\R^{d\times d}$.
\end{df}
\begin{rmk}
	Then, to know if a minimum exists, we need information on the matrix $A$ and the vector $b$.	
\end{rmk}
\begin{df}{Vector, $\R^d$}
	We define a \textit{vector} in $\R^d$ as a column vector. \[x=\mqty(x_1\\\vdots\\x_d)\in\R^d,\ x_i\in\R.\] On $\R^d$, we also have the following operations defined
	\begin{itemize}
		\item Addition: \[\mqty(x_1\\\vdots\\x_d)+\mqty(y_1\\\vdots\\y_d)=\mqty(x_1+y_1\\\vdots\\x_d+y_d),\ x_i,y_i\in\R\]
		\item Scalar multiplication: \[\alpha\mqty(x_1\\\vdots\\x_d)=\mqty(\alpha x_1\\\vdots\\\alpha x_d),\alpha,x_i\in\R\]
	\end{itemize}
\end{df}
\begin{df}{Basis of $\R^d$}
	A collection of vectors $v_1\dots,v_d\in\R^d$ is a \textit{basis} in $\R^d$ if $\forall\ x\in\R^d$, $\exists!\ \alpha_1,\dots,\alpha_d\in\R\st x=\alpha_1v_1+\cdots+\alpha_dv_d$.	
\end{df}
\begin{eg}{The Standard Basis}
	The \textit{standard basis} is defines as \[e_i=\mqty(0\\\vdots\\1\\\vdots\\0),\]	 where $1$ is at the $i$-th position for $1\leq i\leq d$. Note that $\forall x\in\R^d,\ x=x_1e_1+\cdots+x_de_d$.
\end{eg}
\begin{nota}
	\[0_d=\mqty(0\\\vdots\\0).\]	
\end{nota}
\begin{df}{Inner Product}
	$\inprod{\cdot}{\cdot}:\R^d\times\R^d\to\R$ is an \textit{inner product} if
	\begin{itemize}
		\item (symmetry) $\langle x,y\rangle=\langle y,x\rangle\quad\forall x,y\in\R^d$
		\item (additivity) $\langle x,y+z\rangle=\langle x,y\rangle+\langle x,z\rangle\quad\forall x,y,z\in\R^d$
		\item (homogeneity) $\langle\lambda x,y\rangle=\lambda\langle x,y\rangle\quad\forall x,y\in\R^d,\ \lambda\in\R$
		\item (positive definiteness) $\langle x,x\rangle\geq\quad\forall x\in\R^d$ and $\langle x,x\rangle=0\iff x=0$
	\end{itemize}
\end{df}
\begin{eg}{Examples of Inner Products}
	\begin{enumerate}
		\item \begin{df}{Dot Product} The \textit{dot product} of $x,y\in\R^d$ is defined as \[\langle x,y\rangle=x_1y_1+\cdots+x_dy_d=\sum_{i=1}^dx_iy_i\quad\forall x,y\in\R^d.\] It is also referred as the \textit{standard inner product}, and we often use the notation $x\cdot y$ to denote it. \end{df}
		\item \begin{df}{Weighted Dot Product} The \textit{weighted dot product} of $x,y\in\R^d$ with some weight $w$ is defined as \[\langle x,y\rangle_w=\sum_{i=1}^dw_ix_iy_i,\] where $w_1,\dots,w_d>0$ are called \textit{weights}.\end{df}
	\end{enumerate}	
	\begin{rmk}
		When $d=2$, then $\langle x,y\rangle=\abs{x}\abs{y}\cos\angle(x,y)$. Dot product measure how correlated are two vectors (with respect to their directions). 	
	\end{rmk}
\end{eg}
\begin{df}{Vector Norm}
	$\norm{\cdot}:\R^d\to\R$ is a \textit{norm} if
	\begin{itemize}
		\item (non-negativity) $\norm{x}\geq0\quad\forall x\in\R^d$ and $\norm{x}=0\iff x=0$
		\item (positive homogeneity) $\norm{\lambda x}=\abs{\lambda}\norm{x}\quad\forall\lambda\in\R,\ x\in\R^d$
		\item (triangular inequality) $\norm{x+y}\leq\norm{x}+\norm{y}\quad\forall x,y\in\R^d$.
	\end{itemize}
\end{df}
\begin{rmk}
	Vector norm introduces the notion of length of vectors in $\R^d$.	
\end{rmk}
\begin{eg}{Examples of Vector Norms}
	\begin{itemize}
		\item If $\inprod{\cdot}{\cdot}$ is an inner product on $\R^d$, then \[\norm{x}=\sqrt{\inprod{x}{x}}\quad\forall x\in\R^d\] is a norm. For instance, \[\norm{x}_2=\sqrt{x\cdot x}=\qty(\sum_{i=1}^dx_i^2)^{\frac{1}{2}}.\] This norm is called the \textit{standard (Euclidean)} or $\l_2$ norm in $\R^d$.
		\item \begin{df}{$\l_p$ Norms} Suppose $p\geq1$, then \[\norm{x}_p\coloneqq\qty(\sum_{i=1}^dx_i^p)^{\frac{1}{p}}.\]\end{df}
		\item \begin{df}{$\infty$-Norms}\[\norm{x}_\infty\coloneqq\max_{1\leq i\leq d}\abs{x_i}\quad\forall x\in\R^d.\]\end{df}
	\end{itemize}
	\begin{rmk}
		$\dsst\lim_{p\to\infty}\norm{x}_p=\norm{x}_{\infty}$.
	\end{rmk}
\end{eg}
\begin{thm}{Cauchy-Schwarz Inequality}
	Assume that $\inprod{\cdot}{\cdot}:\R^d\times\R^d\to\R$ is an inner product, then \[\qty|\inprod{x}{y}|^2\leq\inprod{x}{x}\cdot\inprod{y}{y}\quad\forall x,y\in\R^d.\] In particular, if $\|x\|=\sqrt{\inprod{x}{x}}$, then \[\qty|\inprod{x}{y}|\leq\|x\|\cdot\|y\|\quad\forall x,y\in\R^d.\] For the standard inner product, we have \[\qty|\sum_{i=1}^nx_iy_i|\leq\|x\|_2\cdot\|y\|_2\quad\forall x,y\in\R^d.\] The equality holds when $x$ and $y$ are linearly dependent.
\end{thm}
\begin{df}{Matrix}
	Let $d,m\in\N$. We say that $A\in\R^{d\times m}$ is a \textit{$d\times m$ matrix} if \[A=\mqty(a_{11}&a_{12}&\cdots&a_{1m}\\a_{21}&a_{22}&\cdots&a_{2m}\\\vdots&\vdots&\ddots&\vdots\\a_{d1}&a_{d2}&\cdots&a_{dm})=\mqty(a_{ij})_{i=1,j=1}^{d,m}\]
\end{df}
\begin{df}{Operations with Matrices}
	\begin{itemize}
		\item Let $A,B\in\R^{d\times m}$, then $\mqty(A+B)_{i,j}=a_{ij}+b_{ij}\quad\forall i,j$.
		\item Let $A\in\R^{d\times m}$ and $\alpha\in\R$, then $\mqty(\alpha A)_{ij}=\alpha a_{ij}\quad\forall i,j$.
		\item Let $A\in\R^{d\times m}$ and $B\in\R^{m,n}$, then $AB\in\R^{d\times n}$, and $\dsst\mqty(AB)_{ij}=\sum_{k=1}^ma_{ik}b_{kj}\quad\forall i,j$.
	\end{itemize}	
\end{df}
\begin{rmk}
	Matrix multiplication is not commutative. In fact, if $A\in\R^{d\times m}$ and $B\in\R^{m\times n}$, then $BA$ is defined if and only if $n=d$. In that case, $AB\in\R^{d\times d}$ and $BA\in\R^{m\times m}$, and so if $m\neq d$, $AB$ and $BA$ have different sizes. Finally, even if $m=d=n$, $AB\neq BA$ in general. 	
\end{rmk}
\begin{df}{Linear Transformation}
	The mapping $\L:\R^m\to\R^d$ is called \textit{linear} if $\L(\alpha x_1+\beta x_2)=\alpha\L(x_1)+\beta\L(x_2)$.	
\end{df}
\begin{thm}{Matrices and Linear Transformation}
	$\forall A\in\R^{d\times m}$, $\L_A(x)=Ax$ is a linear mapping from $\R^m$ to $\R^d$. Moreover, $\forall\L:\R^m\to\R^d$ linear, $\exists!A\in\R^{d\times m}\st\L=\L_A$.
\end{thm}
\begin{prf}
	Here, we offer an intuition on why this is true. Suppose $A\in\R^{d\times m}$ and $x\in\R^{m}\st$ \[A=\mqty(a_{11}&\cdots&a_{1m}\\\vdots&\ddots&\vdots\\a_{d1}&\cdots&a_{dm})\quad\text{and}\quad x\in\mqty(x_1\\\vdots\\x_m)\in\R^{m\times1}.\]	 Then, $Ax\in\R^{d\times1}$ is the following \[Ax=\mqty(a_{11}&\cdots&a_{1m}\\\vdots&\ddots&\vdots\\a_{d1}&\cdots&a_{dm})\mqty(x_1\\\vdots\\x_m)=\mqty(a_{11}x_1+\cdots+a_{1m}x_m\\\vdots\\a_{d1}x_1+\cdots+a_{dm}x_m)\in\R^{d\times1}.\] So, if $\L_A(x)=Ax$ for $x\in\R^m$, then $\L_A:\R^m\to\R^d$ is linear. 
\end{prf}
\begin{thm}{Matrix Multiplication as Composite Linear Transformations}
	Suppose $\L_A:\R^m\to\R^d$ and $\L_B:\R^n\to\R^m$, where $A\in\R^{d\times m}$ and $B\in\R^{m\times n}$. Define $\L(x)=\L_A\circ\L_B(x)=\L_A(\L_B(x))\quad\forall x\in\R^n$	. Then, $\L:\R^n\to\R^d$. Since $\L_A$ and $\L_B$ are linear, we found that $\L$ is also linear. Hence, $\L=\L_C\fs C\in\R^{d\times n}$. It turns out that $C=AB$.
\end{thm}
\begin{df}{Transpose of Matrix}
	Let $A\in\R^{d\times m}$, then its transpose $A^T\in\R^{m\times d}$, and \[\mqty(A^T)_{ij}=a_{ji}.\]	
\end{df}
\begin{cor}{}
	If $x,y\in\R^d$, then $\inprod{x}{y}=\dsst\sum_{i=1}^dx_iy_i=x^Ty=xy^T$.
\end{cor}
\begin{prf}
	Suppose $x=\mqty(x_1\\\vdots\\x_d)$, then $x^T=\mqty(x_1&\cdots&x_d)$.\[x^Ty=\mqty(x_1&\cdots&x_d)\mqty(y_1\\\vdots\\y_d)=x_1y_1+\cdots+x_dy_d.\]	
\end{prf}
\begin{cor}{Cauchy-Schwarz}
	$\qty|x^Ty|\leq\|x\|_2\|y\|_2$.	
\end{cor}
\begin{df}{Trace of a Matrix}
	Assume that $A\in\R^{d\times d}$, the \textit{trace} of $A$, denoted as $\Tr(A)$, is defined as \[\Tr(A)=\sum_{i=1}^da_{ii}.\]	
\end{df}
\begin{df}{Determinant of a Matrix}
	Assume that $A\in\R^{d\times d}$, the \textit{determinant} of $A$, denoted as $\det(A)$, is defined as \[\det(A)=\sum_{\sigma\in S_d}(-1)^{i(\sigma)}a_{1\sigma(1)}a_{2\sigma(2)}\cdots a_{d\sigma(d)},\] where $S_d$ is the set of all possible permutation of size $d$ and $i(\sigma)$ denotes the sign of the permutation. 
\end{df}
\begin{df}{Eigenvalue and Eigenvector}
	Assume that	$A\in\R^{d\times d}$. We say that $\lambda$ is an \textit{eigenvalue} for $A$ if $\exists x\in\R^d\backslash\qty{0}\st Ax=\lambda x$. In this case, $x$ is called an \textit{eigenvector}.
\end{df}
\begin{df}{Diagonalizability}
	A matrix $A\in\R^{d\times d}$ is called \textit{diagonalizable} if $\exists$ basis $v_1,\dots,v_d\st Av_i=\lambda v_i\quad\forall1\leq i\leq d$. 
\end{df}
\begin{thm}{Diagonalization, Singular Value Decomposition (SVD) of Squared Matrices}
	Assume that $A$ is diagonalizable and \[V=\mqty(v_1&v_2&\cdots&v_d).\] Then, $A=VDV^{-1}$, where $D$ is a diagonal matrix such that \[D=\mqty(\lambda_1&&0\\&\ddots&\\0&&\lambda_d).\]
\end{thm}
\begin{eg}{Application of Diagonalization}
	\[A^2=\qty(VDV^{-1})\qty(VDV^{-1})=VD\underbrace{V^{-1}V}_IDV^{-1}=VD^2V^{-1}.\] Generally, \[A^n=VD^nV^{-1}=V\mqty(\lambda_1^n&&0\\&\ddots&\\0&&v_d^n)V^{-1}.\]
\end{eg}
\begin{rmk} Remarks on Diagonalization
	\begin{itemize}
		\item There might be repeating eigenvalues. Typically, we enumerate $\lambda$'s $\st\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_d$.
		\item In general, it is hard to decide whether $A$ is diagonalizable. \emph{For example, rotation matrices have no eigenvectors nor eigenvalues.}
		\item If $A$ is symmetric; that is $A=A^T$, then $A$ is diagonalizable. Moreover, we can choose basis $v_1,\dots,v_d \st$ \[v_i^Tv_j=\begin{cases}0,\quad i\neq j\\1,\quad i=j\end{cases}.\] Such bases are called \emph{orthonormal}. In matrix form, if $V=\mqty(v_1&v_2&\cdots&v_d)$, then \[V^TV=\mqty(v_1^T\\\vdots\\v_d^T)\mqty(v_1&\cdots&v_d)=I.\] That is, $V^T=V^{-1}$, and hence $A=VDV^{-1}=VDV^{T}$.
	\end{itemize}	
\end{rmk}



\newpage
\section{Unconstrained Optimization}

\newpage
\section{Least Square}

\newpage
\section{Constrained Optimization}

\end{document}
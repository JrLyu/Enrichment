\include{preamble}

\title{Emory University\\\textbf{MATH 362 Mathematical Statistics II}\\ Learning Notes}
\author{Jiuru Lyu}
\date{\today}

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Estimation}
\subsection{Introduction}
\begin{df}{Model}
	A \textit{model} is a distribution with certain parameters.	
\end{df}
\begin{eg}
	The normal distribution: $N(\mu,\sigma^2)$.	
\end{eg}
\begin{df}{Population}
	The \textit{population}	is all the objects in the experiment.
\end{df}
\begin{df}{Data, Sample, and Random Sample}
	\textit{Data} refers to observed value from sample. The \textit{sample}	is a subset of the population. A \textit{random sample} is a sequence of independent, identical ($\iid$) random variables.
\end{df}
\begin{df}{Statistics}
	\textit{Statistics} refers to a function of the random sample.
\end{df}
\begin{eg}
	The sample mean is a function of the sample: \[\bar{Y}=\dfrac{1}{n}\qty(Y_1+\cdots+Y_n).\]	
\end{eg}
\begin{eg} 
	Central Limit Theorem\par 
	We randomly toss $n=200$ fair coins on the table. Calculate, using the central limit theorem, the probability that at least $110$ coins have turned on the same side. 
	\[\bar{X}=\dfrac{X_1+\cdots+X_{200}}{200}\quad\stackrel{\text{CLT}}{\sim}\quad N\qty(\mu,\sigma^2),\] where \[\mu=\E\qty(\bar{X})=\dfrac{\dsst\sum_{i=1}^{200}\E\qty(X_i)}{200},\] \[\sigma^2=\Var\qty(\bar{X})=\Var\qty(\dfrac{X_1+\cdots+X_{200}}{200})=\dfrac{\dsst\sum_{i=1}^{200}\Var\qty(X_i)}{200^2}.\]
\end{eg}
\begin{df}{Statistical Inference}
	The process of \textit{statistical inference} is defined to be the process of using data from a sample to gain information about the population.	
\end{df}
\begin{eg}
	Goals in statistical inference
	\begin{enumerate}
		\item \begin{df}{Estimation} To obtain values of the parameters from the data. \end{df}
		\item \begin{df}{Hypothesis Testing} To test a conjecture about the parameters. \end{df}
		\item \begin{df}{Goodness of Fit} How well does the data fit a given distribution. \end{df}
		\item Linear Regression
	\end{enumerate}	
\end{eg}


\subsection{The Method of Maximum Likelihood and the Method of Moments}
\begin{eg}
	Given an unfair coin, or $p$-coin, such that \[X=\begin{cases}1\quad\text{head with probability }p,\\0\quad\text{tail with probability }1-p.\end{cases}\]	How can we determine the value $p$?
	\begin{sol}
		\begin{enumerate}
			\item Try to flip the coin several times, say, three times. Suppose we get HHT. 
			\item Draw a conclusion from the experiment. 
		\end{enumerate}	
		\textbf{Key idea: The choice of the parameter $p$ should be the value that maximizes the probability of the sample.}
		\[\P\qty(X_1=1,X_2=1,X_3=0)=\P\qty(X_1=1)\P(X_2=1)\P(X_3=0)=p^2(1-p)\coloneqq f(p).\] Solving the optimization problem $\dsst\max_{p>0}f(p)$, we find it is most likely that $p=\dfrac{2}{3}$. This method is called the \textit{likelihood maximization method}.
	\end{sol}
\end{eg}
\begin{df}{Likelihood Function}
	For a random sample of size $n$ from the discrete (or continuous) pdf $p_X(k;\theta)$ (or $f_Y(y;\theta)$), the \textit{likelihood function}, $\L(\theta)$, is the product of the pdf evaluated at $X_i=k_i$ (or $Y_i=y_i$). That is, \[\L(\theta)\coloneqq\prod_{i=1}^np_X\qty(k_i;\theta)\quad\text{or}\quad \L(\theta)\coloneqq\prod_{i=1}^nf_Y\qty(y_i;\theta).\]	
\end{df}
\begin{df}{Maximum Likelihood Estimate}
	Let $\L(\theta)$ be as defined in Definition 1.2.2. If $\theta_e$ is a value of the parameter such that $\L\qty(\theta_e)\geq \L(\theta)$ for all possible values of $\theta$, then we call $\theta_e$ the \textit{maximum likelihood estimate} for $\theta$.
\end{df}
\begin{thm}{The Method of Maximum Likelihood}
	Given random samples $X_1,\dots,X_N$ and a density function $p_X(x)$ (or $f_X(x)$), then we have the likelihood function defined as \begin{align*}\L(\theta)=p_X(X;\theta)&=\P(X_1,X_2,\dots,X_N)\\&=\P(X_1)\P(X_2)\cdots\P(X_N)&[independent]\\&=\prod_{i=1}^Np_X(X_i;\theta)&[identical]\end{align*} Then, the maximum likelihood estimate for $\theta$ is given by \[\theta^*=\argmax_\theta L(\theta),\] where \[\L\qty(\argmax_\theta L(\theta))=\L^*(\theta)=\max_\theta \L(\theta).\]
\end{thm}
\begin{eg}
	Consider the Poisson distribution $X=0,1,\dots,$ with $\lambda>0$. Then, the pdf is given by \[p_X(k,\lambda)=e^{-\lambda}\dfrac{\lambda^k}{k!},\quad k=0,1,\dots\] Given data $k_1,\dots,k_n$, we have the likelihood function \[\L(\lambda)=\prod_{i=1}^np_X\qty(X=k;\lambda)=\prod_{i=1}^ne^{-\lambda}\dfrac{\lambda^{k_i}}{k_i!}=e^{-n\lambda}\dfrac{\lambda^{\sum{k_i}}}{k_1!\cdots k_n!}\] Then, to find the maximum likelihood estimate of $\lambda$, we need to $\dsst\max_{\lambda}\L(\lambda)$. That is to solve $\dsst\pdv{\L(\lambda)}{\lambda}=0$ and $\dsst\pdv[2]{\L(\lambda)}{\lambda}<0$.
\end{eg}
\begin{eg}
	Waiting Time. \par 
	Consider the exponential distribution $f_Y(y)=\lambda e^{-\lambda y}$ for $y\geq 0$. Find the MLE $\lambda_e$ of $\lambda$.
	\begin{sol}
		The likelihood function of the exponential distribution is given by
		\[\L(\lambda)=\prod_{i=1}^n\lambda e^{-\lambda y_i}=\lambda^n\exp\qty(-\lambda\sum_{i=1}^ny_i).\] Now, define \[\l(\lambda)=\ln\L(\lambda)=n\ln\lambda-\lambda\sum_{i=1}^ny_i.\] To optimize $\l(\lambda)$, we compute \[\dv{\lambda}\l(\lambda)=\dfrac{n}{\lambda}-\sum_{i=1}^ny_i\stackrel{set}{=}0\] So, \[\dfrac{n}{\lambda}=\sum_{i=1}^ny_i\implies\lambda_e=\dfrac{n}{\dsst\sum_{i=1}^ny_i}\eqqcolon\dfrac{1}{\bar{y}},\] where $\bar{y}$ is the sample mean. 
	\end{sol}
\end{eg}
\begin{eg}
	Given the exponential distribution $f_Y(y)=\lambda e^{-\lambda y}$ for $y\geq0$. Find the MLE of $\lambda^2$.
	\begin{sol}
		Define $\tau=\lambda^2$. Then, $\lambda=\sqrt{\tau}$, and so \[f_Y(y)=\sqrt{\tau}e^{-\sqrt{\tau}y},\quad y\geq0.\] Then, the likelihood function becomes \[\L(\tau)=\prod_{i=1}^nf_Y(y)=\tau^{\frac{n}{2}}\exp\qty(-\sqrt{\tau}\sum_{i=1}^ny_i).\] Similarly, after maximization, we find \[\tau_e=\dfrac{1}{\qty(\bar{y})^2}.\]
	\end{sol}
\end{eg}
\begin{thm}{Invariant Property for MLE}
	Suppose $\lambda_e$	is the MLE of $\lambda$. Define $\tau\coloneqq h(\lambda)$. Then, $\tau_e=h(\lambda_e)$.
\end{thm}
\begin{prf}
	In this proof, we will prove the case when $h$ is a one-to-one function. The case of $h$ being a many-to-one function is beyond the scope of this course. \par 
	Suppose $h(\cdot)$ is a one-to-one function. Then, $\lambda=h^{-1}(\tau)$ is well-defined. Then, \[\max_{\lambda}\L(\lambda;y_1,\dots,y_n)=\max_{\tau}\L\qty(h^{-1}(\tau);y_1,\dots,y_n)=\max_{\tau}\L(\tau;y_1,\dots,y_n).\]	
\end{prf}
\begin{eg}
	Waiting Time with an unknown Threshold.\par 
	Let $\lambda=1$ in exponential but there is an unknown threshold $\theta$, that, is $f_Y(y)=e^{-(y-\theta)}$ for $y\geq\theta,\ \theta>0$.	
	\begin{sol}
		Note that the likelihood function is given by \begin{align*}\L(\theta;y_1,\dots,y_n)=\prod_{i=1}^nf_Y(y_1)&=\exp\qty(-\sum_{i=1}^n(y_i-\theta)),\quad y_i\geq\theta,\ \theta>0\\&=\exp\qty(-\sum_{i=1}^n(y_i-\theta))\cdot\1_{\qty[y_i\geq0,\ \theta>0]},\end{align*} where \[\1_{x\in A}=\begin{cases}1\quad\text{if }x\in A\\0\quad\text{if }x\notin A.\end{cases}\] Using order statistics, \begin{align*}\L(\theta)&=\exp\qty(-\sum_{i=1}^n(y_i-\theta))\cdot\1_{\qty[y_{(n)}\geq y_{(n-1)}\geq\cdots\geq y_{(1)}\geq\theta,\ \theta>0]}\\&=\exp\qty(-\sum_{i=1}^ny_i+n\theta)\1_{\qty[y_{(n)}\geq\cdots\geq y_{(1)}\geq\theta,\ \theta>0]}.\end{align*}
		So, we know $\theta\leq y_{(1)}=y_{\min}$. \par 
		To maximize the likelihood function, we want to maximize $-\sum y_i+n\theta$. That is, to maximize $\theta$, as $\theta\leq y_{\min}$, it must be that $\theta_{\max}=y_{\min}$. Therefore, the MLE is $\theta^*=y_{\min}.$
	\end{sol}
\end{eg}
\begin{eg}
	Suppose $Y_1,\dots,Y_n\sim\text{Uniform}[0,a]$. That is, $f_Y(y;a)=\dfrac{1}{a}$ for $y\in[0,a]$. Find MLE $a_e$ of $a$. 
	\begin{sol}
		Note that \begin{align*}f_Y(y;a)&=\dfrac{1}{a}\cdot\1_{\qty{y\in[0,a]}}\\&=\dfrac{1}{a}\cdot\1_{\qty{0\leq y_{(1)}\leq\cdots\leq y_{(n)}\leq a}}&\text{where } y_{(1)}=\min y_i\text{ and } y_{(n)}=\max y_i\end{align*} Then, \[\L(a)=\dfrac{1}{a^n}\1_{\qty{0\leq y_{(1)}\leq\cdots\leq y_{(n)}\leq a}}\] To maximize $\L(a)$, we want to minimize $a^n$. Since $a\geq y_{(n)}$, it must be that $a_e=y_{(n)}$. 
		Here, we call $a_e=y_{(n)}$ an \textit{estimate}, and $\hat{a_{\text{MLE}}}=Y_{(n)}$ an \textit{estimator}.
	\end{sol}
\end{eg}
\begin{eg}
	\textbf{MLE that Does Not Esist}
	\par Suppose $f_Y(y;a)=\dfrac{1}{a},\quad y\in[0, a)$. Find the MLE.
	\begin{sol}
		The likelihood function is the same: \[\L(a)=\dfrac{1}{a^n}\1_{\qty{0\leq y_{(1)}\leq\cdots\leq y_{(n)}<a}}.\] However, since $[0,a)$ is not a closed set, the optimization problem $\dsst\max_{a\in[0,a)}\L(a)$ does not have a solution. Hence, the estimate does not exist. 
	\end{sol}
\end{eg}
\begin{rmk}
	MLE may not be unique all the time.	
\end{rmk}
\begin{eg}
	\textbf{Multiple MLE Values}
	\par Suppose  $X_1,\dots,X_n\sim\text{Uniform}\qty[a-\dfrac{1}{2},a+\dfrac{1}{2}]$, where $f_X(x;a)=1,\ x\in\qty[a-\dfrac{1}{2}, a+\dfrac{1}{2}]$. Find the MLE. 
	\begin{sol}
		In the indicator function notation, we can rewrite the pdf to be \[f_X(x;a)=\1_{\qty{a-\frac{1}{2}\leq x\leq a+\frac{1}{2}}}=\1_\qty{a-\frac{1}{2}\leq x_{(1)}\leq\cdots\leq x_{(n)}\leq a+\frac{1}{2}}.\] So, the likelihood function will be \[\L(a)=\prod_{i=1}^nf_x(x_i;a)=\begin{cases}1,\quad a\in\qty[x_{(n)}-\dfrac{1}{2}, x_{(1)}+\dfrac{1}{2}]\\0,\quad \text{otherwise}.\end{cases}\] So, the $\L(a)$ will be maximized whenever $a\in\qty[x_{(n)}-\dfrac{1}{2}, x_{(1)}+\dfrac{1}{2}]$. Therefore, MLE can be any value in the range $\qty[x_{(n)}-\dfrac{1}{2}, x_{(1)}+\dfrac{1}{2}]$. Say, \[a_e=x_{(n)}-\dfrac{1}{2}\quad\text{or}\quad a_e=x_{(1)}-\frac{1}{2}\quad\text{or}\quad a_e=\dfrac{x_{(n)}-\frac{1}{2}+x_{(1)}+\frac{1}{2}}{2}=\dfrac{x_{(n)}+x_{(1)}}{2}.\]
	\end{sol}
\end{eg}
\begin{thm}{MLE for Multiple Parameters}
	In general, we have the likelihood function $\L(\theta)$, where $\theta=\qty(\theta_1,\dots,\theta_p)$. To find the MLE, we need \[\pdv{\L(\theta)}{\theta_i}=0\quad i=1,\dots,p,\] and the Hessian matrix \[\mqty(\dsst\pdv[2]{\L(\theta)}{\theta_i}{\theta_j})_{i,j=1,\dots,p}\coloneqq\mqty(\dsst\pdv[2]{\L(\theta)}{\theta_1}&\cdots&\dsst\pdv[2]{\L(\theta)}{\theta_1}{\theta_p}\\\vdots&\ddots&\vdots\\\dsst\pdv[2]{\L(\theta)}{\theta_p}{\theta_1}&\cdots&\dsst\pdv[2]{\L(\theta)}{\theta_p})\] should be negative dfinite. 
\end{thm}
\begin{eg}
	\textbf{MLE for Multiple Parameters: Normal Distribution}
	\par Suppose $Y_1,\dots,Y_n\sim N(\mu,\sigma)$. Then, \[f_{Y_i}(u;\mu,\sigma)=\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\qty(y_i-\mu)^2/\qty(2\sigma^2)}.\] Find the MLE for $\mu$ and $\sigma$.
	\begin{sol}
		The likelihood function will be \[\L(\mu,\sigma)=\prod_{i=1}^n\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\qty(y_i-\mu)^2/\qty(2\sigma^2)}.\] Then, we define \[\l(\mu,\sigma)=\ln \L(\mu,\sigma)=-\dfrac{n}{2}\ln2\pi-\dfrac{n}{2}\ln\sigma^2-\dfrac{1}{2}\qty(\sigma^2)^-1\sum_{i=1}^n\qty(y_i-\mu)^2.\] Set \[\begin{cases}\dsst\pdv{\l(\mu,\sigma)}{\mu}=0\quad\quad\text{\ding{172}}\\\dsst\pdv{\l(\mu,\sigma)}{\sigma}=0\qquad\text{\ding{173}}\end{cases}\] From \ding{172}, we have \begin{align*}\dfrac{1}{\sigma^2}\sum_{i=1}^n\qty(y_1-\mu)&=0\\\sum_{i=1}^ny_i&=n\mu\implies\boxed{\mu_e=\dfrac{\sum y_i}{n}=\bar{y}}\end{align*} From \ding{173}, by the invariant property of MLE, we instead set \begin{align*}\dsst\pdv{\l(\mu,\sigma)}{\sigma^2}&=0\\-\dfrac{n}{2}\cdot\dfrac{1}{\sigma^2}+\dfrac{1}{2}\qty(\dfrac{1}{\sigma^2})^2\sum_{i=1}^n\qty(y_i-\mu)^2&=0\\\dfrac{1}{2\sigma^2}\qty(-n+\dfrac{1}{\sigma^2}\sum_{i=1}^n\qty(y_i-\mu)^2)&=0\\-n\sigma^2+\sum_{i=1}^n\qty(y_i-\mu)^2&=0\qquad\qquad(\mu_e=\bar{y})\\\sum_{i=1}^n\qty(y_i-\bar{y})^2&=n\sigma^2\\\sigma_e^2=\dfrac{1}{n}\sum_{i=1}^n\qty(y_i-\bar{y})^2&\implies\boxed{\sigma_e=\sqrt{\dfrac{1}{n}\sum_{i=1}^n\qty(y_i-\bar{y})^2}}\end{align*}
	\end{sol}
\end{eg}

\subsection{The Method of Moment}
\begin{df}{Moment Generating Function}
	The \textit{Moment Generating Function (MGF)} is defined as \[\M_X(t)=\E\qty[e^{tX}],\] and it uniquely determines a probability distribution.
\end{df}
\begin{df}{Moment}
	The \textit{$k$-th order moment} of $X$ is $\E\qty[X^k]$.	
\end{df}
\begin{eg}{Meaning of Different Moments}
	\begin{itemize}
		\item $\E[X]$: location of a distribution
		\item $\E\qty[X^2]=\Var(X)-\E[X]^2$: width of a distribution
		\item $\E\qty[X^3]$: skewness -- positively skewed / negatively skewed
		\item $\E\qty[X^4]$: kurtosis / tailedness -- speed decaying to $0$.
	\end{itemize}	
\end{eg}
\begin{eg} 
	\textbf{Moment Estimate: Moments of Population and Sample}
	\begin{center}
	\begin{tabular}{c|c}
		\textbf{Population}&\textbf{Sample, $X_1,\dots,X_n$}\\\hline\\
		$\E[X]=\mu$&$\hat\mu=\bar X=\dfrac{X_1+\cdots+X_n}{n}$\\\\
		$\E\qty[X^2]=\mu^2+\sigma^2$&$\hat\mu^2+\hat\sigma^2=\dfrac{X_1^2+\cdots+X_n^2}{n}$\\
		$\vdots$&$\vdots$\\
		$\E\qty[X^k]$&$\dfrac{X_1^k+\cdots+X_n^k}{n}$
	\end{tabular}
	\end{center}
	\textbf{Rationale}: The population moments should be close to the sample moments. 
\end{eg}
\begin{eg}
	\begin{itemize}
		\item Consider $N(\mu,\sigma^2)$, where $\sigma$ is given. Estimate $\mu$.\par By the method of moment estimate, we have $\mu_e=\bar{X}$.
		\item Consider $N(\mu,\sigma^2)$. Estimate $\mu$ and $\sigma$.\par We have $\mu_e=\bar X$ and $\mu_e^2+\sigma_e^2=\dfrac{X_1^2+\cdots+X_n^2}{n}$.
		\item Consider $N(\theta,\sigma^2)$. Given $\E\qty(X^4)=3\sigma^4$, estimate $\mu$ and $\sigma$.\par We have $\mu_e=\bar X$, $\mu_e^2+\sigma_e^2=\dfrac{X_1^2+\cdots+X_n^2}{n}$, and $3\sigma^4=\dfrac{X_1^4+\cdots+X_n^4}{n}$. We have three equations but only two unknowns, then a solution is not guaranteed. So, we need some restrictions on this method (see Remark 1.2). 
	\end{itemize}
\end{eg}
\begin{thm}{Method of Moments Estimates}
	For a random sample of size $n$ from the discrete (or continuous) population/pdf $p_X(k;\theta_1,\dots,\theta_s)$ (or $f_Y(y;\theta_1,\dots,\theta_s)$), solutions to the system \[\begin{cases}\E(Y)=\dfrac{1}{n}\dsst\sum_{i=1}^ny_i\\\qquad\vdots\\\E\qty(Y^s)=\dfrac{1}{n}\dsst\sum_{i=1}^ny_i^s\end{cases}\] which are denoted by $\theta_{1e},\dots,\theta_{se}$, are called the \textbf{method of moments estimates} of $\theta_1,\dots,\theta_s$.
\end{thm}
\begin{rmk}
	To estimate $k$ parameters with the method of moments estimates, we will only match the first $k$ orders of moments. 
\end{rmk}
\begin{eg}
	Consider the Gamma distribution: \[f_Y(y;r,\lambda)=\dfrac{\lambda^r}{\Gamma(r)}y^{r-1}e^{-\lambda y}\quad\text{for }y\geq0.\] Given $\E(Y)=\dfrac{r}{\lambda}$ and $\E\qty(Y^2)=\dfrac{r}{\lambda^2}+\dfrac{r^2}{\lambda^2}$. Estimate $r$ and $\lambda$.
	\begin{sol}
		\[\E\qty(Y)=\dfrac{r}{\lambda}\implies\dfrac{r_e}{\lambda_e}=\dfrac{y_1+\cdots+y_n}{n}=\bar{y}\qquad\text{\ding{172}}\] \[\E\qty(Y^2)=\dfrac{r}{\lambda^2}+\dfrac{r^2}{\lambda^2}\implies\dfrac{r_e}{\lambda_e^2}+\dfrac{r_e^2}{\lambda_e^2}=\dfrac{y_1^2+\cdots+y_n^2}{n}\qquad\text{\ding{173}}\] Substitute \ding{172} into \ding{173}, we have \[\dfrac{\bar{y}}{\lambda_e}+\qty(\bar{y})^2=\dfrac{1}{n}\sum_{i=1}^ny_i^2\implies \boxed{\lambda_e=\dfrac{\bar{y}}{\frac{1}{n}\sum y_i^2-\bar{y}^2}}\qquad\text{\ding{174}}\] Substitute \ding{174} into \ding{172}, we have \[r_e=\bar{y}\lambda_e=\boxed{\dfrac{\bar{y}^2}{\frac{1}{n}\sum y_i^2-\bar{y}^2}}.\]
	\end{sol}
\end{eg}
\begin{rmk}
	The \emph{sample variance} is defined as \begin{align*}\dfrac{1}{n}\sum_{i=1}^n\qty(y_i-\bar{y})^2&=\dfrac{1}{n}\sum_{i=1}^n\qty(y_i^2-2y_i\bar{y}+\bar{y}^2)\\&=\dfrac{1}{n}\sum_{i=1}^ny_i^2-2\bar{y}\cdot\dfrac{\sum y_i}{n}+\dfrac{1}{n}\cdot n\bar{y}^2\\&=\dfrac{1}{n}\sum_{i=1}^ny_i^2-2\bar{y}^2+\bar{y}^2&\bar{y}=\dfrac{\sum y_i}{n}\\&=\dfrac{1}{n}\sum_{i=1}^ny_i^2-\bar{y}^2.\end{align*} So, in Example 1.3.7, if we define $\hat{\sigma}^2$ to be the sample variance, we can further simply our estimate as follows: \[\lambda_e=\dfrac{\bar{y}}{\hat{\sigma}^2},\qquad r_e=\dfrac{\bar{y}^2}{\hat{\sigma}^2}.\]
\end{rmk}

\subsection{Interval Estimation}
\begin{eg}{}
	Estimate $\mu$, where $X\sim N(\mu, 1)$.\par 
	We take some samples and compute their sample means: \[\bar{X}^1=\dfrac{x_1+\cdots+x_n}{n},\bar{X}^2=\dfrac{\widetilde{x_1}+\cdots+\widetilde{x_n}}{n},\cdots\] Finding the distribution of $\bar{X}$, we can find an interval $\qty[\hat\theta_L,\hat\theta_U]$ such that \[\P\qty(\hat\theta_L\leq\bar X\leq\hat\theta_U)=1-\alpha.\]
\end{eg}
\begin{rmk}
	By using the variance of the estimator, one can construct an interval such that with a high probability that the interval contains the unknown parameter. 	
\end{rmk}
\begin{df}{Confidence Interval}
	The interval, $\qty[\hat\theta_L,\hat\theta_U]$	is called the \textit{confidence interval}, and the high probability is $1-\alpha$, where $\alpha$ is given. 
\end{df}
\begin{rmk}
	Take $\alpha=5\%$, then $\qty[\hat\theta_L,\hat\theta_U]$ is the $95\%$ confidence interval of $\mu$. It does not mean that $\mu$ has $95\%$ chance to be in $\qty[\hat\theta_L,\hat\theta_U]$. However, if we construct $1000$ such intervals, $950$ of them will contain $\mu$.
\end{rmk}
\begin{eg}
	A random sample of size $4$, $\qty(Y_1=6.5,\ Y_2=9.2,\ Y_3=9.9,\ Y_4=12.4)$, from a normal population: \[f_Y(y;\mu)=\dfrac{1}{\sqrt{2\pi}0.8}e^{-\dfrac{1}{2}\qty(\dfrac{y-\mu}{0.8})^2}\sim N(\mu, \sigma^2=0.64).\] Both MLE and MME give $\mu_e=\bar y=9.5$. The estimator $\hat{\mu}=\bar{Y}$ follows normal distribution. Construct $95\%$-confidence interval for $\mu$.
	\begin{sol}
		$\E\qty(\bar Y)=\mu$ and $\Var\qty(\bar Y)=\dfrac{\sigma^2}{n}=\dfrac{0.64}{4}$. By the Central Limit Theorem, $\bar{Y}$ approximately follow $N\qty(\mu,\dfrac{\sigma^2}{n})$. So, $\dfrac{\bar Y-\mu}{\sqrt{\frac{\sigma^2}{n}}}\sim N(0,1)$. Then, \[\P\qty(z_1\leq\dfrac{\bar Y-\mu}{\sqrt{\frac{\sigma^2}{n}}}\leq z_2)=0.95\implies\P\qty(\bar Y-z_2\sqrt{\dfrac{\sigma^2}{n}}\leq\mu\leq\bar Y-z_1\sqrt{\dfrac{\sigma^2}{n}})=0.95\] There are infinite many ways to construct a confidence interval by selecting different $z_1$ and $z_2$. However, since we don't have any prior knowledge on $\mu$, it is good for us to choose $z_1$ and $z_2$ symmetrically. Moreover, symmetric $z_1$ and $z_2$ will yield a smaller interval. We know the symmetric $z_1,z_2$ pair will be $z_1=-1.96$ and $z_2=1.96$. Therefore, \[\P\qty(\bar Y-1.96\sqrt{\dfrac{0.64}{4}}\leq\mu\leq\bar Y+1.96\sqrt{\dfrac{0.64}{4}})=0.95.\] Then, $95\%$ confidence interval is $\qty[9.5-1.96\times0.4,\ 9.5+1.96\times0.4]$. 
	\end{sol}
\end{eg}
\begin{thm}{Confidence Interval}
	In general, for a normal population with $\sigma$ known, the $100(1-\alpha)\%$ \textit{two-sided confidence interval} for $\mu$ is \[\qty(\bar{y}-z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}},\ \bar{y}+z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}})\]
\end{thm}
\begin{thm}{Variation of Confidence Interval}
	\begin{itemize}
		\item One-sided interval: \[\qty(\bar{y}-z_{\alpha}\dfrac{\sigma}{\sqrt{n}},\ \bar{y})\text{ or }\qty(\bar{y},\ \bar{y}+z_{\alpha}\dfrac{\sigma}{\sqrt{n}})\]
		\item $\sigma$ is unknown and sample size is small: $z$-score $\to$ $t$-score.
		\item $\sigma$ is unknown and sample size is large: $z$-score by CLT.
		\item Non Gaussian population but sample size is large: $z$-score by CLT. 
	\end{itemize}	
\end{thm}
\begin{thm}{}
	Let $k$ be the number of successes in $n$ independent trials, where $n$ is large and $p=\P(\text{success})$ is unknown. An approximate $100(1-\alpha)\%$ confidence interval for $p$ is the set of numbers \[\qty(\dfrac{k}{n}-z_{\alpha/2}\sqrt{\dfrac{(k/n)(1-k/n)}{n}},\ \dfrac{k}{n}+z_{\alpha/2}\sqrt{\dfrac{(k/n)(1-k/n)}{n}}).\]
\end{thm}
\begin{df}{Margin of Error}
	The \textit{margin of error}, denoted by $d$, is the quantity\[d=z_{\alpha/2}\sqrt{\dfrac{(k/n)(1-k/n)}{n}}.\]	
\end{df}
\begin{rmk}
	Stating the sample mean and the margin of error is equivalent to stating the confidence interval. Note that $\operatorname{C.I.}=\hat p\pm d$.	
\end{rmk}
\begin{thm}{Estimate Margin of Error}
	When $p$ is close to $\dfrac{1}{2}$, then $d\approx d_m=\dfrac{z_{\alpha/2}}{2\sqrt{n}}$, which is 	equivalent to $\sigma_n\approx\dfrac{1}{2\sqrt{n}}$. However, if $p$ is away from $\dfrac{1}{2}$, $d$ and $d_m$ are very different.
\end{thm}
\begin{rmk}
	Theorem 1.4.8 gives aconservative estimation of the margin of error, which is $d_m$.	
\end{rmk}
\begin{prop}{}
	Given $d$, we can estimate the sample size. 
	\begin{prf}
		\[d=z_{\alpha/2}\sqrt{\dfrac{\hat{p}(1-\hat{p})}{n}}\implies n\approx\hat{p}(1-\hat{p})/\qty(\dfrac{d}{z_{\alpha/2}})^2.\] However, since $n$ is unknown, $\hat{p}$ is also unknown. We, therefore, need information on the actual $p$ to conclude an estimation of the sample size.
		\begin{itemize}
			\item If $p$ is known, \[n=\dfrac{p(1-p)}{\qty(\frac{d}{z_{\alpha/2}})^2}.\]
			\item If $p$ is unknown. Let $f(p)=p(1-p)$. $f$ will be maximized when $p=0.5$. So, $f(p)=p(1-p)\leq0.25$. Then, \[n\leq\dfrac{0.25}{\qty(\frac{d}{z_{\alpha/2}})^2}.\] Since we are conservative, take $n=\dfrac{\frac{1}{4}z_{\alpha/2}^2}{d^2}=\dfrac{z_{\alpha/2}^2}{4d^2}$. This estimation is a conservative estimation of the sample size. 
		\end{itemize}
	\end{prf}
\end{prop}

\subsection{Properties of Estimation}
The main question is that estimators are not unique in general. How do we choose a good estimator? 
\begin{df}{Unbiasedness}
	Given a random sample of size $n$ when whose population distribution depends on an unknown parameter $\theta$. Let $\hat\theta$ be an estimator of $\theta$. Then, 
	\begin{itemize}
		\item $\hat\theta$ is called \textit{unbiased} if $\E(\hat\theta)=\theta$.
		\item $\hat\theta$ is called \textit{asymptotically unbiased} if $\dsst\lim_{n\to\infty}\E(\hat\theta)=\theta$.
		\item If $\theta$ is biased, then the \textit{bias} is given by the quantity $\B(\hat\theta)=\E(\hat\theta)-\theta$.
	\end{itemize}
\end{df}
\begin{eg}
	Consider the exponential distribution: $f_Y(y;\lambda)=\lambda e^{-\lambda y}$ for $y\geq0$. Determine if the estimator $\hat\lambda=\dfrac{1}{\bar Y}$ is biased or not. \newline\textit{Hint: $n\bar Y=\dsst\sum_{i=1}^nY_i\sim\text{Gamma}(n,\lambda)$.}
	\begin{sol}
		Recall that $\E[g(x)]\dsst=\int_xg(x)f_X(x)\ \d{x}$. Define $X=\dsst\sum_{i=1}^nY_i\sim\text{Gamma}(n,\lambda)$. Also, recall the following facts: \[\Gamma(n)=(n-1)!=(n-1)\Gamma(n-1)\] and the integration over any probability density function will yield a result of $1$ by definition. \par  Then, \begin{align*}\E(\hat\lambda)=\E\qty(\dfrac{1}{\bar Y})=\E\qty(\dfrac{n}{\sum Y_i})&=n\E\qty(\dfrac{1}{\sum Y_i})\\&=n\E\qty(\dfrac{1}{X})\\&=n\int_x\dfrac{1}{x}\cdot\dfrac{\lambda^n}{\Gamma(n)}x^{n-1}e^{-\lambda x}\ \d{x}\\&=n\int_x\dfrac{\lambda^n}{(n-1)!}x^{n-2}e^{-\lambda x}\ \d{x}\\&=\dfrac{n\lambda}{(n-1)}\underbrace{\int_x\dfrac{\lambda^{n-1}}{\Gamma(n-1)}x^{n-2}e^{-\lambda x}\ \d{x}}_{=1}\\&=\dfrac{n}{n-1}\lambda.\end{align*} Therefore, $\E(\hat\lambda)\neq\lambda$, and so $\hat\lambda$ is biased. However, note that \[\lim_{n\to\infty}\E(\hat\lambda)=\lim_{n\to\infty}\dfrac{n}{n-1}\lambda=\lambda.\] By definition, then $\hat\lambda$ is asymptotically unbiased. 
	\end{sol}
\end{eg}
\begin{eg}
	Consider the exponential distribution $f(y;\theta)=\dfrac{1}{\theta}e^{-y/\theta}$ for $y\geq0$. Then, $\hat\theta=\bar Y$ is unbiased. 	
\end{eg}
\begin{rmk}
	Suppose $\qty{X_1,\dots,X_n}$ are $\iid$ random variables, and $\E(X_i)=\mu$ for $i=1,\dots,n$	. Then, $\bar X$, the sample mean, is always an unbiased estimator: \[\E(\bar X)=\E\qty(\dfrac{1}{n}\sum_{i=1}^nX_i)=\dfrac{1}{n}\sum_{i=1}^n\E(X_i)=\dfrac{1}{n}\cdot n\cdot \mu=\mu.\]
\end{rmk}
\begin{thm}{Sample Variance is Biased}
	Suppose $\qty{X_1,\dots,X_n}$ are $\iid$ random variables, and $\E(X_i)=\mu,\ \Var(X_i)=\sigma^2$ for $i=1,\dots,n$. Then, the sample variance $\dsst\hat\sigma^2=\dfrac{1}{n}\sum_{i=1}^n\qty(X_i-\bar X)^2$ is biased. 
\end{thm}
\begin{prf}
	Note that \begin{align*}
		\E(\hat\sigma^2)&=\E\qty(\dfrac{1}{n}\sum_{i=1}^n(X_i-\bar X)^2)\\&=\E\qty(\dfrac{1}{n}\sum_{i=1}^n\qty(X_i-\mu+\mu-\bar{X})^2)\\&=\dfrac{1}{n}\sum_{i=1}^n\E\qty[\qty(X_i-\mu)^2+\qty(\mu-\bar X)^2+2\qty(X_i-\mu)(\mu-\bar X)]\\&=\dfrac{1}{n}\sum_{i=1}^n\qty{\underbrace{\E(X_i-\mu)^2}_{\Var(X_i)}+\E\qty(\mu-\bar X)^2+2\E\qty[(\mu-\bar X)(X_i-\mu)]}\\&\ \ \ \bigg|\quad\textit{Hint: }\dfrac{1}{n}\sum_{i=1}^n(X_i-\mu)=\dfrac{1}{n}\sum_{i=1}^nX_i-\dfrac{1}{n}\sum_{i=1}^n\mu=\bar X-\mu\\&=\dfrac{1}{n}\sum_{i=1}^n\Var(X_i)+\dfrac{1}{n}\cdot n\E\qty(\mu-\bar{X})^2+2\E\qty[(\mu-\bar X)\dfrac{1}{n}\sum_{i=1}^n\qty(X_i-\mu)]\\&=\dfrac{1}{n}\sum_{i=1}^n\sigma^2+\E\qty(\mu-\bar X)^2+2\E\qty[(\mu-\bar X)(\bar X-\mu)]\\&=\dfrac{1}{n}\cdot n\cdot\sigma^2+\E\qty(\mu-\bar X)^2-2\E\qty[(\mu-\bar X)^2]\\&=\sigma^2-\E\qty(\mu-\bar X)^2\\&=\sigma^2-\underbrace{\E\qty(\bar X-\mu)^2}_{=\Var(\bar X)}\\&=\sigma^2-\dfrac{\sigma^2}{n}=\dfrac{n-1}{n}\sigma^2\neq\sigma^2
	\end{align*}
	Therefore, $\hat\sigma^2$ is not an unbiased estimator. 
\end{prf}
\begin{thm}{Adjusted Sample Variance is Unbiased}
	With the same set up in Theorem 1.5.4, define the adjusted sample variance to be \[S^2=\dfrac{n}{n-1}\hat\sigma^2=\dfrac{1}{n-1}\sum_{i=1}^n\qty(X_i-\bar X)^2.\] Then, $S^2$ is an unbiased estimator of $\sigma^2$.
\end{thm}

\begin{df}{Decision Theory}
	Minimize the error of an estimator (sample statistics) relative to the true parameter (population parameter) using a loss function.
\end{df}
\begin{df}{Mean Squared Error}
	The \textit{mean squared error} (MSE) is defined by \[\MSE(\hat\theta)=\E\qty[(\hat\theta-\theta)^2]\]	
\end{df}
\begin{thm}{Decomposition of MSE}
	Generally, \[\MSE(\theta)=\Var(\hat\theta)+\B\qty(\hat\theta)^2\] If $\hat\theta$ is unbiased, $\MSE(\hat\theta)=\Var(\hat\theta)$. $\Var(\theta)$ measures the precision of the estimator. 
\end{thm}
\begin{prf}
	Note that we will the following: \begin{align*}
		\MSE(\hat\theta)&=\E\qty[(\hat\theta-\theta)^2]\\&=\E(\hat\theta^2+\theta^2-2\hat\theta\theta)\\&=\E(\hat\theta)-2\theta\E(\hat\theta)+\theta^2\\&=\underbrace{\E(\hat\theta^2)-\E(\hat\theta)^2}+\underbrace{\E(\hat\theta)^2-2\theta\E(\hat\theta)+\theta^2}\\&=\Var(\hat\theta)+\qty[\E(\hat\theta)-\theta]^2\\&=\Var(\theta)+\B(\hat\theta)^2
	\end{align*}	
	If $\hat\theta$ is unbiased, $\B(\hat\theta)=0$, and so $\MSE(\hat\theta)=\Var(\hat\theta)$.
\end{prf}
\begin{df}{Efficiency}
	Let $\hat\theta_1$ and $\hat\theta_2$ be two unbiased estimators for a parameter $\theta$. If we have $\Var(\hat\theta_1)<\Var(\hat\theta_2)$, then we say that $\hat\theta_1$ is \textit{more efficient} than $\hat\theta_2$. The \textit{relative efficiency} of $\hat\theta_1$ with respect to $\hat\theta_2$ is the ratio $\dfrac{\Var(\hat\theta_2)}{\Var(\hat\theta_1)}$.
\end{df}

\subsection{Best Unbiased Estimator}
\begin{df}{Best/Minimum-Variance Estimator}
	Let $\Theta$ be the set of all estimators $\hat\theta$ that are unbiased for the parameter $\theta$. We way that $\hat\theta^*$ is a \textit{best} or \textit{minimum-variance estimator} (MVE) if $\hat\theta^*\in\Theta$ and $\Var(\hat\theta^*)\leq\Var(\hat\theta)\quad\forall\ \hat\theta\in\Theta$.
\end{df}
\begin{df}{Fisher's Information}
	The \textit{Fisher's information} of a continuous random variable $Y$ with pdf $f_Y(y;\theta)$ is defined as \[\I(\theta)=\E\qty[\qty(\pdv{\ln f_Y(y;\theta)}{\theta})^2]=-\E\qty[\pdv[2]{\theta}\ln f_Y(y;\theta)].\]
\end{df}
\begin{rmk}
	The Fisher's information measures the amount of information that a sample $Y$ contains about the unknown parameter $\theta$. If $\I(\theta)$ is big, then the curvature of $f_Y(y;\theta)$ is big, and thus it is more likely that we can find a region where $\hat\theta$ is concentrated. 
\end{rmk}
\begin{ext}[Joint Fisher's Information]
	Suppose	$Y_1,\dots,Y_n$ are continuous $\iid$ random variables, each has a Fisher's information of $\I(\theta)$. Then, \[\E\qty[\qty(\pdv{\theta}\ln f_{Y_1,\dots,Y_n}(y_1,\dots,y_n;\theta))^2]=n\I(\theta).\]
\end{ext}
\begin{thm}{Properties of Fisher's Information}
	Define the \textit{Fisher's Score Function}	$\dsst\pdv{\theta}\ln f_Y(y;\theta)$. Then, \[\E_Y\qty[\pdv{\theta}\ln f_Y(y;\theta)]=0.\]
\end{thm}
\begin{prf}
	Note that by chain rule, we have \begin{align*}
		\E_Y\qty[\pdv{\theta}\ln f_Y(y;\theta)]&=\int_Y\qty(\pdv{\theta}\ln f_Y(y;\theta))f_Y(y;\theta)\ \d{y}\\&=\int_Y\dfrac{1}{f_Y(y;\theta)}\qty(\pdv{\theta}f_Y(y;\theta))f_Y(y;\theta)\ \d{y}\\&=\int_Y\pdv{\theta}f_Y(y;\theta)\ \d{y}\\&=\pdv{\theta}\int_Yf_Y(y;\theta)\ \d{y}=\pdv{\theta}(1)=0.
	\end{align*}
\end{prf}
\begin{cor}{}
	\[\I(\theta)=\Var\qty(\pdv{\theta}\ln f_Y(y;\theta)).\]
\end{cor}
\begin{prf}
	By definition, we have \begin{align*}
		\Var\qty(\pdv{\theta}\ln f_Y(y;\theta))&=\E\qty[\qty(\pdv{\theta}\ln f_Y(y;\theta))^2]-\qty(\underbrace{\E\qty(\pdv{\theta}\ln f_Y(y;\theta))}_{=0,\text{ by Theorem 1.6.3.}})^2\\&=\E\qty[\qty(\pdv{\theta}\ln f_Y(y;\theta))^2]\\&=\I(\theta).
	\end{align*}
\end{prf}
\begin{thm}{Cramér-Rao Inequality}
	Under regular condition, let $Y_1,\dots,Y_n$ be a random sample of size $n$ form the continuous population pdf $f_Y(y;\theta)$. Let $\hat\theta=\hat\theta(Y_1,\dots,Y_n)$ be any unbiased estimator for $\theta$. Then, \[\Var(\hat\theta)\geq\dfrac{1}{n\I(\theta)}.\]
\end{thm}
\begin{rmk} A similar statement holds for the discrete case $p_X(k;\theta)$. \end{rmk}
\begin{df}{Efficiency of Unbiased Estimator}
	An unbiased estimator $\hat\theta$ is \textit{efficient} if $\Var(\hat\theta)$ is equal to the Cramér-Rao lower bound. That is, $\Var(\hat\theta)=\qty(n\I(\theta))^{-1}$. Such an estimator is the MVE defined in Definition 1.6.1. The \textit{efficiency} of an unbiased estimator $\hat\theta$ is defined to be the quantity \[\qty(n\I(\theta)\Var(\hat\theta))^{-1}.\]
\end{df}
\begin{eg}
	Suppose $X\sim\text{Bernoulli}(p)$. Is $\hat p=\bar X$ efficient? 
	\begin{sol}
		Note that we have the following\begin{align*}
			f_X(x;p)&=p^x(1-p)^{1-x},\quad x=0,1\\\ln f_X(x;p)&=x\ln p+(1-x)\ln(1-p)\\\pdv{p}\ln f_X(x;p)&=\dfrac{x}{p}-\dfrac{1-x}{1-p}\\\pdv[2]{p}\ln f_X(x;p)&=-\dfrac{x}{p^2}-\dfrac{1-x}{(1-p)^2}\end{align*} 
		Therefore, the Fisher's information can be computed by \begin{align*}\I(p)=-\E\qty[\pdv[2]{p}\ln f_X(x;p)]&=-\E\qty[-\dfrac{x}{p^2}-\dfrac{1-x}{(1-p)^2}]\\&=\E\qty[\dfrac{x}{p^2}]+\E\qty[\dfrac{1-x}{(1-p)^2}]\\&=\dfrac{\E(x)}{p^2}+\dfrac{1-\E(x)}{(1-p)^2}\\&=\dfrac{p}{p^2}+\dfrac{1-p}{(1-p)^2}=\dfrac{1}{p}+\dfrac{1}{1-p}=\dfrac{1}{p(1-p)}.\end{align*} 
		Note that \[\Var(\bar X)=\Var\qty(\dfrac{1}{n}\sum_{i=1}^nX_i)=\dfrac{1}{n^2}\sum_{i=1}^n\Var(X_i)=\dfrac{1}{n}\Var(X_i)=\dfrac{1}{n}\cdot p(1-p).\] So, we have \[\Var(\bar X)=\dfrac{p(1-p)}{n}=\dfrac{1}{n\qty(\frac{1}{p(1-p)})}=\dfrac{1}{n\I(p)}.\] Therefore, $\hat p$ is efficient. 
	\end{sol}
\end{eg}
\begin{eg}
	Suppose $X\sim N(\mu,\sigma^2)$, with $\sigma^2$ is known. What is $\I(\mu)$?
	\begin{sol}
		Note that \[\dv[2]{\mu}\ln f_X(x;\mu)=-\dfrac{1}{\sigma^2}.\] Then, \[\I(\mu)=-\E\qty[\dv[2]{\mu}\ln f_X(x;\mu)]=-\E\qty[-\dfrac{1}{\sigma^2}]=\dfrac{1}{\sigma^2}.\]
	\end{sol}
\end{eg}

\subsection{Sufficiency}
\begin{rmk} Use Likelihood Function to Define Fisher's Information
\begin{itemize}
	\item We can define the \emph{score function} as $\dsst\pdv{\ln \L\qty(Y_1,\dots,y_n;\theta)}{\theta}=0\implies$ MLE.
	\item $\E\qty[\dsst\pdv{\ln\L(Y;\theta)}{\theta}]=0$
	\item $\I(\theta)=\E\qty[\dsst\qty(\pdv{\ln\L(Y;\theta)}{\theta})^2]=-\E_Y\qty[\dsst\pdv[2]{\ln\L(Y;\theta)}{\theta}]$
	\item $-\E_Y\qty[\dsst\pdv[2]{\ln\L(Y_1,\dots,Y_n;\theta)}{\theta}]=n\I(\theta)$.
	\begin{prf}
		\begin{align*}
			-\E_Y\qty[\dsst\pdv[2]{\ln\L(Y_1,\dots,Y_n;\theta)}{\theta}]&=-\E_Y\qty[\pdv[2]{\theta}\ln\L\qty(Y_1,\dots,Y_m;\theta)]\\&=-\E_Y\qty[\pdv[2]{\theta}\ln\qty(\prod_{i=1}^nf_Y\qty(Y_i;\theta))]\\&=-\E_Y\qty[\pdv[2]{\theta}\sum_{i=1}^nf_Y\qty(y_i;\theta)]=\sum_{i=1}^n\qty(-\E_Y\qty[\pdv[2]{\theta}f_Y(y_i;\theta)])=n\I(\theta)
		\end{align*}
	\end{prf}
	\item $\hat{\theta_\text{MLE}}\xrightarrow{n\to\infty}N\qty(\theta,\dfrac{1}{\I(\theta)})$. Note that $\dfrac{1}{\I(\theta)}$ is the C-R lower bound. We see that $\hat{\theta_\text{MLE}}$ is asymptotically efficient. 
\end{itemize}
\end{rmk}
\begin{rmk}[Sufficiency Intuition]
	Sufficiency tells us how much information can we get out of the data. 
	\begin{description}
		\item[Rationale] Let $\hat\theta$ be an estimator to the unknown parameter $\theta$. Does $\hat\theta$ contain all information about $\theta$? \emph{e.g., The data itself is a sufficient estimator. }
	\end{description}	
\end{rmk}
\begin{df}{Sufficiency}
	Let $(X_1,\dots,X_n)$ be a random sample of size $n$ from a continuous population with an unknown parameter $\theta$. We call $\theta$ is \textit{sufficient} if \[f_{Y_1,\dots,Y_n\mid\hat\theta}\qty(Y_1,\dots,Y_n\mid\hat\theta=\theta_e)=b(y_1,\dots,y_n),\] where $b(y_1,\dots,y_n)$ is independent of $\theta$ ($\independ\theta$). Also, $\hat\theta=h\qty(Y_1,\dots,Y_n)$ and $\theta_e=h\qty(y_1,\dots,y_n)$. In this case, $\hat\theta$ contains all the information about $\theta$ from $\qty{y_1,\dots,y_n}$.
\end{df}
\begin{eg}
	\begin{itemize}
		\item Toss a coin $5$ times and get $3$ heads. Estimate $p=$ probability of $H$.
		\begin{sol}
			\[\P\qty(HHHTT\mid p_e=\dfrac{3}{5})=\dfrac{1}{\dsst\binom{3}{5}}\independ p\implies\text{sufficient}\]
		\end{sol}
		\item A random sample of size $n$ from Bernoulli$(p)$. Check the sufficiency of $p=\dsst\sum_{i=1}^nX_i$.
		\begin{sol}
			Suppose the random sample is $\qty{X_1,\dots,X_n}$. Then, consider \[\P(X_1=x_1,\dots,X_n=x_n,\dsst\sum_{i=1}^nX_i=C\mid\hat p=C)=\dfrac{\P(X_1=x_1,\dots,X_n=x_n,\dsst\sum_{i=1}^nX_i=C)}{\P\qty(\hat{p}=C)}.\] What new information can $\dsst\sum_{i=1}^nX_i=C$ tell us? $X_n=C-\dsst\sum_{i=1}^{n-1}X_i$.\par 
			Note that $\P(\hat{p}=C)=\P\qty(\dsst\sum_{i=1}^nX_i=C)$. Since the summation of Bernoulli($p$) random variables is a Binomial$(n,p)$ random variable, we have $\P(\hat{p}=C)=\dsst\binom{n}{C}p^C(1-p)^{n-C}$.\par 
			$\boxed{\text{Case I}}$ Suppose $\dsst\sum_{i=1}^nX_i=C$. Then, \begin{align*}&\dfrac{\P(X_1=x_1,\dots,X_n=x_n,\dsst\sum_{i=1}^nX_i=C)}{\P\qty(\hat{p}=C)}\\&=\dfrac{\qty(\dsst\prod_{i=1}^{n-1})p^{X_i}(1-p)^{1-X_i}p^{C-\dsst\sum_{i=1}^{n-1}X_i}(1-p)^{\qty(1-C+\dsst\sum_{i=1}^{n-1}X_i)}}{\dsst\binom{n}{C}p^C(1-p)^{n-C}}\\&=\dfrac{p^{\dsst\sum_{i=1}^{n-1}X_i+C-\sum_{i=1}^{n-1}X_i}(1-p)^{(n-1)-\dsst\sum_{i=1}^{n-1}X_i+1-C+\sum_{i=1}^{n-1}X_i}}{\dsst\binom{n}{C}p^C(1-p)^{n-C}}\\&=\dfrac{p^C(1-p)^{n-C}}{\dsst\binom{n}{C}p^C(1-p)^{n-C}}=\dfrac{1}{\dsst\binom{n}{C}}\independ p\implies\text{sufficient}\end{align*}\par 
			$\boxed{\text{Case II}}$ Suppose $\dsst\sum_{i=1}^nX_i\neq C$. Then, \[\dfrac{\P(X_1=x_1,\dots,X_n=x_n,\dsst\sum_{i=1}^nX_i=C)}{\P\qty(\hat{p}=C)}=\dfrac{0}{\P(\hat{p}=C)}=0\independ{p}\implies\text{sufficient}\]
		\end{sol}
	\end{itemize}	
\end{eg}
\begin{thm}{Factorization Property}
	$\hat\theta$ is sufficient if and only if the likelihood can be factorized as \[\L(\theta)=\underbrace{g(\theta_e;\theta)}_{\theta_e=h(y_1,\dots,y_n)\ \&\ \theta}\cdot \underbrace{u(y_1,\dots,y_n)}_{\independ\theta}.\]	
\end{thm}

\subsection{Consistency}
\begin{df}{Consistency}
	An estimator $\hat\theta_n=h\qty(W_1,\dots,W_n)$ is said to be \textit{consistent} if it converges to $\theta$ in probability; i.e., for any $\epsilon>0$, \[\lim_{n\to\infty}\P\qty(\abs{\hat\theta_n-\theta}<\epsilon)=1.\]
\end{df}
\begin{rmk}
\begin{enumerate}
	\item Consistency is an asymptotical property (defined in a large sample limit).
	\item $n$= sample size. $\abs{\hat\theta_n-\theta}$ is the distance between estimator and true $\theta$.
\end{enumerate}	
\end{rmk}
\begin{lem}{Markov Inequality}
	Suppose $X\geq0$ is a random variable and $a>0$ is a constant. Then, \[\P(X\geq a)\leq\dfrac{\E(X)}{a}.\]
\end{lem}
\begin{rmk}
	Markov inequality is good for determining extreme values. If $\E(X)$ is small, then it is very unlikely that $X$ will take some extremely large numbers. 	
\end{rmk}
\begin{thm}{Chebyshev Inequality}
	Let $W$ be some random variable with finite mean $\mu$ and variance $\sigma^2$. Then, for any $\epsilon>0$, we have \[\P\qty(\qty|W-\mu|<\epsilon)\leq 1-\dfrac{\sigma^2}{\epsilon^2}\] or, equivalently, \[\P\qty(\qty|W-\mu|\geq\epsilon)\leq\dfrac{\sigma^2}{\epsilon^2}.\]
\end{thm}
\begin{prf}
	Consider the random variable $\abs{W-\mu}$. Then, by Markov Inequality, \begin{align*}\P\qty(\abs{X-\mu}\geq\epsilon)&=\P\qty(\abs{X-\mu}^2\geq\epsilon^2)\\&=\P\qty(\qty(X-\mu)^2\geq\epsilon^2)\leq\dfrac{\E\qty[(X-\mu)^2]}{\epsilon^2}=\dfrac{\sigma^2}{\epsilon^2}\end{align*}	
\end{prf}
\begin{cor}{}
	The sample mean $\hat\mu_n=\dfrac{1}{n}\dsst\sum_{i=1}^nW_i$ is a consistent estimator for $\E(W)=\mu$, provided that the population $W$ has finite mean $\mu$ and variance $\sigma^2$.	
\end{cor}
\begin{prop}{}
	If $\hat\theta_n$ is an unbiased estimator of $\theta$, then $\hat\theta_n$ is consistent if \[\lim_{n\to\infty}\Var\qty(\hat\theta_n)=0.\]	
\end{prop}
\begin{prf}
	Suppose $\hat\theta_n$ is an unbiased estimator of $\theta$. Then, $\E\qty(\hat\theta_n)=\theta$. So, by Chebyshev Inequality, we have \[\P\qty(\abs{\hat\theta_n\theta}\geq\epsilon)=\P\qty(\abs{\hat\theta_n-\E\qty(\hat\theta_n)}\geq\epsilon)\leq\dfrac{\E\qty[\qty(\hat\theta_n-\E\qty(\hat\theta_n))^2]}{\epsilon^2}=\dfrac{\Var\qty(\hat\theta_n)}{\epsilon^2}.\] If we have $\Var\qty(\hat\theta_n)\to0$ when $n\to\infty$, then \[\lim_{n\to\infty}\P\qty(\abs{\hat\theta_n-\theta}\geq\epsilon)\leq\lim_{n\to\infty}\dfrac{\Var\qty(\hat\theta_n)}{\epsilon^2}=\dfrac{0}{\epsilon}=0.\] Therefore, it must be that $\dsst\lim_{n\to\infty}\P\qty(\abs{\hat\theta_n-\theta}\geq\epsilon)=0$ as probability cannot take negative values. Hence, \begin{align*}\lim_{n\to\infty}\P\qty(\abs{\hat\theta_n-\theta}<\epsilon)&=\lim_{n\to\infty}\qty(1-\P\qty(\abs{\hat\theta_n-\theta}\geq\epsilon))\\&=1-\lim_{n\to\infty}\P\qty(\abs{\hat\theta_n-\theta}\geq\epsilon)\\&=1-0=1.\end{align*} Then, by definition, $\hat\theta_n$ is consistent. 
\end{prf}


\end{document}
\include{preamble}

\title{Emory University\\\textbf{MATH 362 Mathematical Statistics II}\\ Learning Notes}
\author{Jiuru Lyu}
\date{\today}

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Estimation}
\subsection{Introduction}
\begin{df}{Model}
	A \textit{model} is a distribution with certain parameters.	
\end{df}
\begin{eg}
	The normal distribution: $N(\mu,\sigma^2)$.	
\end{eg}
\begin{df}{Population}
	The \textit{population}	is all the objects in the experiment.
\end{df}
\begin{df}{Data, Sample, and Random Sample}
	\textit{Data} refers to observed value from sample. The \textit{sample}	is a subset of the population. A \textit{random sample} is a sequence of independent, identical ($\iid$) random variables.
\end{df}
\begin{df}{Statistics}
	\textit{Statistics} refers to a function of the random sample.
\end{df}
\begin{eg}
	The sample mean is a function of the sample: \[\bar{Y}=\dfrac{1}{n}\qty(Y_1+\cdots+Y_n).\]	
\end{eg}
\begin{eg} 
	Central Limit Theorem\par 
	We randomly toss $n=200$ fair coins on the table. Calculate, using the central limit theorem, the probability that at least $110$ coins have turned on the same side. 
	\[\bar{X}=\dfrac{X_1+\cdots+X_{200}}{200}\quad\stackrel{\text{CLT}}{\sim}\quad N\qty(\mu,\sigma^2),\] where \[\mu=\E\qty(\bar{X})=\dfrac{\dsst\sum_{i=1}^{200}\E\qty(X_i)}{200},\] \[\sigma^2=\Var\qty(\bar{X})=\Var\qty(\dfrac{X_1+\cdots+X_{200}}{200})=\dfrac{\dsst\sum_{i=1}^{200}\Var\qty(X_i)}{200^2}.\]
\end{eg}
\begin{df}{Statistical Inference}
	The process of \textit{statistical inference} is defined to be the process of using data from a sample to gain information about the population.	
\end{df}
\begin{eg}
	Goals in statistical inference
	\begin{enumerate}
		\item \begin{df}{Estimation} To obtain values of the parameters from the data. \end{df}
		\item \begin{df}{Hypothesis Testing} To test a conjecture about the parameters. \end{df}
		\item \begin{df}{Goodness of Fit} How well does the data fit a given distribution. \end{df}
		\item Linear Regression
	\end{enumerate}	
\end{eg}


\subsection{The Method of Maximum Likelihood and the Method of Moments}
\begin{eg}
	Given an unfair coin, or $p$-coin, such that \[X=\begin{cases}1\quad\text{head with probability }p,\\0\quad\text{tail with probability }1-p.\end{cases}\]	How can we determine the value $p$?
	\begin{sol}
		\begin{enumerate}
			\item Try to flip the coin several times, say, three times. Suppose we get HHT. 
			\item Draw a conclusion from the experiment. 
		\end{enumerate}	
		\textbf{Key idea: The choice of the parameter $p$ should be the value that maximizes the probability of the sample.}
		\[\P\qty(X_1=1,X_2=1,X_3=0)=\P\qty(X_1=1)\P(X_2=1)\P(X_3=0)=p^2(1-p)\coloneqq f(p).\] Solving the optimization problem $\dsst\max_{p>0}f(p)$, we find it is most likely that $p=\dfrac{2}{3}$. This method is called the \textit{likelihood maximization method}.
	\end{sol}
\end{eg}
\begin{df}{Likelihood Function}
	For a random sample of size $n$ from the discrete (or continuous) pdf $p_X(k;\theta)$ (or $f_Y(y;\theta)$), the \textit{likelihood function}, $\L(\theta)$, is the product of the pdf evaluated at $X_i=k_i$ (or $Y_i=y_i$). That is, \[\L(\theta)\coloneqq\prod_{i=1}^np_X\qty(k_i;\theta)\quad\text{or}\quad \L(\theta)\coloneqq\prod_{i=1}^nf_Y\qty(y_i;\theta).\]	
\end{df}
\begin{df}{Maximum Likelihood Estimate}
	Let $\L(\theta)$ be as defined in Definition 1.2.2. If $\theta_e$ is a value of the parameter such that $\L\qty(\theta_e)\geq \L(\theta)$ for all possible values of $\theta$, then we call $\theta_e$ the \textit{maximum likelihood estimate} for $\theta$.
\end{df}
\begin{thm}{The Method of Maximum Likelihood}
	Given random samples $X_1,\dots,X_N$ and a density function $p_X(x)$ (or $f_X(x)$), then we have the likelihood function defined as \begin{align*}\L(\theta)=p_X(X;\theta)&=\P(X_1,X_2,\dots,X_N)\\&=\P(X_1)\P(X_2)\cdots\P(X_N)&[independent]\\&=\prod_{i=1}^Np_X(X_i;\theta)&[identical]\end{align*} Then, the maximum likelihood estimate for $\theta$ is given by \[\theta^*=\argmax_\theta L(\theta),\] where \[\L\qty(\argmax_\theta L(\theta))=\L^*(\theta)=\max_\theta \L(\theta).\]
\end{thm}
\begin{eg}
	Consider the Poisson distribution $X=0,1,\dots,$ with $\lambda>0$. Then, the pdf is given by \[p_X(k,\lambda)=e^{-\lambda}\dfrac{\lambda^k}{k!},\quad k=0,1,\dots\] Given data $k_1,\dots,k_n$, we have the likelihood function \[\L(\lambda)=\prod_{i=1}^np_X\qty(X=k;\lambda)=\prod_{i=1}^ne^{-\lambda}\dfrac{\lambda^{k_i}}{k_i!}=e^{-n\lambda}\dfrac{\lambda^{\sum{k_i}}}{k_1!\cdots k_n!}\] Then, to find the maximum likelihood estimate of $\lambda$, we need to $\dsst\max_{\lambda}\L(\lambda)$. That is to solve $\dsst\pdv{\L(\lambda)}{\lambda}=0$ and $\dsst\pdv[2]{\L(\lambda)}{\lambda}<0$.
\end{eg}
\begin{eg}
	Waiting Time. \par 
	Consider the exponential distribution $f_Y(y)=\lambda e^{-\lambda y}$ for $y\geq 0$. Find the MLE $\lambda_e$ of $\lambda$.
	\begin{sol}
		The likelihood function of the exponential distribution is given by
		\[\L(\lambda)=\prod_{i=1}^n\lambda e^{-\lambda y_i}=\lambda^n\exp\qty(-\lambda\sum_{i=1}^ny_i).\] Now, define \[\l(\lambda)=\ln\L(\lambda)=n\ln\lambda-\lambda\sum_{i=1}^ny_i.\] To optimize $\l(\lambda)$, we compute \[\dv{\lambda}\l(\lambda)=\dfrac{n}{\lambda}-\sum_{i=1}^ny_i\stackrel{set}{=}0\] So, \[\dfrac{n}{\lambda}=\sum_{i=1}^ny_i\implies\lambda_e=\dfrac{n}{\dsst\sum_{i=1}^ny_i}\eqqcolon\dfrac{1}{\bar{y}},\] where $\bar{y}$ is the sample mean. 
	\end{sol}
\end{eg}
\begin{eg}
	Given the exponential distribution $f_Y(y)=\lambda e^{-\lambda y}$ for $y\geq0$. Find the MLE of $\lambda^2$.
	\begin{sol}
		Define $\tau=\lambda^2$. Then, $\lambda=\sqrt{\tau}$, and so \[f_Y(y)=\sqrt{\tau}e^{-\sqrt{\tau}y},\quad y\geq0.\] Then, the likelihood function becomes \[\L(\tau)=\prod_{i=1}^nf_Y(y)=\tau^{\frac{n}{2}}\exp\qty(-\sqrt{\tau}\sum_{i=1}^ny_i).\] Similarly, after maximization, we find \[\tau_e=\dfrac{1}{\qty(\bar{y})^2}.\]
	\end{sol}
\end{eg}
\begin{thm}{Invariant Property for MLE}
	Suppose $\lambda_e$	is the MLE of $\lambda$. Define $\tau\coloneqq h(\lambda)$. Then, $\tau_e=h(\lambda_e)$.
\end{thm}
\begin{prf}
	In this proof, we will prove the case when $h$ is a one-to-one function. The case of $h$ being a many-to-one function is beyond the scope of this course. \par 
	Suppose $h(\cdot)$ is a one-to-one function. Then, $\lambda=h^{-1}(\tau)$ is well-defined. Then, \[\max_{\lambda}\L(\lambda;y_1,\dots,y_n)=\max_{\tau}\L\qty(h^{-1}(\tau);y_1,\dots,y_n)=\max_{\tau}\L(\tau;y_1,\dots,y_n).\]	
\end{prf}
\begin{eg}
	Waiting Time with an unknown Threshold.\par 
	Let $\lambda=1$ in exponential but there is an unknown threshold $\theta$, that, is $f_Y(y)=e^{-(y-\theta)}$ for $y\geq\theta,\ \theta>0$.	
	\begin{sol}
		Note that the likelihood function is given by \begin{align*}\L(\theta;y_1,\dots,y_n)=\prod_{i=1}^nf_Y(y_1)&=\exp\qty(-\sum_{i=1}^n(y_i-\theta)),\quad y_i\geq\theta,\ \theta>0\\&=\exp\qty(-\sum_{i=1}^n(y_i-\theta))\cdot\1_{\qty[y_i\geq0,\ \theta>0]},\end{align*} where \[\1_{x\in A}=\begin{cases}1\quad\text{if }x\in A\\0\quad\text{if }x\notin A.\end{cases}\] Using order statistics, \begin{align*}\L(\theta)&=\exp\qty(-\sum_{i=1}^n(y_i-\theta))\cdot\1_{\qty[y_{(n)}\geq y_{(n-1)}\geq\cdots\geq y_{(1)}\geq\theta,\ \theta>0]}\\&=\exp\qty(-\sum_{i=1}^ny_i+n\theta)\1_{\qty[y_{(n)}\geq\cdots\geq y_{(1)}\geq\theta,\ \theta>0]}.\end{align*}
		So, we know $\theta\leq y_{(1)}=y_{\min}$. \par 
		To maximize the likelihood function, we want to maximize $-\sum y_i+n\theta$. That is, to maximize $\theta$, as $\theta\leq y_{\min}$, it must be that $\theta_{\max}=y_{\min}$. Therefore, the MLE is $\theta^*=y_{\min}.$
	\end{sol}
\end{eg}
\begin{eg}
	Suppose $Y_1,\dots,Y_n\sim\text{Uniform}[0,a]$. That is, $f_Y(y;a)=\dfrac{1}{a}$ for $y\in[0,a]$. Find MLE $a_e$ of $a$. 
	\begin{sol}
		Note that \begin{align*}f_Y(y;a)&=\dfrac{1}{a}\cdot\1_{\qty{y\in[0,a]}}\\&=\dfrac{1}{a}\cdot\1_{\qty{0\leq y_{(1)}\leq\cdots\leq y_{(n)}\leq a}}&\text{where } y_{(1)}=\min y_i\text{ and } y_{(n)}=\max y_i\end{align*} Then, \[\L(a)=\dfrac{1}{a^n}\1_{\qty{0\leq y_{(1)}\leq\cdots\leq y_{(n)}\leq a}}\] To maximize $\L(a)$, we want to minimize $a^n$. Since $a\geq y_{(n)}$, it must be that $a_e=y_{(n)}$. 
		Here, we call $a_e=y_{(n)}$ an \textit{estimate}, and $\hat{a_{\text{MLE}}}=Y_{(n)}$ an \textit{estimator}.
	\end{sol}
\end{eg}
\begin{eg}
	\textbf{MLE that Does Not Esist}
	\par Suppose $f_Y(y;a)=\dfrac{1}{a},\quad y\in[0, a)$. Find the MLE.
	\begin{sol}
		The likelihood function is the same: \[\L(a)=\dfrac{1}{a^n}\1_{\qty{0\leq y_{(1)}\leq\cdots\leq y_{(n)}<a}}.\] However, since $[0,a)$ is not a closed set, the optimization problem $\dsst\max_{a\in[0,a)}\L(a)$ does not have a solution. Hence, the estimate does not exist. 
	\end{sol}
\end{eg}
\begin{rmk}
	MLE may not be unique all the time.	
\end{rmk}
\begin{eg}
	\textbf{Multiple MLE Values}
	\par Suppose  $X_1,\dots,X_n\sim\text{Uniform}\qty[a-\dfrac{1}{2},a+\dfrac{1}{2}]$, where $f_X(x;a)=1,\ x\in\qty[a-\dfrac{1}{2}, a+\dfrac{1}{2}]$. Find the MLE. 
	\begin{sol}
		In the indicator function notation, we can rewrite the pdf to be \[f_X(x;a)=\1_{\qty{a-\frac{1}{2}\leq x\leq a+\frac{1}{2}}}=\1_\qty{a-\frac{1}{2}\leq x_{(1)}\leq\cdots\leq x_{(n)}\leq a+\frac{1}{2}}.\] So, the likelihood function will be \[\L(a)=\prod_{i=1}^nf_x(x_i;a)=\begin{cases}1,\quad a\in\qty[x_{(n)}-\dfrac{1}{2}, x_{(1)}+\dfrac{1}{2}]\\0,\quad \text{otherwise}.\end{cases}\] So, the $\L(a)$ will be maximized whenever $a\in\qty[x_{(n)}-\dfrac{1}{2}, x_{(1)}+\dfrac{1}{2}]$. Therefore, MLE can be any value in the range $\qty[x_{(n)}-\dfrac{1}{2}, x_{(1)}+\dfrac{1}{2}]$. Say, \[a_e=x_{(n)}-\dfrac{1}{2}\quad\text{or}\quad a_e=x_{(1)}-\frac{1}{2}\quad\text{or}\quad a_e=\dfrac{x_{(n)}-\frac{1}{2}+x_{(1)}+\frac{1}{2}}{2}=\dfrac{x_{(n)}+x_{(1)}}{2}.\]
	\end{sol}
\end{eg}
\begin{thm}{MLE for Multiple Parameters}
	In general, we have the likelihood function $\L(\theta)$, where $\theta=\qty(\theta_1,\dots,\theta_p)$. To find the MLE, we need \[\pdv{\L(\theta)}{\theta_i}=0\quad i=1,\dots,p,\] and the Hessian matrix \[\mqty(\dsst\pdv[2]{\L(\theta)}{\theta_i}{\theta_j})_{i,j=1,\dots,p}\coloneqq\mqty(\dsst\pdv[2]{\L(\theta)}{\theta_1}&\cdots&\dsst\pdv[2]{\L(\theta)}{\theta_1}{\theta_p}\\\vdots&\ddots&\vdots\\\dsst\pdv[2]{\L(\theta)}{\theta_p}{\theta_1}&\cdots&\dsst\pdv[2]{\L(\theta)}{\theta_p})\] should be negative dfinite. 
\end{thm}
\begin{eg}
	\textbf{MLE for Multiple Parameters: Normal Distribution}
	\par Suppose $Y_1,\dots,Y_n\sim N(\mu,\sigma)$. Then, \[f_{Y_i}(u;\mu,\sigma)=\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\qty(y_i-\mu)^2/\qty(2\sigma^2)}.\] Find the MLE for $\mu$ and $\sigma$.
	\begin{sol}
		The likelihood function will be \[\L(\mu,\sigma)=\prod_{i=1}^n\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\qty(y_i-\mu)^2/\qty(2\sigma^2)}.\] Then, we define \[\l(\mu,\sigma)=\ln \L(\mu,\sigma)=-\dfrac{n}{2}\ln2\pi-\dfrac{n}{2}\ln\sigma^2-\dfrac{1}{2}\qty(\sigma^2)^-1\sum_{i=1}^n\qty(y_i-\mu)^2.\] Set \[\begin{cases}\dsst\pdv{\l(\mu,\sigma)}{\mu}=0\quad\quad\text{\ding{172}}\\\dsst\pdv{\l(\mu,\sigma)}{\sigma}=0\qquad\text{\ding{173}}\end{cases}\] From \ding{172}, we have \begin{align*}\dfrac{1}{\sigma^2}\sum_{i=1}^n\qty(y_1-\mu)&=0\\\sum_{i=1}^ny_i&=n\mu\implies\boxed{\mu_e=\dfrac{\sum y_i}{n}=\bar{y}}\end{align*} From \ding{173}, by the invariant property of MLE, we instead set \begin{align*}\dsst\pdv{\l(\mu,\sigma)}{\sigma^2}&=0\\-\dfrac{n}{2}\cdot\dfrac{1}{\sigma^2}+\dfrac{1}{2}\qty(\dfrac{1}{\sigma^2})^2\sum_{i=1}^n\qty(y_i-\mu)^2&=0\\\dfrac{1}{2\sigma^2}\qty(-n+\dfrac{1}{\sigma^2}\sum_{i=1}^n\qty(y_i-\mu)^2)&=0\\-n\sigma^2+\sum_{i=1}^n\qty(y_i-\mu)^2&=0\qquad\qquad(\mu_e=\bar{y})\\\sum_{i=1}^n\qty(y_i-\bar{y})^2&=n\sigma^2\\\sigma_e^2=\dfrac{1}{n}\sum_{i=1}^n\qty(y_i-\bar{y})^2&\implies\boxed{\sigma_e=\sqrt{\dfrac{1}{n}\sum_{i=1}^n\qty(y_i-\bar{y})^2}}\end{align*}
	\end{sol}
\end{eg}

\subsection{The Method of Moment}
\begin{df}{Moment Generating Function}
	The \textit{Moment Generating Function (MGF)} is defined as \[\M_X(t)=\E\qty[e^{tX}],\] and it uniquely determines a probability distribution.
\end{df}
\begin{df}{Moment}
	The \textit{$k$-th order moment} of $X$ is $\E\qty[X^k]$.	
\end{df}
\begin{eg}{Meaning of Different Moments}
	\begin{itemize}
		\item $\E[X]$: location of a distribution
		\item $\E\qty[X^2]=\Var(X)-\E[X]^2$: width of a distribution
		\item $\E\qty[X^3]$: skewness -- positively skewed / negatively skewed
		\item $\E\qty[X^4]$: kurtosis / tailedness -- speed decaying to $0$.
	\end{itemize}	
\end{eg}
\begin{eg} 
	\textbf{Moment Estimate: Moments of Population and Sample}
	\begin{center}
	\begin{tabular}{c|c}
		\textbf{Population}&\textbf{Sample, $X_1,\dots,X_n$}\\\hline\\
		$\E[X]=\mu$&$\hat\mu=\bar X=\dfrac{X_1+\cdots+X_n}{n}$\\\\
		$\E\qty[X^2]=\mu^2+\sigma^2$&$\hat\mu^2+\hat\sigma^2=\dfrac{X_1^2+\cdots+X_n^2}{n}$\\
		$\vdots$&$\vdots$\\
		$\E\qty[X^k]$&$\dfrac{X_1^k+\cdots+X_n^k}{n}$
	\end{tabular}
	\end{center}
	\textbf{Rationale}: The population moments should be close to the sample moments. 
\end{eg}
\begin{eg}
	\begin{itemize}
		\item Consider $N(\mu,\sigma^2)$, where $\sigma$ is given. Estimate $\mu$.\par By the method of moment estimate, we have $\mu_e=\bar{X}$.
		\item Consider $N(\mu,\sigma^2)$. Estimate $\mu$ and $\sigma$.\par We have $\mu_e=\bar X$ and $\mu_e^2+\sigma_e^2=\dfrac{X_1^2+\cdots+X_n^2}{n}$.
		\item Consider $N(\theta,\sigma^2)$. Given $\E\qty(X^4)=3\sigma^4$, estimate $\mu$ and $\sigma$.\par We have $\mu_e=\bar X$, $\mu_e^2+\sigma_e^2=\dfrac{X_1^2+\cdots+X_n^2}{n}$, and $3\sigma^4=\dfrac{X_1^4+\cdots+X_n^4}{n}$. We have three equations but only two unknowns, then a solution is not guaranteed. So, we need some restrictions on this method (see Remark 1.2). 
	\end{itemize}
\end{eg}
\begin{thm}{Method of Moments Estimates}
	For a random sample of size $n$ from the discrete (or continuous) population/pdf $p_X(k;\theta_1,\dots,\theta_s)$ (or $f_Y(y;\theta_1,\dots,\theta_s)$), solutions to the system \[\begin{cases}\E(Y)=\dfrac{1}{n}\dsst\sum_{i=1}^ny_i\\\qquad\vdots\\\E\qty(Y^s)=\dfrac{1}{n}\dsst\sum_{i=1}^ny_i^s\end{cases}\] which are denoted by $\theta_{1e},\dots,\theta_{se}$, are called the \textbf{method of moments estimates} of $\theta_1,\dots,\theta_s$.
\end{thm}
\begin{rmk}
	To estimate $k$ parameters with the method of moments estimates, we will only match the first $k$ orders of moments. 
\end{rmk}
\begin{eg}
	Consider the Gamma distribution: \[f_Y(y;r,\lambda)=\dfrac{\lambda^r}{\Gamma(r)}y^{r-1}e^{-\lambda y}\quad\text{for }y\geq0.\] Given $\E(Y)=\dfrac{r}{\lambda}$ and $\E\qty(Y^2)=\dfrac{r}{\lambda^2}+\dfrac{r^2}{\lambda^2}$. Estimate $r$ and $\lambda$.
	\begin{sol}
		\[\E\qty(Y)=\dfrac{r}{\lambda}\implies\dfrac{r_e}{\lambda_e}=\dfrac{y_1+\cdots+y_n}{n}=\bar{y}\qquad\text{\ding{172}}\] \[\E\qty(Y^2)=\dfrac{r}{\lambda^2}+\dfrac{r^2}{\lambda^2}\implies\dfrac{r_e}{\lambda_e^2}+\dfrac{r_e^2}{\lambda_e^2}=\dfrac{y_1^2+\cdots+y_n^2}{n}\qquad\text{\ding{173}}\] Substitute \ding{172} into \ding{173}, we have \[\dfrac{\bar{y}}{\lambda_e}+\qty(\bar{y})^2=\dfrac{1}{n}\sum_{i=1}^ny_i^2\implies \boxed{\lambda_e=\dfrac{\bar{y}}{\frac{1}{n}\sum y_i^2-\bar{y}^2}}\qquad\text{\ding{174}}\] Substitute \ding{174} into \ding{172}, we have \[r_e=\bar{y}\lambda_e=\boxed{\dfrac{\bar{y}^2}{\frac{1}{n}\sum y_i^2-\bar{y}^2}}.\]
	\end{sol}
\end{eg}
\begin{rmk}
	The \emph{sample variance} is defined as \begin{align*}\dfrac{1}{n}\sum_{i=1}^n\qty(y_i-\bar{y})^2&=\dfrac{1}{n}\sum_{i=1}^n\qty(y_i^2-2y_i\bar{y}+\bar{y}^2)\\&=\dfrac{1}{n}\sum_{i=1}^ny_i^2-2\bar{y}\cdot\dfrac{\sum y_i}{n}+\dfrac{1}{n}\cdot n\bar{y}^2\\&=\dfrac{1}{n}\sum_{i=1}^ny_i^2-2\bar{y}^2+\bar{y}^2&\bar{y}=\dfrac{\sum y_i}{n}\\&=\dfrac{1}{n}\sum_{i=1}^ny_i^2-\bar{y}^2.\end{align*} So, in Example 1.3.7, if we define $\hat{\sigma}^2$ to be the sample variance, we can further simply our estimate as follows: \[\lambda_e=\dfrac{\bar{y}}{\hat{\sigma}^2},\qquad r_e=\dfrac{\bar{y}^2}{\hat{\sigma}^2}.\]
\end{rmk}

\subsection{Interval Estimation}
\begin{eg}{}
	Estimate $\mu$, where $X\sim N(\mu, 1)$.\par 
	We take some samples and compute their sample means: \[\bar{X}^1=\dfrac{x_1+\cdots+x_n}{n},\bar{X}^2=\dfrac{\widetilde{x_1}+\cdots+\widetilde{x_n}}{n},\cdots\] Finding the distribution of $\bar{X}$, we can find an interval $\qty[\hat\theta_L,\hat\theta_U]$ such that \[\P\qty(\hat\theta_L\leq\bar X\leq\hat\theta_U)=1-\alpha.\]
\end{eg}
\begin{rmk}
	By using the variance of the estimator, one can construct an interval such that with a high probability that the interval contains the unknown parameter. 	
\end{rmk}
\begin{df}{Confidence Interval}
	The interval, $\qty[\hat\theta_L,\hat\theta_U]$	is called the \textit{confidence interval}, and the high probability is $1-\alpha$, where $\alpha$ is given. 
\end{df}
\begin{rmk}
	Take $\alpha=5\%$, then $\qty[\hat\theta_L,\hat\theta_U]$ is the $95\%$ confidence interval of $\mu$. It does not mean that $\mu$ has $95\%$ chance to be in $\qty[\hat\theta_L,\hat\theta_U]$. However, if we construct $1000$ such intervals, $950$ of them will contain $\mu$.
\end{rmk}
\begin{eg}
	A random sample of size $4$, $\qty(Y_1=6.5,\ Y_2=9.2,\ Y_3=9.9,\ Y_4=12.4)$, from a normal population: \[f_Y(y;\mu)=\dfrac{1}{\sqrt{2\pi}0.8}e^{-\dfrac{1}{2}\qty(\dfrac{y-\mu}{0.8})^2}\sim N(\mu, \sigma^2=0.64).\] Both MLE and MME give $\mu_e=\bar y=9.5$. The estimator $\hat{\mu}=\bar{Y}$ follows normal distribution. Construct $95\%$-confidence interval for $\mu$.
	\begin{sol}
		$\E\qty(\bar Y)=\mu$ and $\Var\qty(\bar Y)=\dfrac{\sigma^2}{n}=\dfrac{0.64}{4}$. By the Central Limit Theorem, $\bar{Y}$ approximately follow $N\qty(\mu,\dfrac{\sigma^2}{n})$. So, $\dfrac{\bar Y-\mu}{\sqrt{\frac{\sigma^2}{n}}}\sim N(0,1)$. Then, \[\P\qty(z_1\leq\dfrac{\bar Y-\mu}{\sqrt{\frac{\sigma^2}{n}}}\leq z_2)=0.95\implies\P\qty(\bar Y-z_2\sqrt{\dfrac{\sigma^2}{n}}\leq\mu\leq\bar Y-z_1\sqrt{\dfrac{\sigma^2}{n}})=0.95\] There are infinite many ways to construct a confidence interval by selecting different $z_1$ and $z_2$. However, since we don't have any prior knowledge on $\mu$, it is good for us to choose $z_1$ and $z_2$ symmetrically. Moreover, symmetric $z_1$ and $z_2$ will yield a smaller interval. We know the symmetric $z_1,z_2$ pair will be $z_1=-1.96$ and $z_2=1.96$. Therefore, \[\P\qty(\bar Y-1.96\sqrt{\dfrac{0.64}{4}}\leq\mu\leq\bar Y+1.96\sqrt{\dfrac{0.64}{4}})=0.95.\] Then, $95\%$ confidence interval is $\qty[9.5-1.96\times0.4,\ 9.5+1.96\times0.4]$. 
	\end{sol}
\end{eg}
\begin{thm}{Confidence Interval}
	In general, for a normal population with $\sigma$ known, the $100(1-\alpha)\%$ \textit{two-sided confidence interval} for $\mu$ is \[\qty(\bar{y}-z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}},\ \bar{y}+z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}})\]
\end{thm}
\begin{thm}{Variation of Confidence Interval}
	\begin{itemize}
		\item One-sided interval: \[\qty(\bar{y}-z_{\alpha}\dfrac{\sigma}{\sqrt{n}},\ \bar{y})\text{ or }\qty(\bar{y},\ \bar{y}+z_{\alpha}\dfrac{\sigma}{\sqrt{n}})\]
		\item $\sigma$ is unknown and sample size is small: $z$-score $\to$ $t$-score.
		\item $\sigma$ is unknown and sample size is large: $z$-score by CLT.
		\item Non Gaussian population but sample size is large: $z$-score by CLT. 
	\end{itemize}	
\end{thm}
\begin{thm}{}
	Let $k$ be the number of successes in $n$ independent trials, where $n$ is large and $p=\P(\text{success})$ is unknown. An approximate $100(1-\alpha)\%$ confidence interval for $p$ is the set of numbers \[\qty(\dfrac{k}{n}-z_{\alpha/2}\sqrt{\dfrac{(k/n)(1-k/n)}{n}},\ \dfrac{k}{n}+z_{\alpha/2}\sqrt{\dfrac{(k/n)(1-k/n)}{n}}).\]
\end{thm}
\begin{df}{Margin of Error}
	The \textit{margin of error}, denoted by $d$, is the quantity\[d=z_{\alpha/2}\sqrt{\dfrac{(k/n)(1-k/n)}{n}}.\]	
\end{df}
\begin{rmk}
	Stating the sample mean and the margin of error is equivalent to stating the confidence interval. Note that $\operatorname{C.I.}=\hat p\pm d$.	
\end{rmk}
\begin{thm}{Estimate Margin of Error}
	When $p$ is close to $\dfrac{1}{2}$, then $d\approx d_m=\dfrac{z_{\alpha/2}}{2\sqrt{n}}$, which is 	equivalent to $\sigma_n\approx\dfrac{1}{2\sqrt{n}}$. However, if $p$ is away from $\dfrac{1}{2}$, $d$ and $d_m$ are very different.
\end{thm}
\begin{rmk}
	Theorem 1.4.8 gives aconservative estimation of the margin of error, which is $d_m$.	
\end{rmk}
\begin{prop}{}
	Given $d$, we can estimate the sample size. 
	\begin{prf}
		\[d=z_{\alpha/2}\sqrt{\dfrac{\hat{p}(1-\hat{p})}{n}}\implies n\approx\hat{p}(1-\hat{p})/\qty(\dfrac{d}{z_{\alpha/2}})^2.\] However, since $n$ is unknown, $\hat{p}$ is also unknown. We, therefore, need information on the actual $p$ to conclude an estimation of the sample size.
		\begin{itemize}
			\item If $p$ is known, \[n=\dfrac{p(1-p)}{\qty(\frac{d}{z_{\alpha/2}})^2}.\]
			\item If $p$ is unknown. Let $f(p)=p(1-p)$. $f$ will be maximized when $p=0.5$. So, $f(p)=p(1-p)\leq0.25$. Then, \[n\leq\dfrac{0.25}{\qty(\frac{d}{z_{\alpha/2}})^2}.\] Since we are conservative, take $n=\dfrac{\frac{1}{4}z_{\alpha/2}^2}{d^2}=\dfrac{z_{\alpha/2}^2}{4d^2}$. This estimation is a conservative estimation of the sample size. 
		\end{itemize}
	\end{prf}
\end{prop}

\subsection{Properties of Estimation}
The main question is that estimators are not unique in general. How do we choose a good estimator? 
\begin{df}{Unbiasedness}
	Given a random sample of size $n$ when whose population distribution depends on an unknown parameter $\theta$. Let $\hat\theta$ be an estimator of $\theta$. Then, 
	\begin{itemize}
		\item $\hat\theta$ is called \textit{unbiased} if $\E(\hat\theta)=\theta$.
		\item $\hat\theta$ is called \textit{asymptotically unbiased} if $\dsst\lim_{n\to\infty}\E(\hat\theta)=\theta$.
		\item If $\theta$ is biased, then the \textit{bias} is given by the quantity $\B(\hat\theta)=\E(\hat\theta)-\theta$.
	\end{itemize}
\end{df}
\begin{eg}
	Consider the exponential distribution: $f_Y(y;\lambda)=\lambda e^{-\lambda y}$ for $y\geq0$. Determine if the estimator $\hat\lambda=\dfrac{1}{\bar Y}$ is biased or not. \newline\textit{Hint: $n\bar Y=\dsst\sum_{i=1}^nY_i\sim\text{Gamma}(n,\lambda)$.}
	\begin{sol}
		Recall that $\E[g(x)]\dsst=\int_xg(x)f_X(x)\ \d{x}$. Define $X=\dsst\sum_{i=1}^nY_i\sim\text{Gamma}(n,\lambda)$. Also, recall the following facts: \[\Gamma(n)=(n-1)!=(n-1)\Gamma(n-1)\] and the integration over any probability density function will yield a result of $1$ by definition. \par  Then, \begin{align*}\E(\hat\lambda)=\E\qty(\dfrac{1}{\bar Y})=\E\qty(\dfrac{n}{\sum Y_i})&=n\E\qty(\dfrac{1}{\sum Y_i})\\&=n\E\qty(\dfrac{1}{X})\\&=n\int_x\dfrac{1}{x}\cdot\dfrac{\lambda^n}{\Gamma(n)}x^{n-1}e^{-\lambda x}\ \d{x}\\&=n\int_x\dfrac{\lambda^n}{(n-1)!}x^{n-2}e^{-\lambda x}\ \d{x}\\&=\dfrac{n\lambda}{(n-1)}\underbrace{\int_x\dfrac{\lambda^{n-1}}{\Gamma(n-1)}x^{n-2}e^{-\lambda x}\ \d{x}}_{=1}\\&=\dfrac{n}{n-1}\lambda.\end{align*} Therefore, $\E(\hat\lambda)\neq\lambda$, and so $\hat\lambda$ is biased. However, note that \[\lim_{n\to\infty}\E(\hat\lambda)=\lim_{n\to\infty}\dfrac{n}{n-1}\lambda=\lambda.\] By definition, then $\hat\lambda$ is asymptotically unbiased. 
	\end{sol}
\end{eg}
\begin{eg}
	Consider the exponential distribution $f(y;\theta)=\dfrac{1}{\theta}e^{-y/\theta}$ for $y\geq0$. Then, $\hat\theta=\bar Y$ is unbiased. 	
\end{eg}
\begin{rmk}
	Suppose $\qty{X_1,\dots,X_n}$ are $\iid$ random variables, and $\E(X_i)=\mu$ for $i=1,\dots,n$	. Then, $\bar X$, the sample mean, is always an unbiased estimator: \[\E(\bar X)=\E\qty(\dfrac{1}{n}\sum_{i=1}^nX_i)=\dfrac{1}{n}\sum_{i=1}^n\E(X_i)=\dfrac{1}{n}\cdot n\cdot \mu=\mu.\]
\end{rmk}
\begin{thm}{Sample Variance is Biased}
	Suppose $\qty{X_1,\dots,X_n}$ are $\iid$ random variables, and $\E(X_i)=\mu,\ \Var(X_i)=\sigma^2$ for $i=1,\dots,n$. Then, the sample variance $\dsst\hat\sigma^2=\dfrac{1}{n}\sum_{i=1}^n\qty(X_i-\bar X)^2$ is biased. 
\end{thm}
\begin{prf}
	Note that \begin{align*}
		\E(\hat\sigma^2)&=\E\qty(\dfrac{1}{n}\sum_{i=1}^n(X_i-\bar X)^2)\\&=\E\qty(\dfrac{1}{n}\sum_{i=1}^n\qty(X_i-\mu+\mu-\bar{X})^2)\\&=\dfrac{1}{n}\sum_{i=1}^n\E\qty[\qty(X_i-\mu)^2+\qty(\mu-\bar X)^2+2\qty(X_i-\mu)(\mu-\bar X)]\\&=\dfrac{1}{n}\sum_{i=1}^n\qty{\underbrace{\E(X_i-\mu)^2}_{\Var(X_i)}+\E\qty(\mu-\bar X)^2+2\E\qty[(\mu-\bar X)(X_i-\mu)]}\\&\ \ \ \bigg|\quad\textit{Hint: }\dfrac{1}{n}\sum_{i=1}^n(X_i-\mu)=\dfrac{1}{n}\sum_{i=1}^nX_i-\dfrac{1}{n}\sum_{i=1}^n\mu=\bar X-\mu\\&=\dfrac{1}{n}\sum_{i=1}^n\Var(X_i)+\dfrac{1}{n}\cdot n\E\qty(\mu-\bar{X})^2+2\E\qty[(\mu-\bar X)\dfrac{1}{n}\sum_{i=1}^n\qty(X_i-\mu)]\\&=\dfrac{1}{n}\sum_{i=1}^n\sigma^2+\E\qty(\mu-\bar X)^2+2\E\qty[(\mu-\bar X)(\bar X-\mu)]\\&=\dfrac{1}{n}\cdot n\cdot\sigma^2+\E\qty(\mu-\bar X)^2-2\E\qty[(\mu-\bar X)^2]\\&=\sigma^2-\E\qty(\mu-\bar X)^2\\&=\sigma^2-\underbrace{\E\qty(\bar X-\mu)^2}_{=\Var(\bar X)}\\&=\sigma^2-\dfrac{\sigma^2}{n}=\dfrac{n-1}{n}\sigma^2\neq\sigma^2
	\end{align*}
	Therefore, $\hat\sigma^2$ is not an unbiased estimator. 
\end{prf}
\begin{thm}{Adjusted Sample Variance is Unbiased}
	With the same set up in Theorem 1.5.4, define the adjusted sample variance to be \[S^2=\dfrac{n}{n-1}\hat\sigma^2=\dfrac{1}{n-1}\sum_{i=1}^n\qty(X_i-\bar X)^2.\] Then, $S^2$ is an unbiased estimator of $\sigma^2$.
\end{thm}

\begin{df}{Decision Theory}
	Minimize the error of an estimator (sample statistics) relative to the true parameter (population parameter) using a loss function.
\end{df}
\begin{df}{Mean Squared Error}
	The \textit{mean squared error} (MSE) is defined by \[\MSE(\hat\theta)=\E\qty[(\hat\theta-\theta)^2]\]	
\end{df}
\begin{thm}{Decomposition of MSE}
	Generally, \[\MSE(\theta)=\Var(\hat\theta)+\B\qty(\hat\theta)^2\] If $\hat\theta$ is unbiased, $\MSE(\hat\theta)=\Var(\hat\theta)$. $\Var(\theta)$ measures the precision of the estimator. 
\end{thm}
\begin{prf}
	Note that we will the following: \begin{align*}
		\MSE(\hat\theta)&=\E\qty[(\hat\theta-\theta)^2]\\&=\E(\hat\theta^2+\theta^2-2\hat\theta\theta)\\&=\E(\hat\theta)-2\theta\E(\hat\theta)+\theta^2\\&=\underbrace{\E(\hat\theta^2)-\E(\hat\theta)^2}+\underbrace{\E(\hat\theta)^2-2\theta\E(\hat\theta)+\theta^2}\\&=\Var(\hat\theta)+\qty[\E(\hat\theta)-\theta]^2\\&=\Var(\theta)+\B(\hat\theta)^2
	\end{align*}	
	If $\hat\theta$ is unbiased, $\B(\hat\theta)=0$, and so $\MSE(\hat\theta)=\Var(\hat\theta)$.
\end{prf}
\begin{df}{Efficiency}
	Let $\hat\theta_1$ and $\hat\theta_2$ be two unbiased estimators for a parameter $\theta$. If we have $\Var(\hat\theta_1)<\Var(\hat\theta_2)$, then we say that $\hat\theta_1$ is \textit{more efficient} than $\hat\theta_2$. The \textit{relative efficiency} of $\hat\theta_1$ with respect to $\hat\theta_2$ is the ratio $\dfrac{\Var(\hat\theta_2)}{\Var(\hat\theta_1)}$.
\end{df}

\subsection{Best Unbiased Estimator}
\begin{df}{Best/Minimum-Variance Estimator}
	Let $\Theta$ be the set of all estimators $\hat\theta$ that are unbiased for the parameter $\theta$. We way that $\hat\theta^*$ is a \textit{best} or \textit{minimum-variance estimator} (MVE) if $\hat\theta^*\in\Theta$ and $\Var(\hat\theta^*)\leq\Var(\hat\theta)\quad\forall\ \hat\theta\in\Theta$.
\end{df}
\begin{df}{Fisher's Information}
	The \textit{Fisher's information} of a continuous random variable $Y$ with pdf $f_Y(y;\theta)$ is defined as \[\I(\theta)=\E\qty[\qty(\pdv{\ln f_Y(y;\theta)}{\theta})^2]=-\E\qty[\pdv[2]{\theta}\ln f_Y(y;\theta)].\]
\end{df}
\begin{rmk}
	The Fisher's information measures the amount of information that a sample $Y$ contains about the unknown parameter $\theta$. If $\I(\theta)$ is big, then the curvature of $f_Y(y;\theta)$ is big, and thus it is more likely that we can find a region where $\hat\theta$ is concentrated. 
\end{rmk}
\begin{ext}[Joint Fisher's Information]
	Suppose	$Y_1,\dots,Y_n$ are continuous $\iid$ random variables, each has a Fisher's information of $\I(\theta)$. Then, \[\E\qty[\qty(\pdv{\theta}\ln f_{Y_1,\dots,Y_n}(y_1,\dots,y_n;\theta))^2]=n\I(\theta).\]
\end{ext}
\begin{thm}{Properties of Fisher's Information}
	Define the \textit{Fisher's Score Function}	$\dsst\pdv{\theta}\ln f_Y(y;\theta)$. Then, \[\E_Y\qty[\pdv{\theta}\ln f_Y(y;\theta)]=0.\]
\end{thm}
\begin{prf}
	Note that by chain rule, we have \begin{align*}
		\E_Y\qty[\pdv{\theta}\ln f_Y(y;\theta)]&=\int_Y\qty(\pdv{\theta}\ln f_Y(y;\theta))f_Y(y;\theta)\ \d{y}\\&=\int_Y\dfrac{1}{f_Y(y;\theta)}\qty(\pdv{\theta}f_Y(y;\theta))f_Y(y;\theta)\ \d{y}\\&=\int_Y\pdv{\theta}f_Y(y;\theta)\ \d{y}\\&=\pdv{\theta}\int_Yf_Y(y;\theta)\ \d{y}=\pdv{\theta}(1)=0.
	\end{align*}
\end{prf}
\begin{cor}{}
	\[\I(\theta)=\Var\qty(\pdv{\theta}\ln f_Y(y;\theta)).\]
\end{cor}
\begin{prf}
	By definition, we have \begin{align*}
		\Var\qty(\pdv{\theta}\ln f_Y(y;\theta))&=\E\qty[\qty(\pdv{\theta}\ln f_Y(y;\theta))^2]-\qty(\underbrace{\E\qty(\pdv{\theta}\ln f_Y(y;\theta))}_{=0,\text{ by Theorem 1.6.3.}})^2\\&=\E\qty[\qty(\pdv{\theta}\ln f_Y(y;\theta))^2]\\&=\I(\theta).
	\end{align*}
\end{prf}
\begin{thm}{Cramér-Rao Inequality}
	Under regular condition, let $Y_1,\dots,Y_n$ be a random sample of size $n$ form the continuous population pdf $f_Y(y;\theta)$. Let $\hat\theta=\hat\theta(Y_1,\dots,Y_n)$ be any unbiased estimator for $\theta$. Then, \[\Var(\hat\theta)\geq\dfrac{1}{n\I(\theta)}.\]
\end{thm}
\begin{rmk} A similar statement holds for the discrete case $p_X(k;\theta)$. \end{rmk}
\begin{df}{Efficiency of Unbiased Estimator}
	An unbiased estimator $\hat\theta$ is \textit{efficient} if $\Var(\hat\theta)$ is equal to the Cramér-Rao lower bound. That is, $\Var(\hat\theta)=\qty(n\I(\theta))^{-1}$. Such an estimator is the MVE defined in Definition 1.6.1. The \textit{efficiency} of an unbiased estimator $\hat\theta$ is defined to be the quantity \[\qty(n\I(\theta)\Var(\hat\theta))^{-1}.\]
\end{df}
\begin{eg}
	Suppose $X\sim\text{Bernoulli}(p)$. Is $\hat p=\bar X$ efficient? 
	\begin{sol}
		Note that we have the following\begin{align*}
			f_X(x;p)&=p^x(1-p)^{1-x},\quad x=0,1\\\ln f_X(x;p)&=x\ln p+(1-x)\ln(1-p)\\\pdv{p}\ln f_X(x;p)&=\dfrac{x}{p}-\dfrac{1-x}{1-p}\\\pdv[2]{p}\ln f_X(x;p)&=-\dfrac{x}{p^2}-\dfrac{1-x}{(1-p)^2}\end{align*} 
		Therefore, the Fisher's information can be computed by \begin{align*}\I(p)=-\E\qty[\pdv[2]{p}\ln f_X(x;p)]&=-\E\qty[-\dfrac{x}{p^2}-\dfrac{1-x}{(1-p)^2}]\\&=\E\qty[\dfrac{x}{p^2}]+\E\qty[\dfrac{1-x}{(1-p)^2}]\\&=\dfrac{\E(x)}{p^2}+\dfrac{1-\E(x)}{(1-p)^2}\\&=\dfrac{p}{p^2}+\dfrac{1-p}{(1-p)^2}=\dfrac{1}{p}+\dfrac{1}{1-p}=\dfrac{1}{p(1-p)}.\end{align*} 
		Note that \[\Var(\bar X)=\Var\qty(\dfrac{1}{n}\sum_{i=1}^nX_i)=\dfrac{1}{n^2}\sum_{i=1}^n\Var(X_i)=\dfrac{1}{n}\Var(X_i)=\dfrac{1}{n}\cdot p(1-p).\] So, we have \[\Var(\bar X)=\dfrac{p(1-p)}{n}=\dfrac{1}{n\qty(\frac{1}{p(1-p)})}=\dfrac{1}{n\I(p)}.\] Therefore, $\hat p$ is efficient. 
	\end{sol}
\end{eg}
\begin{eg}
	Suppose $X\sim N(\mu,\sigma^2)$, with $\sigma^2$ is known. What is $\I(\mu)$?
	\begin{sol}
		Note that \[\dv[2]{\mu}\ln f_X(x;\mu)=-\dfrac{1}{\sigma^2}.\] Then, \[\I(\mu)=-\E\qty[\dv[2]{\mu}\ln f_X(x;\mu)]=-\E\qty[-\dfrac{1}{\sigma^2}]=\dfrac{1}{\sigma^2}.\]
	\end{sol}
\end{eg}

\subsection{Sufficiency}
\begin{rmk} Use Likelihood Function to Define Fisher's Information
\begin{itemize}
	\item We can define the \emph{score function} as $\dsst\pdv{\ln \L\qty(Y_1,\dots,y_n;\theta)}{\theta}=0\implies$ MLE.
	\item $\E\qty[\dsst\pdv{\ln\L(Y;\theta)}{\theta}]=0$
	\item $\I(\theta)=\E\qty[\dsst\qty(\pdv{\ln\L(Y;\theta)}{\theta})^2]=-\E_Y\qty[\dsst\pdv[2]{\ln\L(Y;\theta)}{\theta}]$
	\item $-\E_Y\qty[\dsst\pdv[2]{\ln\L(Y_1,\dots,Y_n;\theta)}{\theta}]=n\I(\theta)$.
	\begin{prf}
		\begin{align*}
			-\E_Y\qty[\dsst\pdv[2]{\ln\L(Y_1,\dots,Y_n;\theta)}{\theta}]&=-\E_Y\qty[\pdv[2]{\theta}\ln\L\qty(Y_1,\dots,Y_m;\theta)]\\&=-\E_Y\qty[\pdv[2]{\theta}\ln\qty(\prod_{i=1}^nf_Y\qty(Y_i;\theta))]\\&=-\E_Y\qty[\pdv[2]{\theta}\sum_{i=1}^nf_Y\qty(y_i;\theta)]=\sum_{i=1}^n\qty(-\E_Y\qty[\pdv[2]{\theta}f_Y(y_i;\theta)])=n\I(\theta)
		\end{align*}
	\end{prf}
	\item $\hat{\theta_\text{MLE}}\xrightarrow{n\to\infty}N\qty(\theta,\dfrac{1}{\I(\theta)})$. Note that $\dfrac{1}{\I(\theta)}$ is the C-R lower bound. We see that $\hat{\theta_\text{MLE}}$ is asymptotically efficient. 
\end{itemize}
\end{rmk}
\begin{rmk}[Sufficiency Intuition]
	Sufficiency tells us how much information can we get out of the data. 
	\begin{description}
		\item[Rationale] Let $\hat\theta$ be an estimator to the unknown parameter $\theta$. Does $\hat\theta$ contain all information about $\theta$? \emph{e.g., The data itself is a sufficient estimator. }
	\end{description}	
\end{rmk}
\begin{df}{Sufficiency}
	Let $(X_1,\dots,X_n)$ be a random sample of size $n$ from a continuous population with an unknown parameter $\theta$. We call $\theta$ is \textit{sufficient} if \[f_{Y_1,\dots,Y_n\mid\hat\theta}\qty(Y_1,\dots,Y_n\mid\hat\theta=\theta_e)=b(y_1,\dots,y_n),\] where $b(y_1,\dots,y_n)$ is independent of $\theta$ ($\independ\theta$). Also, $\hat\theta=h\qty(Y_1,\dots,Y_n)$ and $\theta_e=h\qty(y_1,\dots,y_n)$. In this case, $\hat\theta$ contains all the information about $\theta$ from $\qty{y_1,\dots,y_n}$.
\end{df}
\begin{eg}
	\begin{itemize}
		\item Toss a coin $5$ times and get $3$ heads. Estimate $p=$ probability of $H$.
		\begin{sol}
			\[\P\qty(HHHTT\mid p_e=\dfrac{3}{5})=\dfrac{1}{\dsst\binom{3}{5}}\independ p\implies\text{sufficient}\]
		\end{sol}
		\item A random sample of size $n$ from Bernoulli$(p)$. Check the sufficiency of $p=\dsst\sum_{i=1}^nX_i$.
		\begin{sol}
			Suppose the random sample is $\qty{X_1,\dots,X_n}$. Then, consider \[\P(X_1=x_1,\dots,X_n=x_n,\dsst\sum_{i=1}^nX_i=C\mid\hat p=C)=\dfrac{\P(X_1=x_1,\dots,X_n=x_n,\dsst\sum_{i=1}^nX_i=C)}{\P\qty(\hat{p}=C)}.\] What new information can $\dsst\sum_{i=1}^nX_i=C$ tell us? $X_n=C-\dsst\sum_{i=1}^{n-1}X_i$.\par 
			Note that $\P(\hat{p}=C)=\P\qty(\dsst\sum_{i=1}^nX_i=C)$. Since the summation of Bernoulli($p$) random variables is a Binomial$(n,p)$ random variable, we have $\P(\hat{p}=C)=\dsst\binom{n}{C}p^C(1-p)^{n-C}$.\par 
			$\boxed{\text{Case I}}$ Suppose $\dsst\sum_{i=1}^nX_i=C$. Then, \begin{align*}&\dfrac{\P(X_1=x_1,\dots,X_n=x_n,\dsst\sum_{i=1}^nX_i=C)}{\P\qty(\hat{p}=C)}\\&=\dfrac{\qty(\dsst\prod_{i=1}^{n-1})p^{X_i}(1-p)^{1-X_i}p^{C-\dsst\sum_{i=1}^{n-1}X_i}(1-p)^{\qty(1-C+\dsst\sum_{i=1}^{n-1}X_i)}}{\dsst\binom{n}{C}p^C(1-p)^{n-C}}\\&=\dfrac{p^{\dsst\sum_{i=1}^{n-1}X_i+C-\sum_{i=1}^{n-1}X_i}(1-p)^{(n-1)-\dsst\sum_{i=1}^{n-1}X_i+1-C+\sum_{i=1}^{n-1}X_i}}{\dsst\binom{n}{C}p^C(1-p)^{n-C}}\\&=\dfrac{p^C(1-p)^{n-C}}{\dsst\binom{n}{C}p^C(1-p)^{n-C}}=\dfrac{1}{\dsst\binom{n}{C}}\independ p\implies\text{sufficient}\end{align*}\par 
			$\boxed{\text{Case II}}$ Suppose $\dsst\sum_{i=1}^nX_i\neq C$. Then, \[\dfrac{\P(X_1=x_1,\dots,X_n=x_n,\dsst\sum_{i=1}^nX_i=C)}{\P\qty(\hat{p}=C)}=\dfrac{0}{\P(\hat{p}=C)}=0\independ{p}\implies\text{sufficient}\]
		\end{sol}
	\end{itemize}	
\end{eg}
\begin{thm}{Factorization Property}
	$\hat\theta$ is sufficient if and only if the likelihood can be factorized as \[\L(\theta)=\underbrace{g(\theta_e;\theta)}_{\theta_e=h(y_1,\dots,y_n)\ \&\ \theta}\cdot \underbrace{u(y_1,\dots,y_n)}_{\independ\theta}.\]	
\end{thm}

\subsection{Consistency}
\begin{df}{Consistency}
	An estimator $\hat\theta_n=h\qty(W_1,\dots,W_n)$ is said to be \textit{consistent} if it converges to $\theta$ in probability; i.e., for any $\epsilon>0$, \[\lim_{n\to\infty}\P\qty(\abs{\hat\theta_n-\theta}<\epsilon)=1.\]
\end{df}
\begin{rmk}
\begin{enumerate}
	\item Consistency is an asymptotical property (defined in a large sample limit).
	\item $n$= sample size. $\abs{\hat\theta_n-\theta}$ is the distance between estimator and true $\theta$.
\end{enumerate}	
\end{rmk}
\begin{lem}{Markov Inequality}
	Suppose $X\geq0$ is a random variable and $a>0$ is a constant. Then, \[\P(X\geq a)\leq\dfrac{\E(X)}{a}.\]
\end{lem}
\begin{rmk}
	Markov inequality is good for determining extreme values. If $\E(X)$ is small, then it is very unlikely that $X$ will take some extremely large numbers. 	
\end{rmk}
\begin{thm}{Chebyshev Inequality}
	Let $W$ be some random variable with finite mean $\mu$ and variance $\sigma^2$. Then, for any $\epsilon>0$, we have \[\P\qty(\qty|W-\mu|<\epsilon)\leq 1-\dfrac{\sigma^2}{\epsilon^2}\] or, equivalently, \[\P\qty(\qty|W-\mu|\geq\epsilon)\leq\dfrac{\sigma^2}{\epsilon^2}.\]
\end{thm}
\begin{prf}
	Consider the random variable $\abs{W-\mu}$. Then, by Markov Inequality, \begin{align*}\P\qty(\abs{X-\mu}\geq\epsilon)&=\P\qty(\abs{X-\mu}^2\geq\epsilon^2)\\&=\P\qty(\qty(X-\mu)^2\geq\epsilon^2)\leq\dfrac{\E\qty[(X-\mu)^2]}{\epsilon^2}=\dfrac{\sigma^2}{\epsilon^2}\end{align*}	
\end{prf}
\begin{cor}{}
	The sample mean $\hat\mu_n=\dfrac{1}{n}\dsst\sum_{i=1}^nW_i$ is a consistent estimator for $\E(W)=\mu$, provided that the population $W$ has finite mean $\mu$ and variance $\sigma^2$.	
\end{cor}
\begin{prop}{}
	If $\hat\theta_n$ is an unbiased estimator of $\theta$, then $\hat\theta_n$ is consistent if \[\lim_{n\to\infty}\Var\qty(\hat\theta_n)=0.\]	
\end{prop}
\begin{prf}
	Suppose $\hat\theta_n$ is an unbiased estimator of $\theta$. Then, $\E\qty(\hat\theta_n)=\theta$. So, by Chebyshev Inequality, we have \[\P\qty(\abs{\hat\theta_n\theta}\geq\epsilon)=\P\qty(\abs{\hat\theta_n-\E\qty(\hat\theta_n)}\geq\epsilon)\leq\dfrac{\E\qty[\qty(\hat\theta_n-\E\qty(\hat\theta_n))^2]}{\epsilon^2}=\dfrac{\Var\qty(\hat\theta_n)}{\epsilon^2}.\] If we have $\Var\qty(\hat\theta_n)\to0$ when $n\to\infty$, then \[\lim_{n\to\infty}\P\qty(\abs{\hat\theta_n-\theta}\geq\epsilon)\leq\lim_{n\to\infty}\dfrac{\Var\qty(\hat\theta_n)}{\epsilon^2}=\dfrac{0}{\epsilon}=0.\] Therefore, it must be that $\dsst\lim_{n\to\infty}\P\qty(\abs{\hat\theta_n-\theta}\geq\epsilon)=0$ as probability cannot take negative values. Hence, \begin{align*}\lim_{n\to\infty}\P\qty(\abs{\hat\theta_n-\theta}<\epsilon)&=\lim_{n\to\infty}\qty(1-\P\qty(\abs{\hat\theta_n-\theta}\geq\epsilon))\\&=1-\lim_{n\to\infty}\P\qty(\abs{\hat\theta_n-\theta}\geq\epsilon)\\&=1-0=1.\end{align*} Then, by definition, $\hat\theta_n$ is consistent. 
\end{prf}

\subsection{Bayesian Estimator}
\begin{thm}{Bayes' Rule}
	\[\P(A\mid B)=\dfrac{\P(B\mid A)\P(A)}{\P(B\mid A)\P(A)+\P(B\mid A^C)\P(A^C)}.\]
	\[\P(A\mid B^C)=1-\P(A\mid B)=\dfrac{\P(B^C\mid A)\P(A)}{\P(B^C\mid A)\P(A)+\P(B^C\mid A^C)\P(A^C)}.\]	
\end{thm}
\begin{description}
	\item[Rationale] Let $W$ be an estimator dependent on a parameter $\theta$.
	\begin{enumerate}
		\item Frequentists view $\theta$ as a parameter whose exact value to be estimated (\textit{$\theta$} is fixed).
		\item Bayesians view $\theta$ is the value of a random variable $\Theta$. (\textit{$\theta$ is uncertain and has its known parameter distribution}).
	\end{enumerate}
	\item[Data Generation] The following procedure generates data with an additional layer of randomness.
	\begin{enumerate}
		\item $\theta$ is sampled from a distribution. 
		\item Under this $\theta$, we sample the data.
	\end{enumerate}
\end{description}
\begin{df}{Prior distribution, Posterior distribution}
	Our prior knowledge on $\Theta$ is called the \textit{prior distribution}: $p_\Theta(\theta)$. The conditional distribution of the data given the parameter is the \textit{likelihood}: $p\qty(X\mid\Theta)$. Then, the Bayes' Rule will be \[\underbrace{\P\qty(\Theta\mid X)}_\text{posterior distribution given the observation}=\dfrac{\overbrace{\P(X\mid\Theta)}^\text{likelihood}\cdot\overbrace{\P(\Theta)}^\text{prior distribution}}{\underbrace{\P(X)}_\text{margin distirbution of data}}\]	
\end{df}
\begin{thm}{Bayesian Estimator}
	\[g_\Theta(\theta\mid W=w)=\begin{cases}\dfrac{p_W(w\mid\Theta=\theta)p_\Theta(\theta)}{p_W(w)}&\text{if }W\text{ and }\Theta\text{ are discrete}\\\\\dfrac{f_W(w\mid\Theta=\theta)f_\Theta(\theta)}{f_W(w)}&\text{if }W\text{ and }\Theta\text{ are constinuous},\end{cases}\]	
	where \begin{align*}f_W(x)&=\int_Hf_{W,\Theta}(w,\theta)\ \d{\theta}\quad\text{for }\theta\in H\\&=\int_Hf_W(w\mid\Theta=\theta)f_\Theta(\theta)\ \d{\theta}.\end{align*} Further, let $A=f_W(w)=\dsst\int_Hf_W(w\mid\Theta=\theta)f_\Theta(\theta)\ \d{\theta}$. Then, $A$ normalizes $\mathrm{likelihood}\times\mathrm{prior}$: \[1=\int\dfrac{f_W(w\mid\Theta=\theta)f_\Theta(\theta)}{A}\ \d{\theta}.\] So, \[g_\Theta(\theta\mid W=w)=\text{constant}\cdot f_W(w\mid\Theta=\theta)f_\Theta(\theta)\quad\text{or}\quad\mathrm{posterior}\propto\mathrm{likelihood}\times\mathrm{prior}.\]
\end{thm}
\begin{eg}
	A call center. Let $X=$number of calls coming into the center. Then we know that $X\sim\text{Poisson}(\lambda)$. This particular call center believes that $\Lambda$ is distributed with pdf \[p_\Lambda(8)=0.25\quad\text{and}\quad p_\Lambda(10)=0.75.\] The call center believes that the number of calls coming into the center has recently changed, so they pick an hour and observe that $X=7$ calls come in. 
	\begin{sol}
		We want to find: $\P(\Lambda=8\mid X=7)$ and $\P(\Lambda=10\mid X=7)$. By Bayes' Rule: \begin{align*}
			\P(\Lambda=8\mid X=7)&=\dfrac{\P(X=7\mid \Lambda=8)\P(\Lambda=8)}{\P(X=7)}\\&=\dfrac{\P(X=7\mid \Lambda=8)\P(\Lambda=8)}{\P(X=7\mid \Lambda=8)\P(\Lambda=8)+\P(X=7\mid \Lambda=10)\P(\Lambda=10)}\\&=\dfrac{e^{-8}\qty(\dfrac{8^7}{7!})(0.25)}{e^{-8}\qty(\dfrac{8^7}{7!})(0.25)+e^{-10}\qty(\dfrac{10^7}{7!})(0.75)}\approx0.66
		\end{align*}
		Then, $\P(\Lambda=10\mid X=7)=1-\P(\Lambda=8\mid X=7)=1-0.66=0.34$. Or, alternatively, we can use the Bayes' Rule again. 
	\end{sol}
\end{eg}
\begin{table}[H]
\caption{Convention of Picking a Prior Distribution}
\begin{center}
\begin{tabular}{c|c}	
	Parameter&Prior Distribution\\\hline
	$\text{Bernoulli}(p)$&Beta\\
	$\text{Binomial}(p)$&Beta\\
	$\text{Poisson}(\lambda)$&Gamma\\
	$\text{Exponential}(\lambda)$&Gamma\\
	$\text{Normal}(\mu)$&Normal\\
	$\text{Normal}(\sigma^2)$&Inverse Gamma
\end{tabular}
\end{center}
\end{table}
\begin{rmk}
	When we have no prior knowledge on the belief, we choose a uniform distribution.	
\end{rmk}
\begin{eg}
	Consider an unfair coin $\Theta$ (a random variable indicating the probability of getting head). Flip the coin $n$ times, $X=$ number of heads. Find the posterior distribution. 
	\begin{sol}
		By the Bayes' rule, \[f_{\Theta\mid X}(\theta\mid X=x)=\dfrac{f_\Theta(\theta)\P(X=k\mid\theta)}{\P(X=k)}.\] We know $\theta\in[0,1]$, so $\Theta\sim\text{Uniform}[0,1]$ and $f_\Theta(\theta)=1$. So, \[f_{\Theta\mid X}(\theta\mid X=x)=\dfrac{1\cdot\dsst\binom{n}{k}\cdot\theta^k(1-\theta)^{n-k}}{\P(X=k)}=\underbrace{\dfrac{1\cdot\dsst\binom{n}{k}}{\P(X=k)}}_\text{constant}\theta^k(1-\theta)^{n-k}\]
		\begin{df}{Beta Distribution}
			For a distribution $\text{Beta}(\alpha,\beta)$, the pdf is given by \[f_Y(y;\alpha,\beta)=\dfrac{y^{\alpha-1}(1-y)^{\beta-1}}{\B(\alpha,\beta)}\quad\text{for }y\in[0,1]\text{ and }\alpha,\beta>0,\] where \[\B(\alpha,\beta)\coloneqq\int_0^1y^{\alpha-1}(1-y)^{\beta-1}\ \d{y}=\dfrac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)},\quad\alpha,\beta>0.\] The expectation of $X\sim\text{Beta}(\alpha,beta)$ is given by \[\E(X)=\dfrac{\alpha}{\alpha+\beta}.\]
		\end{df}
		Disregarding the constant, $\theta^k(1-\theta)^{n-k}$ is part of the Beta distribution with $\alpha=k+1$ and $\beta=n-k+1$. So, $\Theta\sim\text{Beta}(k+1,n-k+1)$. To form a distribution, the constant must, therefore, be \begin{align*}\dfrac{\dsst\binom{n}{k}}{\P(X=k)}=\dfrac{1}{\B(k+1,n-k+1)}&=\dfrac{\Gamma(k+1+n-k+1)}{\Gamma(k+1)\Gamma(n-k+1)}\\&=\dfrac{\Gamma(n+2)}{\Gamma(k+1)\Gamma(n-k+1)}\\&=\dfrac{(n+1)!}{k!(n-k)!}&If\ n\in\N,\ then\ \Gamma(n)=(n-1)!\end{align*}
		Note that \underline{$\text{Beta}(\alpha=1,\beta=1)=\text{Unform}(0,1)$}. So, in this example, \[\text{Beta}(1,1)\xrightarrow{\text{ Data }}\text{Beta}(k+1,n-k+1).\]
		Moreover, $\E(\Theta)=\dfrac{k+1}{k+1+n-k+1}=\dfrac{k+1}{n+2}$.
	\end{sol}
\end{eg}
\begin{eg}
	Let $X_1,\dots,X_n$ be a random sample form $\text{Bernoulli}(\theta)$: $p_X(k;\theta)=\theta^k(1-\theta)^{1-k}$ for $k=0,1$. Let $X=\dsst\sum_{i=1}^nX_i$. Then, $X$ follows $\text{Binomial}(n,\theta)$. Consider the prior distribution $\Theta\sim\text{Beta}(r,s)$, i.e., $f_\Theta(\theta)=\dfrac{\Gamma(r+s)}{\Gamma(r)\Gamma(s)}\theta^{r-1}(1-\theta)^{s-1}$ for $\theta\in[0,1]$. Then, the posterior distribution is \[\Theta\mid X\sim\text{Beta}(r+k, s+n-k).\]
	\begin{prf}
		Note that \begin{align*}f_{\Theta\mid X}(\theta\mid X=x)&=\dfrac{p_X(X=k\mid\theta)f_{\Theta}(\theta)}{\dsst\int_0^1p_X(X=k\mid\theta)f_{\Theta}(\theta)\ \d{\theta}}\\&=\dfrac{\dsst\binom{n}{k}\theta^k(1-\theta)^{n-k}\dfrac{\Gamma(r+s)}{\Gamma(r)\Gamma(s)}\theta^{r-1}(1-\theta)^{s-1}}{\dsst\int_0^1\dsst\binom{n}{k}\theta^k(1-\theta)^{n-k}\dfrac{\Gamma(r+s)}{\Gamma(r)\Gamma(s)}\theta^{r-1}(1-\theta)^{s-1}\ \d{\theta}}\\&=\dfrac{\dsst\binom{n}{k}\dfrac{\Gamma(r+s)}{\Gamma(r)\Gamma(s)}\theta^{k+r-1}(1-\theta)^{n-k+s-1}}{\dsst\binom{n}{k}\dfrac{\Gamma(r+s)}{\Gamma(r)\Gamma(s)}\int_0^1\theta^{k+r-1}(1-\theta)^{n-k+s-1}\ \d{\theta}}\end{align*}
		Note that $\theta^{k+r-1}(1-\theta)^{n-k+s-1}$ is part of $\text{Beta}(k+r,n-k+s)$.  So, \begin{align*}1&=\int_0^1\dfrac{\Gamma(k+r+n-k+s)}{\Gamma(k+r)\Gamma(n-k+s)}\theta^{k+r-1}(1-\theta)^{n-k+s-1}\ \d{\theta}\\1&=\dfrac{\Gamma(r+n+s)}{\Gamma(k+r)\Gamma(n-k+s)}\int_0^1\theta^{k+r-1}(1-\theta)^{n-k+s-1}\ \d{\theta}\\\int_0^1\theta^{k+r-1}(1-\theta)^{n-k+s-1}\ \d{\theta}&=\dfrac{\Gamma(k+r)\Gamma(n-k+s)}{\Gamma(r+n+s)}.\end{align*} Therefore, \[f_{\Theta\mid X}(\theta\mid X=x)=\dfrac{\theta^{k+r-1}(1-\theta)^{n-k+s-1}}{\dfrac{\Gamma(k+r)\Gamma(n-k+s)}{\Gamma(r+n+s)}}=\dfrac{\Gamma(r+n+s)}{\Gamma(k+r)\Gamma(n-k+s)}\theta^{k+r-1}(1-\theta)^{n-k+s-1}.\] This is exactly a Beta distribution with parameter $\alpha=k+r$ and $\beta=n-k+s$.
	\end{prf}
\end{eg}
\begin{df}{Conjugate Prior}
	If the posterior distributions $p(\Theta\mid X)$ are in the sample probability distribution family as the prior probability distribution $p(\Theta)$, the prior and posterior are called \textit{conjugate distributions} and the prior is called a \textit{conjugate prior} for the likelihood function.	
\end{df}
\begin{rmk}
	Common Conjugate Priors
	\begin{itemize}
		\item Beta distributions are conjugate priors for Bernoulli, Binomial, Negative binomial, and Geometric likelihood.
		\item Gamma distributions are conjugate priors for Poisson and Exponential likelihood
	\end{itemize}	
\end{rmk}
\begin{df}{Bayesian Point Estimation}
	Given the posterior $f_{\Theta\mid W}(\theta\mid W=w)$, how can one calculate the appropriate point estimate $\theta_e$?
\end{df}
\begin{df}{Loss Function}
	Let $\theta_e$ be an estimate for $\theta$ based on a statistic $W$. The \textit{loss function} associated with $\theta_e$ is denoted $\L(\theta_e,\theta)$, where $\L(\theta_e,\theta)\geq0$ and $\L(\theta,\theta)=0$.	
\end{df}
\begin{itemize}
	\item The lost function is $\E\qty[\L(\hat\theta,\theta)]$.
	\item The MSE, mean square error, is $\E\qty[\qty(\hat\theta-\theta)^2]$.
	\begin{enumerate}
		\item If we have not data, then notice that \[\E\qty[(\theta-c)^2]=\E\qty(\theta^2)+\E\qty(c^2)-2c\E(\theta)\] is minimized at $c=\E(\theta)$. Therefore, \[\min\E\qty[(\theta-\hat\theta)^2]=\E\qty[(\theta-\E(\theta))]^2=\Var(\theta).\] So, $\hat\theta^*=\E(\theta)$, the prior expectation.
		\item If we have data $X=x$, then \[\min\E\qty[(\theta-\hat\theta)^2\mid X=x]\implies\hat\theta^*=\E\qty[\theta\mid X=x].\] This $\hat\theta^*$ is called the posterior expectation. 
	\end{enumerate}
\end{itemize}
\begin{thm}{Squared-Loss Bayesian Estimation}
	\begin{description}
		\item[Step 1.] Solve the posterior distribution.
		\item[Step 2.] Calculate the posterior expectation. 
	\end{description}
	Generally, if we know the posterior pdf $f_\Theta(\theta\mid X=x)$, the point estimate is \[\E[\theta\mid X=x]=\int_\Theta\theta f_\Theta(\theta\mid X=x)\ \d{\theta}.\]
\end{thm}
\begin{thm}{}
	Let $f_\Theta(\theta\mid W=w)$ be the posterior distribution of the random variable $\Theta$.
	\begin{itemize}
		\item If $\L(\theta_e,\theta)=\abs{\theta_e-\theta}$, then the Bayesian point estimate for $\theta$ is the median of the posterior distribution $f_\Theta(\theta\mid W=w)$;
		\item If $\L(\theta_e,\theta)=(\theta_e-\theta)^2$, then the Bayesian point estimate for $\theta$ is the mean of the posterior distribution $f_\Theta(\theta\mid W=w)$.
	\end{itemize}	
\end{thm}

\newpage
\section{Inference Based on Normal}
\subsection{Sample Variance and Chi-Square Distribution}
Recall that if $Y\sim\text{Normal}(\mu,\sigma^2)$, we have MLEs defined as \[\hat\mu=\bar Y\quad\text{and}\quad\hat\sigma^2=\dfrac{1}{n}\sum_{i=1}^n\qty(Y_i-\bar Y)^2.\] If $\sigma$ is known, we can do the interval estimation: \[Z\coloneqq\dfrac{\bar Y-\E(\bar Y)}{\sqrt{\Var(\bar Y)}}\sim N(0,1).\] However, what if we don't know $\sigma$? We will have to estimate it with a sample variance. 
\begin{df}{Sample Variance}
	To estimate $\sigma^2$, we define the following unbiased \textit{sample variance}: \[S^2=\dfrac{1}{n-1}\sum_{i=1}^n(Y_i-\bar Y)^2.\]	
\end{df}
\begin{rmk}
	We often compute $S^2$ using the fact that \[\sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^ny_i^2-n\bar y^2\quad\text{i.e., }S^2=\dfrac{1}{n-1}\qty[\sum_{i=1}^ny_i^2-n\bar y^2]\]	
\end{rmk}
\begin{df}{Chi-Squared Distribution}
	Suppose $W_k\sim\chi^2(k)$, the \textit{chi-squared distribution with degree of freedom $k$}. Then, \[W_k=Z_1^2+Z_2^2+\cdots+Z_k^2,\text{ where }Z_i\stackrel{\iid}{\sim}N(0,1).\] $k$ is called the \textit{degree of freedom} of the chi-squared distribution and is denoted as $df=k$.
\end{df}
\begin{thm}{Chi-Squared Distribution and Gamma Distribution}
	$\chi^2(1)$	 is equivalent to $\text{Gamma}\qty(\dfrac{1}{2},\dfrac{1}{2})$. Hence, $\chi^2(n)$ is equivalent to $\text{Gamma}\qty(\dfrac{n}{2},\dfrac{1}{2})$.
\end{thm}
\begin{prf}
	Recall: For $Y_1\sim\text{Gamma}(n,\lambda)$ and $Y_2\sim\text{Gamma}(m,\lambda)$, we have the following sum rule \[Y_1+Y_2\sim\text{Gamma}(n+m,\lambda).\] Then, as $Z_1^2\sim\chi^2(1)=\text{Gamma}\qty(\dfrac{1}{2},\dfrac{1}{2})$, we have \[Z_1^2+Z_2^2+\cdots+Z_n^2\sim\chi^2(n)=\text{Gamma}\qty(\dfrac{1}{2}+\cdots+\dfrac{1}{2},\dfrac{1}{2})=\text{Gamma}\qty(\dfrac{n}{2},\dfrac{1}{2}).\]
\end{prf}
\begin{thm}{Expectation and Variance of $\chi^2(n)$}
	If $W_n\sim\chi^2(n)$, then \[\E(W_n)=n=df\quad\text{and}\quad\Var(W_n)=2n\]	
\end{thm}
\begin{prf}
	For $Y\sim\text{Gamma}(n,\lambda)$, $\E(Y)=\dfrac{n}{\lambda}$ and $\Var(Y)=\dfrac{n}{\lambda^2}$. As $W_n\sim\chi^2(n)=\text{Gamma}\qty(\dfrac{n}{2},\dfrac{1}{2})$, we have \[\E(W_n)=\dfrac{n/2}{1/2}=n\quad\text{and}\quad\Var(W_n)=\dfrac{n/2}{1/4}=2n.\]
\end{prf}
\begin{thm}{}
	Consider a random sample $Y_1,\dots,Y_n$ drawn from $N(0,1)$. Let $S^2$	be the sample variance and $\bar Y$ be the sample mean. Then, 
	\begin{itemize}
		\item $S^2$ and $\bar Y$ are independent;
		\item $\dfrac{(n-1)}{\sigma^2}S^2\sim\chi^2(n-1)$
	\end{itemize}
\end{thm}
\begin{rmk}
	We can think of the second bullet point as the following rationale: knowing $\bar Y$, we only need $(n-1)$ data, and we can calculate $Y_n$ from $\bar Y$ and $Y_1,\dots,Y_{n-1}$. This explains why the chi-squared distribution is of $df=n-1$.	
\end{rmk}
\begin{prf}(informally)
	\begin{enumerate}
		\item We will prove the case when $n=2$. \par $S^2=\dfrac{1}{n-1}\sum_{i=1}^n\qty(Y_i-\bar{Y})^2$. If $n=2$, $\bar{Y}=\dfrac{Y_1+Y_2}{2}$, then \begin{align*}S^2&=\qty(Y_1-\bar{Y})^2+\qty(Y_2-\bar{Y})^2\\&=\qty(Y_1-\dfrac{Y_1+Y_2}{2})^2+\qty(Y_2-\dfrac{Y_1+Y_2}{2})\\&=\qty(\dfrac{Y_1-Y_2}{2})^2+\qty(\dfrac{Y_2-Y_1}{2})^2\\&=\dfrac{1}{2}\qty(Y_1-Y_2)^2.\end{align*}
		\begin{clm*}
			Recall that if $X_1$ and $X_2$ are independent, then \begin{equation}\label{eq1}\E(X_1X_2)=\E(X_1)\E(X_2).\end{equation} The backward implication is not true in general, but specially for normal distributions. That is, if  \eqref{eq1} holds and $X_1,\ X_2$ normal are normal, then $X_1\independ X_2$.
		\end{clm*}
		As $Y_1-Y_2$ and $Y_1+Y_2$ are both normal distributed, to show they are independent of each other, we only need to show that \[\E\qty[(Y_1-Y_2)(Y_1+Y_2)]=\E(Y_1-Y_2)\E(Y_1+Y_2).\] The detailed proof is omitted, but the equality holds. 
		\item Show that $\dfrac{(n-1)}{\sigma^2}S^2\sim\chi_{n-1}^2$. Note that $Y_i\sim N(\mu,\sigma)$. Then, \[\dfrac{Y_i-\mu}{\sigma}\sim N(0,1)\quad\text{and}\quad\dfrac{\bar Y-\mu}{\sqrt{\sigma^2/n}}\sim N(0,1).\] So, \[\dfrac{(Y_i-\mu)^2}{\sigma^2}\sim\chi_1^2\implies\dfrac{\dsst\sum_{i=1}^n(Y_i-\mu)^2}{\sigma^2}\sim \chi_n^2\quad\text{and}\quad\dfrac{(\bar Y-\mu)^2}{\sigma^2/n}\sim\chi_1^2.\]
		\begin{clm*}
			If $U_1\sim\chi^2(m)$ and $U_2\sim\chi^2(n)$ with $U_1\independ U_2$, then $U_1+U_2\sim\chi^2(m+n)$ by the summation rule of Gamma.	
		\end{clm*}
		Therefore, by the Claim, we have \begin{align*}\dfrac{\dsst\sum_{i=1}^n\qty(Y_i-\mu)^2}{\sigma^2}&=\dfrac{\dsst\sum_{i=1}^n\qty(Y_i-\bar Y+\bar Y-\mu)^2}{\sigma^2}\\&\sim\dfrac{\dsst\sum_{i=1}^n(Y_i-\bar Y)^2+\sum_{u=1}^n(\bar Y-\mu)^2}{\sigma^2}\\&=\dfrac{(n-1)S^2}{\sigma^2}+\dfrac{\dsst\sum_{i=1}^n(\bar Y-\mu)^2}{\sigma^2}.\end{align*} Note that $\dfrac{\dsst\sum_{i=1}^n\qty(Y_i-\mu)^2}{\sigma^2}\sim\chi_n^2$ and $\dfrac{\dsst\sum_{i=1}^n(\bar Y-\mu)^2}{\sigma^2}\sim \chi_1^2$. So, it must be that $\dfrac{(n-1)S^2}{\sigma^2}\sim\chi_{m-1}^2$.
	\end{enumerate}	
\end{prf}

\subsection{Inference on $\mu$ and $\sigma$}
\begin{df}{Sampling Distribution}
	The \textit{sampling distributions} are defined as the distributions of functions of random sample of given size. 	
\end{df}
\begin{description}
	\item[Aim:] Determine distributions for the following statistics: 
	\begin{center}
	\begin{tabular}{c|c}
		Statistics&Distribution\\\hline
		(Sample Variance) $S^2\coloneqq\dfrac{1}{n-1}\dsst\sum_{n=1}^n(Y_1-\bar Y)^2$&Chi-square distribution\\\\
		$T\coloneqq \dfrac{\bar Y-\mu}{S/\sqrt{n}}$&Student $t$ distribution\\\\
		$\dfrac{S_1^2}{\sigma_1^2}\Big/\dfrac{S_2^2}{\sigma_2^2}$&$F$ distribution
	\end{tabular}
	\end{center}
\end{description}
\begin{df}{The Test Statistic}
	The \textit{test statistic} is defined as \[T\coloneqq\dfrac{\bar Y-\mu}{S/\sqrt{n}},\]	 with $\bar Y=\dfrac{1}{n}\dsst\sum_{i=1}^n Y_i$ and $S^2=\dsst\dfrac{1}{n-1}\sum_{i=1}^n\qty(Y_i-\bar Y)^2$.
\end{df}
\begin{df}{Student $t$-Ratio}
	Consider \begin{itemize}
		\item $Z\coloneqq\dfrac{\sqrt{\mu}}{\sigma}(\bar Y-\mu)\sim N(0,1)$
		\item $V\sim\chi_n^2$
		\item $Z\independ V$
	\end{itemize}	
	Then, we define the \textit{student $t$-ratio} with $n$ degrees of freedom as \[T_n\coloneqq\dfrac{Z}{\sqrt{V/n}}.\] Note that $Z\sim N(0,1)$ and $\sqrt{V/n}\sim\sqrt{\dfrac{\chi_n^2}{n}}$.
\end{df}
\begin{thm}{Distribution of $\dfrac{\bar Y-\mu}{S/\sqrt{n}}$}
	Consider $Y_1,\dots,Y_n\stackrel{\iid}{\sim}N(\mu,\sigma^2)$. Let $S^2$ to be the sample variance. Then, \[\dfrac{\bar Y-\mu}{S/\sqrt{n}}\sim T_{n-1}.\]
\end{thm}
\begin{prf}
	Note that \begin{equation}\label{eq2}\dfrac{\bar Y-\mu}{\sigma/\sqrt{n}}\sim N(0,1)\end{equation} and \begin{equation}\label{eq3}\dfrac{(n-1)S^2}{\sigma^2}\sim\chi_{n-1}^2\end{equation} Then, consider \begin{align*}\dfrac{\bar Y-\mu}{S/\sqrt{n}}=\dfrac{\bar Y-\mu}{\sigma/\sqrt{n}}\cdot\dfrac{\sigma}{S}&=\dfrac{\dfrac{\bar Y-\mu}{\sigma/\sqrt{n}}}{\sqrt{\dfrac{S^2}{\sigma^2}}}\\&=\dfrac{\dfrac{\bar Y-\mu}{\sigma/\sqrt{n}}}{\sqrt{\dfrac{(n-1)S^2}{\sigma^2}}\cdot\dfrac{1}{\sqrt{n-1}}}\\&=\dfrac{\dfrac{\bar Y-\mu}{\sigma/\sqrt{n}}\sim N(0,1)}{\sqrt{\dfrac{(n-1)S^2/\sigma^2}{n-1}}\sim\chi_{n-1}^2}&S^2\independ\bar Y\\&\sim T_{n-1}.\end{align*}
\end{prf}
\begin{thm}{Connection Between $N(0,1)$ and $t$}
	$T$ distribution is flatter/more spread out than $N(0,1)$. It has heavier tails. 	
\end{thm}
\begin{prf}
	Note that \begin{itemize}
		\item $S_n^2=\dfrac{1}{n-1}\dsst\sum_{i=1}^n(Y_i-\bar Y)^2$ is an unbiased estimator of $\sigma^2$.
		\item $S_n^2$ is a consistent estimator of $\sigma^2$.
	\end{itemize}
	So, $\Var(S_n^2)\to0$ as $n\to\infty$. This implies that the difference between $T$ and $N(0,1)$ is significant when $n$ is small. 
\end{prf}
\begin{thm}{Inference on $\mu$}
	If $\sigma^2$ is known, we inference $\mu$ using $Z=\dfrac{\bar Y-\mu}{\sigma/\sqrt{n}}$. We use $z$-score and $z_\alpha$ table to construct the $100(1-\alpha)\%$ CI as $\qty(\bar y-z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}},\bar y+z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}})$. Alternatively, if $\sigma^2$ is unknown, we use $T_{n-1}=\dfrac{\bar Y-\mu}{S/\sqrt{n}}$. We apply $t_{n-1}$ score and $t_{\alpha,n-1}$ table to construct a similar CI.
\end{thm}
\begin{thm}{Inference on $\sigma$}
	A two-sided $100(1-\alpha)\%$ CI on $\sigma$ will be given 	by \[\qty(\sqrt{\dfrac{(n-1)S^2}{\chi_{1-\alpha/2,n-1}^2}},\sqrt{\dfrac{(n-1)S^2}{\chi_{\alpha/2,n-1}^2}}).\]
\end{thm}
\begin{prf}
	Note that \[X_n\coloneqq\dfrac{(n-1)S^2}{\sigma^2}\sim\chi_{n-1}^2.\] Then, \[\P(x_a\leq X_n\leq x_b)=100(1-\alpha)\%.\] To construct a two-sided CI, since chi-square distribution is not symmetric, we can choose the two points that have the same density value (this will ensure a short CI). However, this method is very numerically expensive. To save computational cost, we will still choose the two points that covers the $\alpha/2\%$ and $(1-\alpha/2)\%$ distribution. It is also known as to find $\chi_{\alpha/2,n-1}^2$ from the $\chi^2$ table. Hence, \begin{align*}\P(\chi_{\alpha/2,n-1}^2\leq X_n\leq \chi_{1-\alpha/2,n-1}^2)&=100(1-\alpha)\%\\\P(\chi_{\alpha/2,n-1}^2\leq \dfrac{(n-1)S^2}{\sigma^2}\leq \chi_{1-\alpha/2,n-1}^2)&=100(1-\alpha)\%\\\implies\dfrac{(n-1)S^2}{\chi_{1-\alpha/2,n-1}^2}\leq\sigma^2\leq&\dfrac{(n-1)S^2}{\chi_{\alpha/2,n-1}^2}\end{align*}So, $100(1-\alpha)\%$ CI of $\sigma^2$ is \[\qty(\dfrac{(n-1)S^2}{\chi_{1-\alpha/2,n-1}^2},\dfrac{(n-1)S^2}{\chi_{\alpha/2,n-1}^2})\] and a $100(1-\alpha)\%$ CI of $\sigma$ is \[\qty(\sqrt{\dfrac{(n-1)S^2}{\chi_{1-\alpha/2,n-1}^2}},\sqrt{\dfrac{(n-1)S^2}{\chi_{\alpha/2,n-1}^2}}).\]
\end{prf}


\end{document}
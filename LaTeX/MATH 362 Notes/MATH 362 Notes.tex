\include{preamble}

\title{Emory University\\\textbf{MATH 362 Mathematical Statistics II}\\ Learning Notes}
\author{Jiuru Lyu}
\date{\today}

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Estimation}
\subsection{Introduction}
\begin{df}{Model}
	A \textit{model} is a distribution with certain parameters.	
\end{df}
\begin{eg}
	The normal distribution: $N(\mu,\sigma^2)$.	
\end{eg}
\begin{df}{Population}
	The \textit{population}	is all the objects in the experiment.
\end{df}
\begin{df}{Data, Sample, and Random Sample}
	\textit{Data} refers to observed value from sample. The \textit{sample}	is a subset of the population. A \textit{random sample} is a sequence of independent, identical ($\iid$) random variables.
\end{df}
\begin{df}{Statistics}
	\textit{Statistics} refers to a function of the random sample.
\end{df}
\begin{eg}
	The sample mean is a function of the sample: \[\bar{Y}=\dfrac{1}{n}\qty(Y_1+\cdots+Y_n).\]	
\end{eg}
\begin{eg} 
	Central Limit Theorem\par 
	We randomly toss $n=200$ fair coins on the table. Calculate, using the central limit theorem, the probability that at least $110$ coins have turned on the same side. 
	\[\bar{X}=\dfrac{X_1+\cdots+X_{200}}{200}\quad\stackrel{\text{CLT}}{\sim}\quad N\qty(\mu,\sigma^2),\] where \[\mu=\E\qty(\bar{X})=\dfrac{\dsst\sum_{i=1}^{200}\E\qty(X_i)}{200},\] \[\sigma^2=\Var\qty(\bar{X})=\Var\qty(\dfrac{X_1+\cdots+X_{200}}{200})=\dfrac{\dsst\sum_{i=1}^{200}\Var\qty(X_i)}{200^2}.\]
\end{eg}
\begin{df}{Statistical Inference}
	The process of \textit{statistical inference} is defined to be the process of using data from a sample to gain information about the population.	
\end{df}
\begin{eg}
	Goals in statistical inference
	\begin{enumerate}
		\item \begin{df}{Estimation} To obtain values of the parameters from the data. \end{df}
		\item \begin{df}{Hypothesis Testing} To test a conjecture about the parameters. \end{df}
		\item \begin{df}{Goodness of Fit} How well does the data fit a given distribution. \end{df}
		\item Linear Regression
	\end{enumerate}	
\end{eg}


\subsection{The Method of Maximum Likelihood and the Method of Moments}
\begin{eg}
	Given an unfair coin, or $p$-coin, such that \[X=\begin{cases}1\quad\text{head with probability }p,\\0\quad\text{tail with probability }1-p.\end{cases}\]	How can we determine the value $p$?
	\begin{sol}
		\begin{enumerate}
			\item Try to flip the coin several times, say, three times. Suppose we get HHT. 
			\item Draw a conclusion from the experiment. 
		\end{enumerate}	
		\textbf{Key idea: The choice of the parameter $p$ should be the value that maximizes the probability of the sample.}
		\[\P\qty(X_1=1,X_2=1,X_3=0)=\P\qty(X_1=1)\P(X_2=1)\P(X_3=0)=p^2(1-p)\coloneqq f(p).\] Solving the optimization problem $\dsst\max_{p>0}f(p)$, we find it is most likely that $p=\dfrac{2}{3}$. This method is called the \textit{likelihood maximization method}.
	\end{sol}
\end{eg}
\begin{df}{Likelihood Function}
	For a random sample of size $n$ from the discrete (or continuous) pdf $p_X(k;\theta)$ (or $f_Y(y;\theta)$), the \textit{likelihood function}, $L(\theta)$, is the product of the pdf evaluated at $X_i=k_i$ (or $Y_i=y_i$). That is, \[L(\theta)\coloneqq\prod_{i=1}^np_X\qty(k_i;\theta)\quad\text{or}\quad L(\theta)\coloneqq\prod_{i=1}^nf_Y\qty(y_i;\theta).\]	
\end{df}
\begin{df}{Maximum Likelihood Estimate}
	Let $L(\theta)$ be as defined in Definition 1.2.2. If $\theta_e$ is a value of the parameter such that $L\qty(\theta_e)\geq L(\theta)$ for all possible values of $\theta$, then we call $\theta_e$ the \textit{maximum likelihood estimate} for $\theta$.
\end{df}
\begin{thm}{The Method of Maximum Likelihood}
	Given random samples $X_1,\dots,X_N$ and a density function $p_X(x)$ (or $f_X(x)$), then we have the likelihood function defined as \begin{align*}L(\theta)=p_X(X;\theta)&=\P(X_1,X_2,\dots,X_N)\\&=\P(X_1)\P(X_2)\cdots\P(X_N)&[independent]\\&=\prod_{i=1}^Np_X(X_i;\theta)&[identical]\end{align*} Then, the maximum likelihood estimate for $\theta$ is given by \[\theta^*=\argmax_\theta L(\theta),\] where \[L\qty(\argmax_\theta L(\theta))=L^*(\theta)=\max_\theta L(\theta).\]
\end{thm}
\begin{eg}
	Consider the Poisson distribution $X=0,1,\dots,$ with $\lambda>0$. Then, the pdf is given by \[p_X(k,\lambda)=e^{-\lambda}\dfrac{\lambda^k}{k!},\quad k=0,1,\dots\] Given data $k_1,\dots,k_n$, we have the likelihood function \[L(\lambda)=\prod_{i=1}^np_X\qty(X=k;\lambda)=\prod_{i=1}^ne^{-\lambda}=\dfrac{\lambda^{k_i}}{k_i!}=e^{-n\lambda}\dfrac{\lambda^{\sum{k_i}}}{k_1!\cdots k_n!}\] Then, to find the maximum likelihood estimate of $\lambda$, we need to $\dsst\max_{\lambda}L(\lambda)$. That is to solve $\dsst\pdv{L(\lambda)}{\lambda}=0$ and $\dsst\pdv[2]{L(\lambda)}{\lambda}<0$.
\end{eg}
\begin{eg}
	Waiting Time. \par 
	Consider the exponential distribution $f_Y(y)=\lambda e^{-\lambda y}$ for $y\geq 0$. Find the MLE $\lambda_e$ of $\lambda$.
	\begin{sol}
		The likelihood function of the exponential distribution is given by
		\[L(\lambda)=\prod_{i=1}^n\lambda e^{-\lambda y_i}=\lambda^n\exp\qty(-\lambda\sum_{i=1}^ny_i).\] Now, define \[\l(\lambda)=\ln L(\lambda)=n\ln\lambda-\lambda\sum_{i=1}^ny_i.\] To optimize $\l(\lambda)$, we compute \[\dv{\lambda}\l(\lambda)=\dfrac{n}{\lambda}-\sum_{i=1}^ny_i\stackrel{set}{=}0\] So, \[\dfrac{n}{\lambda}=\sum_{i=1}^ny_i\implies\lambda_e=\dfrac{n}{\dsst\sum_{i=1}^ny_i}\eqqcolon\dfrac{1}{\bar{y}},\] where $\bar{y}$ is the sample mean. 
	\end{sol}
\end{eg}
\begin{eg}
	Given the exponential distribution $f_Y(y)=\lambda e^{-\lambda y}$ for $y\geq0$. Find the MLE of $\lambda^2$.
	\begin{sol}
		Define $\tau=\lambda^2$. Then, $\lambda=\sqrt{\tau}$, and so \[f_Y(y)=\sqrt{\tau}e^{-\sqrt{\tau}y},\quad y\geq0.\] Then, the likelihood function becomes \[L(\tau)=\prod_{i=1}^nf_Y(y)=\tau^{\frac{n}{2}}\exp\qty(-\sqrt{\tau}\sum_{i=1}^ny_i).\] Similarly, after maximization, we find \[\tau_e=\dfrac{1}{\qty(\bar{y})^2}.\]
	\end{sol}
\end{eg}
\begin{thm}{Invariant Property for MLE}
	Suppose $\lambda_e$	is the MLE of $\lambda$. Define $\tau\coloneqq h(\lambda)$. Then, $\tau_e=h(\lambda_e)$.
\end{thm}
\begin{prf}
	In this proof, we will prove the case when $h$ is a one-to-one function. The case of $h$ being a many-to-one function is beyond the scope of this course. \par 
	Suppose $h(\cdot)$ is a one-to-one function. Then, $\lambda=h^{-1}(\tau)$ is well-defined. Then, \[\max_{\lambda}L(\lambda;y_1,\dots,y_n)=\max_{\tau}L\qty(h^{-1}(\tau);y_1,\dots,y_n)=\max_{\tau}L(\tau;y_1,\dots,y_n).\]	
\end{prf}
\begin{eg}
	Waiting Time with an unknown Threshold.\par 
	Let $\lambda=1$ in exponential but there is an unknown threshold $\theta$, that, is $f_Y(y)=e^{-(y-\theta)}$ for $y\geq\theta,\ \theta>0$.	
	\begin{sol}
		Note that the likelihood function is given by \begin{align*}L(\theta;y_1,\dots,y_n)=\prod_{i=1}^nf_Y(y_1)&=\exp\qty(-\sum_{i=1}^n(y_i-\theta)),\quad y_i\geq\theta,\ \theta>0\\&=\exp\qty(-\sum_{i=1}^n(y_i-\theta))\cdot\1_{\qty[y_i\geq0,\ \theta>0]},\end{align*} where \[\1_{x\in A}=\begin{cases}1\quad\text{if }x\in A\\0\quad\text{if }x\notin A.\end{cases}\] Using order statistics, \begin{align*}L(\theta)&=\exp\qty(-\sum_{i=1}^n(y_i-\theta))\cdot\1_{\qty[y_{(n)}\geq y_{(n-1)}\geq\cdots\geq y_{(1)}\geq\theta,\ \theta>0]}\\&=\exp\qty(-\sum_{i=1}^ny_i+n\theta)\1_{\qty[y_{(n)}\geq\cdots\geq y_{(1)}\geq\theta,\ \theta>0]}.\end{align*}
		So, we know $\theta\leq y_{(1)}=y_{\min}$. \par 
		To maximize the likelihood function, we want to maximize $-\sum y_i+n\theta$. That is, to maximize $\theta$, as $\theta\leq y_{\min}$, it must be that $\theta_{\max}=y_{\min}$. Therefore, the MLE is $\theta^*=y_{\min}.$
	\end{sol}
\end{eg}
\begin{eg}
	Suppose $Y_1,\dots,Y_n\sim\text{Uniform}[0,a]$. That is, $f_Y(y;a)=\dfrac{1}{a}$ for $y\in[0,a]$. Find MLE $a_e$ of $a$. 
	\begin{sol}
		Note that \begin{align*}f_Y(y;a)&=\dfrac{1}{a}\cdot\1_{\qty{y\in[0,a]}}\\&=\dfrac{1}{a}\cdot\1_{\qty{0\leq y_{(1)}\leq\cdots\leq y_{(n)}\leq a}}&\text{where } y_{(1)}=\min y_i\text{ and } y_{(n)}=\max y_i\end{align*} Then, \[L(a)=\dfrac{1}{a^n}\1_{\qty{0\leq y_{(1)}\leq\cdots\leq y_{(n)}\leq a}}\] To maximize $L(a)$, we want to minimize $a^n$. Since $a\geq y_{(n)}$, it must be that $a_e=y_{(n)}$. 
		Here, we call $a_e=y_{(n)}$ an \textit{estimate}, and $\hat{a_{\text{MLE}}}=Y_{(n)}$ an \textit{estimator}.
	\end{sol}
\end{eg}
\begin{eg}
	\textbf{MLE that Does Not Esist}
	\par Suppose $f_Y(y;a)=\dfrac{1}{a},\quad y\in[0, a)$. Find the MLE.
	\begin{sol}
		The likelihood function is the same: \[L(a)=\dfrac{1}{a^n}\1_{\qty{0\leq y_{(1)}\leq\cdots\leq y_{(n)}<a}}.\] However, since $[0,a)$ is not a closed set, the optimization problem $\dsst\max_{a\in[0,a)}L(a)$ does not have a solution. Hence, the estimate does not exist. 
	\end{sol}
\end{eg}
\begin{rmk}
	MLE may not be unique all the time.	
\end{rmk}
\begin{eg}
	\textbf{Multiple MLE Values}
	\par Suppose  $X_1,\dots,X_n\sim\text{Uniform}\qty[a-\dfrac{1}{2},a+\dfrac{1}{2}]$, where $f_X(x;a)=1,\ x\in\qty[a-\dfrac{1}{2}, a+\dfrac{1}{2}]$. Find the MLE. 
	\begin{sol}
		In the indicator function notation, we can rewrite the pdf to be \[f_X(x;a)=\1_{\qty{a-\frac{1}{2}\leq x\leq a+\frac{1}{2}}}=\1_\qty{a-\frac{1}{2}\leq x_{(1)}\leq\cdots\leq x_{(n)}\leq a+\frac{1}{2}}.\] So, the likelihood function will be \[L(a)=\prod_{i=1}^nf_x(x_i;a)=\begin{cases}1,\quad a\in\qty[x_{(n)}-\dfrac{1}{2}, x_{(1)}+\dfrac{1}{2}]\\0,\quad \text{otherwise}.\end{cases}\] So, the $L(a)$ will be maximized whenever $a\in\qty[x_{(n)}-\dfrac{1}{2}, x_{(1)}+\dfrac{1}{2}]$. Therefore, MLE can be any value in the range $\qty[x_{(n)}-\dfrac{1}{2}, x_{(1)}+\dfrac{1}{2}]$. Say, \[a_e=x_{(n)}-\dfrac{1}{2}\quad\text{or}\quad a_e=x_{(1)}-\frac{1}{2}\quad\text{or}\quad a_e=\dfrac{x_{(n)}-\frac{1}{2}+x_{(1)}+\frac{1}{2}}{2}=\dfrac{x_{(n)}+x_{(1)}}{2}.\]
	\end{sol}
\end{eg}
\begin{thm}{MLE for Multiple Parameters}
	In general, we have the likelihood function $L(\theta)$, where $\theta=\qty(\theta_1,\dots,\theta_p)$. To find the MLE, we need \[\pdv{L(\theta)}{\theta_i}=0\quad i=1,\dots,p,\] and the Hessian matrix \[\mqty(\dsst\pdv[2]{L(\theta)}{\theta_i}{\theta_j})_{i,j=1,\dots,p}\coloneqq\mqty(\dsst\pdv[2]{L(\theta)}{\theta_1}&\cdots&\dsst\pdv[2]{L(\theta)}{\theta_1}{\theta_p}\\\vdots&\ddots&\vdots\\\dsst\pdv[2]{L(\theta)}{\theta_p}{\theta_1}&\cdots&\dsst\pdv[2]{L(\theta)}{\theta_p})\] should be negative dfinite. 
\end{thm}
\begin{eg}
	\textbf{MLE for Multiple Parameters: Normal Distribution}
	\par Suppose $Y_1,\dots,Y_n\sim N(\mu,\sigma)$. Then, \[f_{Y_i}(u;\mu,\sigma)=\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\qty(y_i-\mu)^2/\qty(2\sigma^2)}.\] Find the MLE for $\mu$ and $\sigma$.
	\begin{sol}
		The likelihood function will be \[L(\mu,\sigma)=\prod_{i=1}^n\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\qty(y_i-\mu)^2/\qty(2\sigma^2)}.\] Then, we define \[\l(\mu,\sigma)=\ln L(\mu,\sigma)=-\dfrac{n}{2}\ln2\pi-\dfrac{n}{2}\ln\sigma^2-\dfrac{1}{2}\qty(\sigma^2)^-1\sum_{i=1}^n\qty(y_i-\mu)^2.\] Set \[\begin{cases}\dsst\pdv{\l(\mu,\sigma)}{\mu}=0\quad\quad\text{\ding{172}}\\\dsst\pdv{\l(\mu,\sigma)}{\sigma}=0\qquad\text{\ding{173}}\end{cases}\] From \ding{172}, we have \begin{align*}\dfrac{1}{\sigma^2}\sum_{i=1}^n\qty(y_1-\mu)&=0\\\sum_{i=1}^ny_i&=n\mu\implies\boxed{\mu_e=\dfrac{\sum y_i}{n}=\bar{y}}\end{align*} From \ding{173}, by the invariant property of MLE, we instead set \begin{align*}\dsst\pdv{\l(\mu,\sigma)}{\sigma^2}&=0\\-\dfrac{n}{2}\cdot\dfrac{1}{\sigma^2}+\dfrac{1}{2}\qty(\dfrac{1}{\sigma^2})^2\sum_{i=1}^n\qty(y_i-\mu)^2&=0\\\dfrac{1}{2\sigma^2}\qty(-n+\dfrac{1}{\sigma^2}\sum_{i=1}^n\qty(y_i-\mu)^2)&=0\\-n\sigma^2+\sum_{i=1}^n\qty(y_i-\mu)^2&=0\qquad\qquad(\mu_e=\bar{y})\\\sum_{i=1}^n\qty(y_i-\bar{y})^2&=n\sigma^2\\\sigma_e^2=\dfrac{1}{n}\sum_{i=1}^n\qty(y_i-\bar{y})^2&\implies\boxed{\sigma_e=\sqrt{\dfrac{1}{n}\sum_{i=1}^n\qty(y_i-\bar{y})^2}}\end{align*}
	\end{sol}
\end{eg}

\subsection{The Method of Moment}
\begin{df}{Moment Generating Function}
	The \textit{Moment Generating Function (MGF)} is defined as \[\M_X(t)=\E\qty[e^{tX}],\] and it uniquely determines a probability distribution.
\end{df}
\begin{df}{Moment}
	The \textit{$k$-th order moment} of $X$ is $\E\qty[X^k]$.	
\end{df}
\begin{eg}{Meaning of Different Moments}
	\begin{itemize}
		\item $\E[X]$: location of a distribution
		\item $\E\qty[X^2]=\Var(X)-\E[X]^2$: width of a distribution
		\item $\E\qty[X^3]$: skewness -- positively skewed / negatively skewed
		\item $\E\qty[X^4]$: kurtosis / tailedness -- speed decaying to $0$.
	\end{itemize}	
\end{eg}
\begin{eg} 
	\textbf{Moment Estimate: Moments of Population and Sample}
	\begin{center}
	\begin{tabular}{c|c}
		\textbf{Population}&\textbf{Sample, $X_1,\dots,X_n$}\\\hline\\
		$\E[X]=\mu$&$\hat\mu=\bar X=\dfrac{X_1+\cdots+X_n}{n}$\\\\
		$\E\qty[X^2]=\mu^2+\sigma^2$&$\hat\mu^2+\hat\sigma^2=\dfrac{X_1^2+\cdots+X_n^2}{n}$\\
		$\vdots$&$\vdots$\\
		$\E\qty[X^k]$&$\dfrac{X_1^k+\cdots+X_n^k}{n}$
	\end{tabular}
	\end{center}
	\textbf{Rationale}: The population moments should be close to the sample moments. 
\end{eg}
\begin{eg}
	\begin{itemize}
		\item Consider $N(\mu,\sigma^2)$, where $\sigma$ is given. Estimate $\mu$.\par By the method of moment estimate, we have $\mu_e=\bar{X}$.
		\item Consider $N(\mu,\sigma^2)$. Estimate $\mu$ and $\sigma$.\par We have $\mu_e=\bar X$ and $\mu_e^2+\sigma_e^2=\dfrac{X_1^2+\cdots+X_n^2}{n}$.
		\item Consider $N(\theta,\sigma^2)$. Given $\E\qty(X^4)=3\sigma^4$, estimate $\mu$ and $\sigma$.\par We have $\mu_e=\bar X$, $\mu_e^2+\sigma_e^2=\dfrac{X_1^2+\cdots+X_n^2}{n}$, and $3\sigma^4=\dfrac{X_1^4+\cdots+X_n^4}{n}$. We have three equations but only two unknowns, then a solution is not guaranteed. So, we need some restrictions on this method (see Remark 1.2). 
	\end{itemize}
\end{eg}
\begin{thm}{Method of Moments Estimates}
	For a random sample of size $n$ from the discrete (or continuous) population/pdf $p_X(k;\theta_1,\dots,\theta_s)$ (or $f_Y(y;\theta_1,\dots,\theta_s)$), solutions to the system \[\begin{cases}\E(Y)=\dfrac{1}{n}\dsst\sum_{i=1}^ny_i\\\qquad\vdots\\\E\qty(Y^s)=\dfrac{1}{n}\dsst\sum_{i=1}^ny_i^s\end{cases}\] which are denoted by $\theta_{1e},\dots,\theta_{se}$, are called the \textbf{method of moments estimates} of $\theta_1,\dots,\theta_s$.
\end{thm}
\begin{rmk}
	To estimate $k$ parameters with the method of moments estimates, we will only match the first $k$ orders of moments. 
\end{rmk}
\begin{eg}
	Consider the Gamma distribution: \[f_Y(y;r,\lambda)=\dfrac{\lambda^r}{\Gamma(r)}y^{r-1}e^{-\lambda y}\quad\text{for }y\geq0.\] Given $\E(Y)=\dfrac{r}{\lambda}$ and $\E\qty(Y^2)=\dfrac{r}{\lambda^2}+\dfrac{r^2}{\lambda^2}$. Estimate $r$ and $\lambda$.
	\begin{sol}
		\[\E\qty(Y)=\dfrac{r}{\lambda}\implies\dfrac{r_e}{\lambda_e}=\dfrac{y_1+\cdots+y_n}{n}=\bar{y}\qquad\text{\ding{172}}\] \[\E\qty(Y^2)=\dfrac{r}{\lambda^2}+\dfrac{r^2}{\lambda^2}\implies\dfrac{r_e}{\lambda_e^2}+\dfrac{r_e^2}{\lambda_e^2}=\dfrac{y_1^2+\cdots+y_n^2}{n}\qquad\text{\ding{173}}\] Substitute \ding{172} into \ding{173}, we have \[\dfrac{\bar{y}}{\lambda_e}+\qty(\bar{y})^2=\dfrac{1}{n}\sum_{i=1}^ny_i^2\implies \boxed{\lambda_e=\dfrac{\bar{y}}{\frac{1}{n}\sum y_i^2-\bar{y}^2}}\qquad\text{\ding{174}}\] Substitute \ding{174} into \ding{172}, we have \[r_e=\bar{y}\lambda_e=\boxed{\dfrac{\bar{y}^2}{\frac{1}{n}\sum y_i^2-\bar{y}^2}}.\]
	\end{sol}
\end{eg}
\begin{rmk}
	The \emph{sample variance} is defined as \begin{align*}\dfrac{1}{n}\sum_{i=1}^n\qty(y_i-\bar{y})^2&=\dfrac{1}{n}\sum_{i=1}^n\qty(y_i^2-2y_i\bar{y}+\bar{y}^2)\\&=\dfrac{1}{n}\sum_{i=1}^ny_i^2-2\bar{y}\cdot\dfrac{\sum y_i}{n}+\dfrac{1}{n}\cdot n\bar{y}^2\\&=\dfrac{1}{n}\sum_{i=1}^ny_i^2-2\bar{y}^2+\bar{y}^2&\bar{y}=\dfrac{\sum y_i}{n}\\&=\dfrac{1}{n}\sum_{i=1}^ny_i^2-\bar{y}^2.\end{align*} So, in Example 1.3.7, if we define $\hat{\sigma}^2$ to be the sample variance, we can further simply our estimate as follows: \[\lambda_e=\dfrac{\bar{y}}{\hat{\sigma}^2},\qquad r_e=\dfrac{\bar{y}^2}{\hat{\sigma}^2}.\]
\end{rmk}



\end{document}
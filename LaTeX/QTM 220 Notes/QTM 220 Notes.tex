\include{preamble}

\title{Emory University\\\textbf{QTM 220 Regression Analysis}\\ Learning Notes}
\author{Jiuru Lyu}
\date{\today}

\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Statistical Inference}
\subsection{Descriptive Statistics and Binary Covariates}
\begin{df}{Location}
	The \textit{location} of the data is where it is. It is about approximating the data by a constant. \[Y_i\approx\mu,\quad\text{for }i=1,\dots,n\]	
\end{df}
\begin{eg}{}
	Different ways to summarize location: mean, median
\end{eg}
\begin{df}{Spread}
	The \textit{spread} of the data is how far it tends to be from is location. 	
\end{df}
\begin{df}{Residuals}
	Spread summarizes the size of the \textit{residuals} left over after constant approximation. We use $\hat\epsilon$ to denote residuals. \[\residual_i\coloneqq Y_i-\hat\mu.\]
\end{df}
\begin{df}{Median Absolute Deviation and Standard Deviation}
	\begin{itemize}
		\item The \textit{median absolute deviation (MAD)} is the median size of residuals.
		\item The \textit{standard deviation (sd)} is the square root of the mean squared size of residuals. 
		\begin{rmk}The standard deviation is a sort of average in which big residuals count more than smaller ones. \end{rmk}
	\end{itemize}
\end{df}
\begin{df}{Distribution}
	We use \textit{histograms} to summarize the \textit{distribution} of the data. 
\end{df}
\begin{rmk}
	Distribution of the data tells us more information than location and spread, but less than dot plot. \emph{For example, in this context, dot plot also include the identities of the individuals in addition to the number of people having salary in the range. }
\end{rmk}
\begin{df}{Binary Data}
	\textit{Binary data} only have two options, and we usually denote those two options as $1$'s and $0$'s. 	
\end{df}
\begin{cor}{}
	Hence, when drawing a dot plot, everyone falls into either of the two lines representing $1$ and $0$.
\end{cor}
\begin{thm}{Location of Binary Data}
	The median is whichever outcome is the most common, and the mean is the proportion of $1$'s in the data. 
\end{thm}
\begin{rmk}
	Hence, a histogram tells us no more information than $\hat\mu$.	
\end{rmk}
\begin{thm}{Spread of Binary Data}
	\begin{itemize}
		\item Median absolute deviation will always be $0$ in a binary case.
		\item The standard deviation is the square root of the mean squared distance from the mean, and \[\text{sd}=\sqrt{\hat\mu\qty(1-\hat\mu)}.\]
	\end{itemize}
\end{thm}
\begin{prf}
	The claim concerning MAD is trivial. \textit{Hint: there's only two possible values in the data, so median and MAD should always be the same.}\par Now, let's consider the claim on standard deviation. \begin{align*}\sd^2&=\dfrac{1}{n}\sum_{i=1}^n\qty(Y_i-\hat\mu)^2\\&=\dfrac{1}{n}\sum_{y:\qty{0,1}}\sum_{i:Y_i=y}\qty(Y_i-\hat\mu)^2\\&=\dfrac{1}{n}\qty{N_1\qty(1-\hat\mu^2)+\qty(n-N_1)\qty(0-\hat\mu^2)}&[N_1=\text{number of }1\text{'s}]\\&=\dfrac{1}{n}\qty{N_1\qty(1-2\hat\mu+\hat\mu^2)+\qty(n-N_1)\hat\mu^2}\\&=\dfrac{1}{n}\qty{N_1-2N_1\hat\mu+n\hat\mu^2}\\&=\dfrac{1}{n}\qty{n\hat\mu-2n\hat\mu\cdot\hat\mu+n\hat\mu^2}&[N_1=n\hat\mu]\\&=\dfrac{1}{n}\qty{n\hat\mu-n\hat\mu^2}\\&=\hat\mu-\hat\mu^2=\hat\mu\qty(1-\hat\mu).\end{align*} Therefore, we know \[\sd=\sqrt{\hat\mu\qty(1-\hat\mu)}.\]
\end{prf}
\begin{rmk}
	In binary data, knowing the mean $\equiv$ knowing everything else. 
\end{rmk}

\subsection{Population Inference for a Proportion}
\begin{df}{Sampling Distribution}
	The \textit{sampling distribution} is the distribution of estimates we'd get if we \textbf{replicated} our experiment over and over. 
\end{df}
\begin{itemize}
	\item Think of lots of people rolling the dice and reporting what they got. 
	\item We consider this because it actually tells us something: it gives us an \textbf{interval} we can expect the proportion is in, and a statement about how much \textbf{confidence} we should have about it. 
\end{itemize}
\begin{eg}{Connecting Sample and Population}
	For each call $i$, we randomly select a voter with an id we'll call $J_i$. And we record as the call's outcome the turnout of the voter: $Y_i=y_{J_i}$. We can run this simulation using \texttt{R}. 
	\begin{center}
		\includegraphics[width=0.7\textwidth]{figs/SamplingDistribution.png}	
	\end{center}
	\begin{itemize}
		\item The \textit{mean of the sampling distribution} is the solid blue line.
		\item The middle 2/3 of the sampling distribution lies between the dashed blue lines.
		\item The middle 95\% of the sampling distribution lies between the dotted blue lines. 
		\item Also, the population proportion is drawn as a wide green line. 
		\item The question is ``Could we predict how close we can get from the sampling before the election happened?'' -- Yes!
		\begin{itemize}
			\item We will use an \textbf{interval estimate}: a \textit{range of values} the population proportion is likely to be in.
			\item The \textbf{width} of this interval speaks to the ``how close'' question.
			\item The \textbf{coverage probability} (the probability we are right) qualifies this answer.
			\begin{itemize}
				\item Our \textbf{point estimate} of the population proportion is the sample proportion $\bar{Y_n}$, where $n$ is the size of the sample. 
				\item Now, we will try with some size of the interval. Say, $x$. Then, we are interested in the range of data $\bar{Y_n}\pm\dfrac{x}{2}$ (since the interval can be two-tailed).
				\item Repeat the sampling process multiple times, say $M$ times, and we notice that out of $t$ times our interval ``touches'' the population proportion. 
				\item Then, we can define the coverage probability as follows: \[\text{coverage probability}=\dfrac{t}{M}=\P\qty(\bar{Y}_n\in\bar{y}_N\pm \dfrac{x}{2}),\] where $\bar{Y}_n$ is our point estimate, $\bar{y}_N$ is the population proportion, and $x$ is the width of the interval. 
			\end{itemize} 
		\end{itemize}
		\item Most of the time, we would like a 95\% coverage probability, which means we will need to use a wider interval. 
		\item Therefore, what we want to do is to choose a coverage probability and calculate the right width. An interval estimate like this (to ensure a given coverage) is called a \textbf{confidence interval}.
		\item The following figure shows a 95\% coverage probability: \begin{center}\includegraphics[width=0.7\textwidth]{figs/95ConfidenceInterval.png}\end{center}
		\begin{itemize}
			\item Our sample proportion $0.68$ is close to the population proportion $0.69$. Did we get luck? \textit{No! In a million runs, almost all are within 0.05}.
			\item Could we have predicted how close we would get before seeing the $0.69$? \textit{Yes! We can use a calibrated interval estimate -- a Confidence Interval}.
		\end{itemize}
		\item However, notice that this approach is not perfect: we cannot calibrate intervals like this in real life. 
		\begin{itemize}
			\item When we run our pool, we get a single point estimate $\bar{Y}_n$ based on our sample. 
			\item We don't know the sampling distribution of this point estimate until the election day.
			\item However, what we actually do is almost the same: we will use an estimate of the sampling distribution in place of the thing itself. 
		\end{itemize} 
	\end{itemize}
\end{eg}

\subsection{Calibrating Interval Estimates}
\begin{thm}{}
	An interval estimate covers the population proportion $\iff$ the corresponding point estimate is between the population proportion's arms.	
\end{thm}

\begin{rmk}
	With this Theorem, in Example 1.2.2, instead of looking at every interval and its arms to calculate coverage, we can draw arms of the same width around the population proportion. \par Equivalently, we can calculate the \textbf{mass of the histogram} between the population proportion's arms. \par However, even with this Theorem, the problem still exists: unless we've seen the population, we cannot run simulations. Hence, in reality we will do calibration using an \textbf{estimate of the sampling distribution}.
\end{rmk}
\begin{eg}{}
	Here, we use our sample to estimate the sampling distribution. Compared with the actual population mean, the sample mean is a bit lower. Will this impact our estimation?
	\begin{center}\includegraphics[width=0.7\textwidth]{figs/EstimationOfDistribution.png}\end{center}
	\begin{sol}
		It will not because we are not putting arms on draws from the estimated sampling distribution. We are putting arms on our point estimate, which is a draws from the actual sampling distribution. All that matters is the \textbf{width} of the estimated sampling distribution, and not the center. It turns out that the width calculated from the estimated sampling distribution and the population distribution should be the same (or close to the same). 
	\end{sol}
\end{eg}
\begin{thm}{Binomial Distribution Estimation}
	For a binary data type, we collected $Y_1,\cdots,Y_n$ as our sample. The distribution of the sample should follow a \textit{Binomial Distribution}: \[Y_i\sim\text{Binomial}(n,p).\]
\end{thm}
\begin{rmk}
	In theory, $p$ should be calculated from the population. However, in the case of estimation, we will use our sample to estimate $p$. In the binary case, $p=\bar{Y}$.
\end{rmk}
\begin{lstlisting}[title=Binomial Distribution, language=r]
	dbinom(x, n, p) 
	# x = number of heads, n = number of flips, p = probability of heads
\end{lstlisting}
\begin{lstlisting}[title=Binomial Sample, language=r]
	# To draw samples from estimated sampling distribution
	samples <- rbinom(num, n, p)
	# num = total number of draws, 
	# n = number of elements per drawing, 
	# p = probability of head
\end{lstlisting}
\begin{eg}{How does the estimation work}
	\begin{center}\includegraphics[width=0.7\textwidth]{figs/BinomialDistribution.png}\end{center}	
	The Binomial distribution is \textit{continuous} as a function of $p$, so when $p$ cahgnes little, the distribution changes little. That is to say that if we are not far off in the proportion, the estimated and actual sampling distributions are similar. The relevant difference (after centering) is even smaller because the way the binomial changes is mostly location. \par 
	This can be thought of a sort of ``confidence interval'' for our estimate of the sampling distribution. 95\% of the time, we will get an estimate somewhere between the {\color{red}{red}} and {\color{blue}{blue}} ones. As a result, a width of our interval estimate somewhere between the red and blue widths. 
\end{eg}
\begin{eg}{The Bootstrap Interpretation}
	Let's revisit our distribution: \[\text{Binomial}(n,p)/n.\]	This sample distribution indicates the proportion of $1$'s if we poll a sample of $n$ object among whom the proportion of $1$'s is exactly $\bar{Y}=p$. This means that we can get a draw from our estimated sampling distribution by running a ``poll'' of the objects in our sample: rolling a $n$-sided die $n$ times, calling up the corresponding object in our sample, and counting up the $1$'s we observe.  
\end{eg}
\begin{lstlisting}[title=Boostraping Sample, language=r]
bootstrap.samples = array(dim=10000)
for (rr in 1:10000) {
	Y.boot = Y[sample(1:n, n, replace=TRUE] # Boostraping
	bootstrap.samples[rr] = sum(Y.boot)/n
}
\end{lstlisting}


\begin{rmk}[Bootstrap Sampling]
	The usual way of sampling is to use a sample to approximate the population. Since one sample can only generate one estimate, we need many samples. However, we cannot do this in reality. Instead, we only have one sample, so we will use bootstrapping. In that way, we resample the sample multiple times to general different estimates. Using the resampling distribution, we can eventually approximate the population. 
\end{rmk}
\begin{df}{Normal Distribution}
	The \textit{normal distribution} is a function of two parameters: its mean and its standard deviation: \[p_{\mu,\sigma}(x)=\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]
\end{df}
\begin{thm}{Central Limit Theorem}
	The sampling distribution and the bootstrap sampling distribution is approximately normal, and this is a consequence of the Central Limit Theorem. 
\end{thm}

\subsection{Normal Approximation and Power Analysis}
Previously, we have being talking about the context of estimating a \textbf{difference in proportions}. The difference does not have a sampling distribution with a simple parametric form (e.g., the binomial distribution), but we could use the bootstrapping to approximate its sampling distribution. 
\begin{thm}{Approximation using a Normal Distribution}
	A normal approximation $f_{\mu,\sigma}(x)$ for $\mu=p$ and $\sigma=\sqrt{\dfrac{p(1-p)}{n}}$ with $p=\bar{y}$ has a corresponding estimate using the sample: \[\mu=\hat{p},\quad\hat\sigma=\sqrt{\dfrac{\hat p(1-\hat p)}{n}},\quad\text{with}\hat{p}=\bar{Y}.\]
\end{thm}
\begin{thm}{The Width of Normal Distributions}
	\begin{center}
	\begin{tabular}{c|c}
		Data Proportion & $\mu\pm\lambda\sigma$\\
		\hline
		$68.3\%$ & $\mu\pm1\sigma$\\
		$95.4\%$ & $\mu\pm2\sigma$\\
		$99.7\%$ & $\mu\pm3\sigma$\\
		$95\%$ & $\mu\pm1.96\sigma$
	\end{tabular}
	\end{center}
\end{thm}
\begin{eg}{$95\%$ Proportion}
By the CLT, we know $\bar{Y}\sim{N}(\mu,\sigma)$ for $\mu=\bar{y}$ and $\sigma=\sqrt{\bar{y}(1-\bar{y})/n}$. So, in roughly $95\%$ of the polls, our estimate $\bar{Y}$ will be within $1.96$ standard deviations of the population proportion $\bar{y}$. Converly, the population proportion $\bar{y}$ will be within 1.96 standard deviations of our estimate $\bar{Y}$. Or, mathematically, \begin{equation}\label{eq1}\mu\in\bar{Y}\pm1.96\sigma\quad\text{for }\sigma=\sqrt{\bar{y}(1-\bar{y})/n}\text{ in roughly 95\% of polls}.\end{equation}\par 
However, Eq. (\ref{eq1}) is not usable because we do not know the true $\bar{y}$. Therefore, we turn everything into estimate: \begin{equation}\label{eq2; 95 CI}\mu\in\bar{Y}\pm1.96\hat\sigma\quad\text{for }\hat\sigma=\sqrt{\bar{Y}(1-\bar{Y})/n}\text{ in roughly 95\% of polls}.\tag{95\% C.I.}
\end{equation}\par 
With Eq. (\ref{eq2; 95 CI}), we can avoid spending time doing bootstrap. That is, without bootstrapping, we can construct a 95\% confidence interval from the sample mean. 
\end{eg}
\begin{df}{Power Analysis}
	Given the confidence interval (or, margin of error), we can find out the sample size needed.	
\end{df}
\begin{thm}{}
	Suppose we want the estimate $\hat\mu$ with a 95\% confidence interval has a margin of error of $d$. Then, the smallest sample size to achieve so will be $n=\dfrac{1.96^2}{4d^2}$.
\end{thm}
\begin{prf}
	Suppose we want the estimate $\hat\mu$ with a 95\% confidence interval has a margin of error of $d$, then we know $\mu\in\bar{Y}\pm d=\bar{Y}\pm1.96\sqrt{\hat{p}(1-\hat{p})/n}$. Then, we know \begin{equation}\label{eq2} d=1.96\sqrt{\hat{p}(1-\hat{p})/n}.\end{equation} Here, though we are using $\bar{Y}$ to estimate $\hat{p}$, it will not be accurate as different samples will have different $\bar{Y}$'s. So, we have to figure out another way to estimate $\hat{p}$. Before we going to estimating $\hat{p}$, let solve for $n$ from Eq. (\ref{eq2}) first: \[\sqrt{\hat{p}(1-\hat{p})/n}=\dfrac{d}{1.96}\implies\hat{p}(1-\hat{p})/n=\qty(\dfrac{d}{1.96})^2\implies n = \dfrac{1.96^2\hat{p}(1-\hat{p})}{d^2}.\] Now, let's consider $f(\hat{p})=\hat{p}(1-\hat{p})$, a simple quadratic function, which will be maximized when $\hat{p}=\dfrac{1}{2}$. Since we are conservative in our estimation, we want to find the ``smallest'' $n$ that ensures the margin of error, so we take $\max f(\hat{p})=\dfrac{1}{4}$, and thus \[n=\dfrac{1.96^2}{4d^2}.\]
\end{prf}

\newpage
\section{Probability: Expectation and Variance}
\subsection{Probability Review}
\begin{thm}{Properties of Expectations}
	\begin{description}
		\item[Linearity of Expectation] Suppose $Y,Z$ are random variables and $a,b\in\R$. Then \[\E(aY+bZ)= \E(aY)+\E(bZ)=a\E(Y)+b\E(Z).\] 
     	\item[Multiplication Rules] Suppose $Y,Z$ are independent ($\independ$) random variables, then \[\E(Y\cdot Z)=\E(Y)\cdot\E(Z).\]
	\end{description}
\end{thm}
\begin{rmk}
	Without special notice, we assume random variables are independent in this course.	
\end{rmk}
\begin{thm}{Variacne Decomposition}
	If $Y\independ Z$, then $\Var(Y+Z)=\Var(Y)+\Var(Z).$	
\end{thm}
\begin{prf}
	Notice that  \begin{align*}\Var(Y+Z)&=\E[(Y+Z)^2]- \E(Y+Z)^2\\&=\E(Y^2+Z^2+2YZ)-[\E(Y)+\E(Z)]^2 \\
    &=\E(Y^2)+\E(Z^2)+2\E(YZ)-\E(Y)^2-\E(Z)^2-2\E(Y)\E(Z)&[\text{Linearity}]\\&=(\E(Y^2)-\E(Y)^2)+(\E(Z^2)-\E(Z)^2)+2\E(YZ)-2\E(YZ)&[\text{Independence}]\\&=\Var(Y)+\Var(Z).\end{align*}
\end{prf}
\begin{thm}{Binomial Expection and Variance}
	If $Y\sim\text{Binomial}(n,p)$, then \[\E\qty(\bar{Y})= \E\qty[\dfrac{1}{n}\sum_{i=1}^n(Y_i)]=p\] and \[\Var\qty(\bar{Y})=\dfrac{p(1-p)}{n}.\] The proof of these two quantities are omitted. 
\end{thm}
\begin{df}{Conditional Expectation}
    The \textit{conditional expectation} is $\E[Y\mid X=x]$, namely the expected value of $Y$ given that $X=x$.
\end{df}
\begin{thm}{Properties of Conditional Expectation}
	\begin{description}
		\item[Law of Iterated Expectations] for any random variables $X$ and $Y$, we have \[\E(Y)=\E\qty{\E(Y\mid X)}.\]
		\item[Irrelevance of Independent Conditioning Variables] When $Z\independ X$ and $Y$, we have \[\E(Y\mid X,Z)=\E(Y\mid X).\] In other words, if $Z$ is unrelated to $X$ and $Y$, holding it constant does not affect the relationship between $X$ and $Y$.
	\end{description}
\end{thm}

\subsection{Working with Expectations}
Before we going into any details, let's review and introduce some notations.
\begin{nota}
	\begin{itemize}
		\item The mean of our population: $\dsst\mu=\dfrac{1}{m}\sum_{j=1}^my_j$.
		\item The mean of sample: $\hat\mu=\dfrac{1}{n}\dsst\sum_{i=1}^nY_i$.
		\item We use $y$ to denote a number from the list, and $Y$ to represent a random variable. 
		\item To represent the subpopulation, we use $X_i=x$. 
		\item The mean of the subpopulation $X_i=x$ is $\mu(x)=\dfrac{1}{m_x}\dsst\sum_{j:x_j=x}y_j$ where $m_x=\dsst\sum_{j:x_j=x}1$.
		\item The mean of the subsample $X_i=x$ is $\hat\mu(x)=\dfrac{1}{N_x}\dsst\sum_{X_i=x}Y_i$, where $N_x=\dsst\sum_{i:X_i=x}1$.
	\end{itemize}
\end{nota}
\begin{df}{Expectations}
	The \textit{expected value} of a random variable $Y_i$ is the \textbf{probability-weighted average} of the values it can take on. \[\E\qty[Y_i]=\sum_y\P\qty(Y_i=y)\times y=\sum_y\qty(\sum_{j:y_j=y}\dfrac{1}{m})\times y=\dfrac{1}{m}\sum_y\sum_{j:y_j=y}y=\dfrac{1}{m}\sum_{j=1}^my_j.\]
\end{df}
\begin{df}{Independence}
	We say random variables or \textit{independent} ($\independ$) if knowing the value of one does not tell us anything about the value of the other. 	
\end{df}
\begin{cor}{}
	If $Y_1$ and $Y_2$ are independent, \[\P(Y_1=y_1\text{ and }Y_2=y_2)=\P(Y_1=y_1)\P(Y_2=y_2).\]
\end{cor}
\begin{rmk}
	When we draw a sample $Y_1,\dots,Y_n$, we claim $Y_1,\dots,Y_n$ are independent and identically distributed ($\iid$).	
\end{rmk}
\begin{thm}{The Law of Iterated Expectations}
	For any random variables $X$ and $Y$, \[\E(Y)=\E\qty{\E(Y\mid X)}.\]
\end{thm}
\begin{thm}{Irrelevance of Independent Conditional Variables}
	If $X'$ is unrelated to $X$ and $Y$, holding it constant does not affect the relationship between them. Suppose $X'$ is independent of $X$ and $Y$, then \[\E(Y\mid X,X')=\E(Y\mid X).\]	
\end{thm}
\begin{thm}{Linearity of Expectations}
	For random variables $Y$ and $Z$, and numbers $a$ and $b$, \[\E(aY+bZ)=\E(aY)+\E(bZ)=a\E(Y)+b\E(Z).\]
\end{thm}
\begin{prf}
	\begin{align*}
		\E(aY+bZ)&=\sum_y\sum_z(ay+bz)\P(Y=y,Z=z)&\text{def of expectation}\\
		&=\sum_y\sum_zay\P(Y=y,Z=z)+\sum_z\sum_ybz\P(Y=y,Z=z)\\
		&=\sum_yay\sum_z\P(Y=y,Z=z)+\sum_zbz\sum_y\P(Y=y,Z=z)\\
		&=\sum_yay\P(Y=y)+\sum_zbz\sum_y\P(Z=z)\\
		&=a\sum_yy\P(Y=y)+\sum_zz\sum_y\P(Z=z)\\
		&=a\E(Y)+b\E(Z)
	\end{align*}
\end{prf}
\begin{thm}{Linearity of Conditional Expectations}
	For random variables $X$, $Y$, $Z$, and some functions $a$ and $b$ of $X$, \begin{align*}\E\qty{a(X)Y+b(X)Z\mid X}&=\E\qty{a(X)Y\mid X}+\E\qty{b(X)Z\mid X}\\&=a(X)\E(Y\mid X)+b(X)\E(Z\mid X)\end{align*}
\end{thm}
\begin{cor}{}
	When $Y\independ Z$, then $\E\qty[YZ]=\E[Y]\E[Z]$.
\end{cor}
\begin{prf}
	\begin{align*}
		\text{LHS}&=\E[YZ]\\
		&=\E\qty{\E[YZ\mid Z]}&\text{Law of Iterated Expectation}\\
		&=\E\qty[Z\cdot\E\qty[Y\mid Z]]\\
		&=\E\qty[Z\cdot\underbrace{\E[Y]}_\text{constant}]&\text{Indepence}\\
		&=\E[Y]\E[Z]=\text{RHS}&\text{Linearity}
	\end{align*}	
\end{prf}

\begin{thm}{Indicator Trick}
	When $X$ is binary, we have \[\E\qty[X\mu(X)]=\E\qty[X\mu(1)]=\mu(1)\E[X].\] Similarly, \[\E[(1-X)\mu(X)]=\E\qty[(1-X)\mu(0)]=\mu(0)\E(1-X).\]
\end{thm}
\begin{df}{Bias}
	The \textit{bias} of an estimator is the difference between its expected value and the value of the thing it's estimating. When an estimator attains a bias of $0$, we call it an \textit{unbiased} estimator.
\end{df}
\begin{clm}{}
	The sample mean is an unbiased estimator of the population mean:  $\E\qty[\hat{\mu}]=\mu$.
\end{clm}
\begin{prf}
	\[\E\qty[\dfrac{1}{n}\sum_{i=1}^nY_i]=\dfrac{1}{n}\sum_{i=1}^n\E[Y_i] (\text{by linearity})=\dfrac{1}{n}\sum_{i=1}^n\mu=\dfrac{1}{n}\times n\times\mu=\mu.\]
\end{prf}
\begin{clm}
	The subsample mean is unbiased for the subpopulation mean: $\E\qty[\hat{\mu}(1)]=\mu(1).$
\end{clm}
\begin{prf}
	Note that by definition, $\sum_{i=1}^nX_iY_i=\begin{cases}Y_i\quad\text{if }X_i=0\\0\quad\text{if }X_i=0\end{cases}$, so, $\hat{\mu}(1)=\dfrac{\dsst\sum_{i=1}^nX_iY_i}{\dsst\sum_{i=1}^nX_i}$. Then, 
	\begin{align*}
		\E\qty[\hat{\mu}(1)]&=\E\qty[\dfrac{\dsst\sum_{i=1}^nX_iY_i}{\dsst\sum_{i=1}^nX_i}]\\
		&=\E\qty{\E\qty[\dfrac{\dsst\sum X_iY_i}{\underbrace{\sum X_i}_\text{constant}}]\bigg|X_1,\dots,X_n}&\text{Law of Iterated Expectation}\\
		&=\E\qty{\dfrac{\E\qty[\sum X_iY_i\mid X_1,.\dots,X_n]}{\sum X_i}}&\text{Linearity}\\
		&=\E\qty{\dfrac{\sum\qty(X_i\E\qty[Y_i\mid X_1,\dots,X_n])}{\sum X_i}}&\text{Linearity}\\
		&=\E\qty{\dfrac{\sum\qty(X_i\overbrace{\E\qty[Y_i\mid X_i]}^{=\mu(X_i)})}{\sum X_i}}&\text{Irrelavent Expectation: }Y_i\depend X_i \text{ only}\\
		&=\E\qty{\dfrac{\sum\qty(X_i\mu(X_i))}{\sum X_i}}\\
		&=\E\qty{\dfrac{\sum\qty(X_i\mu(1))}{\sum X_i}}&\text{Indicator Trick}\\
		&=\E\qty{\dfrac{\mu(1)\sum X_i}{\sum X_i}}&\text{Linearity}\\
		&=\E\qty(\mu(1))=\mu(1)\implies\text{unbiasedness}
	\end{align*}
\end{prf}
\begin{clm}
	The difference in subsample means is unbiased for the difference in subpopulation means: $\E\qty[\hat{\mu}(1)-\hat{\mu}(0)]=\mu(1)-\mu(0)$.
\end{clm}
\begin{prf}
	This follows from the linearity of expectations and unbiasedness of the subsample means. \[\E\qty[\hat\mu(1)-\hat\mu(0)]=\E\qty[\hat\mu(1)]-\E\qty[\hat\mu(0)]=\mu(1)-\mu(0).\]
\end{prf}

\subsection{Working with Variance}
\begin{df}{Spread and Variance}
	To summarize the \textit{spread} of a distribution, we talk about variance. The \textit{variance}	is an expectation: it is the \textbf{expected squared difference} between a random variable and its expectation: \[\V(Y)=\E\qty[\qty{Y-\E(Y)}^2].\]
\end{df}
\begin{df}{Standard Deviation}
	Standard deviation is the square root of the variance: \[\sd=\sqrt{\V(Y)}.\]
\end{df}
\begin{thm}{Variance as Excess}
	We can think of variance in the idea of spread (the mean square of a centered version of $Y$), but variance can also be defined as the excess: the average amount $Y^2$ exceeds the square of its mean. So, variance can also be defined as \[\V(Y)=\E\qty(Y^2)-\qty{\E(Y)}^2.\]
\end{thm}
\begin{prf}
	\begin{align*}
		\E\qty[\qty{Y-\E(Y)}^2]&=\E\qty[Y^2+\E(Y)^2-2Y\E(Y)]&\text{Complete Square}\\
		&=\E\qty(Y^2)+\E\qty(\underbrace{\E(Y)^2}_\text{constant})-2\E\qty(Y\underbrace{\E(Y)}_\text{constant})&\text{Linearity}\\
		&=\E\qty(Y^2)+\E(Y)^2-2\E(Y)\E(Y)&\text{Linearity}\\
		&=\E\qty(Y^2)+\E(Y)^2-2\E(Y)^2\\
		&=\E\qty(Y^2)-\E(Y)^2.
	\end{align*}	
\end{prf}
\begin{df}{Conditional Variance}
	The \textit{conditional variance} describes the spread of one random variable within groups. The \textit{conditional variance function} is defined as \[\sigma^2(x)\coloneqq\V(Y\mid X=x)=\E\qty[\qty{Y-\E(Y\mid X)}^2\mid X=x]\]
	\begin{itemize}
		\item This is the variance of $Y$, within the subpopulation $X=x$.
		\item It is the conditional expectation function of random variable $\qty{Y-\E(Y\mid X)}^2$.
		\item We call it \textit{the conditional variance of $Y$ given $X=x$}.
	\end{itemize}
	Therefore, the \textit{conditional variance} is defined by \[\sigma^2(X)\coloneqq\V(Y\mid X)=\E\qty[\qty{Y-\E(Y\mid X)}^2\mid X].\]
	\begin{itemize}
		\item This is the variance of $Y$ within a random subpopulation of people.
		\begin{itemize}
			\item It is the conditional variance function evaluated at the random variable $X$.
			\item And the conditional expectation of the random variable $\qty{Y-\E(Y\mid X)}^2$.
		\end{itemize}
		\item We call it \textit{the conditional variance of $Y$ given $X$}.
	\end{itemize}
\end{df}
\begin{thm}{Law of Total Variance}
	The law of total variance relates conditional and unconditional variance: \[\V[Y]=\E\qty{\V(Y\mid X)}+\V\qty{\E(Y\mid X)}.\] This is a useful way to decompose the variance of a random variable. 	
\end{thm}
\begin{prf}
	\begin{align*}
		\text{RHS}&=\E\qty[\E\qty(Y^2\mid X)-\qty{\E(Y\mid X)}^2]+\E\qty(\qty{\E(Y\mid X)}^2)-\qty[\E\qty(\E\qty(Y\mid X))]^2\\
		&=\E\qty(\E\qty(Y^2\mid X))-\E\qty(\E\qty(Y\mid X)^2)+\E\qty(Y\mid X)^2-\qty[\E(\E(Y\mid X))]^2\\
		&=\E\qty(\E\qty(Y^2\mid X))-\E\qty(Y\mid X)^2+\E\qty(Y\mid X)^2-\qty[\E(\E(Y\mid X))]^2\\
		&=\E(Y^2)-\E(Y)^2\qquad[\text{Law of Iterated Expectation}]\\
		&=\V[Y]=\text{LHS}
	\end{align*}	
\end{prf}
\begin{thm}{The Variance of the Mean}
	Suppose $\bar Y$ is the sample mean, $n$ is the sample size, and $\sigma$ is the population standard deviation, then  $\V\qty(\bar Y)=\dfrac{\sigma^2}{n}$.
\end{thm}
\begin{prf}
	\begin{align*}
		\V\qty(\bar{Y})&=\E\qty[\qty{\dfrac{1}{n}\sum_iY_i-\E\qty(\dfrac{1}{n}\sum_iY_i)}^2]&\text{definition}\\
		&=\E\qty[\qty{\dfrac{1}{n}\sum_i\qty(Y_i-\E\qty(Y_i))}^2]&\text{Linearity}\\
		&=\E\qty[\qty{\dfrac{1}{n}\sum_iZ_i}^2]&\text{define }Z_i\equiv Y_i-\E\qty(Y_i)\\
		&=\E\qty[\dfrac{1}{n^2}\sum_i\sum_jZ_iZ_j]&\text{Complete Square}
	\end{align*}
	\begin{lem*} If $Z_i\independ Z_j$, then $\E\qty[Z_iZ_j]=\E[Z_i]\E[Z_j]=0$.
	\begin{prf*}
		\begin{align*}
			\E\qty[Z_iZ_j]&=\E[Z_i]\E[Z_j]&\text{Independence}\\
			&=\E\qty[Y_i-\E\qty(Y_i)]\E\qty[Y_j-\E\qty(Y_j)]&\text{Definition of $Z_i$}\\
			&=\qty(\E\qty[Y_i]-\E\qty[\overbrace{\E\qty(Y_i)}^\text{constant}])\qty(\E\qty[Y_j]-\E\qty[\overbrace{\E\qty(Y_j)}^\text{constant}])\\
			&=\qty(\E(Y_i)-\E(Y_i))\qty(\E(Y_j)-\E(Y_j))\\
			&=0.
		\end{align*}
		 The proof is thereby completed.
	\end{prf*}
	\end{lem*}
Further note that \begin{align*}
	\V\qty(\bar{Y})&=\E\qty[\dfrac{1}{n^2}\sum_i\sum_jZ_iZ_j]\\
	&=\dfrac{1}{n^2}\sum_i\sum_j\E[Z_iZ_j]&\text{Linearity}\\
	&=\dfrac{1}{n^2}\sum_i\qty(\sum_{j\neq i}\E\qty[Z_i^2]+\E[Z_iZ_j])&\text{Take out terms with $i=j$}\\
	&=\dfrac{1}{n^2}\sum_i\qty(\sum_{j\neq i}\E\qty[Z_i^2]+\E[Z_i]\E[Z_j])
\end{align*}
By the Lemma, we know $\E[Z_i]\E[Z_j]=0$ as $Z_i\independ Z_j$ whenever $i\neq j$. Therefore, \begin{align*}
	\V\qty(\bar{Y})&=\dfrac{1}{n^2}\sum_i\qty(\sum_{j\neq i}\E\qty[Z_i^2]+0)&\text{By Lemma}\\
	&=\dfrac{1}{n^2}\sum_i\qty(\sum_{j\neq i}\E\qty[Z_i^2])&\text{Not related to $j$}\\
	&=\dfrac{1}{n^2}\sum_i\E\qty[Z_i^2]\\
	&=\dfrac{1}{n^2}\cdot n\cdot\E\qty[\qty(Y_i-\E[Y_i])^2]&\text{Definition of $Z_i$}\\
	&=\dfrac{1}{n}\V\qty(Y_i)&\text{Definition of Variance}\\
	&=\dfrac{\sigma^2}{n}.
\end{align*}
\end{prf}
\begin{clm}
	The Variance of our point estimate is the \textbf{expected value} of the subpopulation variance divided by the number of poeple in the sub sample. That is, \[\V\qty[\hat\mu(1)]=\E\qty[\dfrac{\sigma^2(1)}{N_1}]\quad\text{for}\quad N_1=\sum_{i=1}^nX_i.\]	
\end{clm}
\begin{prf}
	Recall that $\hat\mu(1)$ is conditionally unbiased. That is, $\E\qty(\hat\mu(1)\mid X_1,\dots,X_n)=\mu(1)$. So, by the law of total variance, 
	\begin{align*}
		\V\qty[\hat\mu(1)]&=\E\qty[\V\qty{\hat\mu(1)\mid X_1,\dots,X_n}]+\V\qty[\E\qty{\hat\mu(1)\mid X_1,\dots,X_n}]\\
		&=\E\qty[\V\qty{\hat\mu(1)\mid X_1,\dots,X_n}]+\V\qty[\underbrace{\mu(1)}_\text{constant}]\\
		&=\E\qty[\V\qty{\hat\mu(1)\mid X_1,\dots,X_n}]&\text{variance of constants=0}\\
		&=\E\qty[\E\qty{\qty(\hat\mu(1)-\mu(1))^2\mid X_1,\dots,X_n}]&\text{definition of variance}\\
		&=\E\qty[\E\qty{\qty(\dfrac{\sum X_iY_i}{\sum X_i}-\dfrac{\mu(1)\sum X_i}{\sum X_i})^2\mid X_1,\dots,X_n}]\\
		&=\E\qty[\E\qty{\qty(\dfrac{\sum X_i(Y_i-\mu(1))^2}{\sum X_i})^2\mid X_1,\dots,X_n}]\\
		&=\E\qty[\dfrac{\E\qty{\qty[\sum X_i(Y_i-\mu(1)]^2}\mid X_1,\dots,X_n}{\qty(\sum X_i)^2}]
	\end{align*}
	Define $Z_i=X_i\qty{Y_i-\mu(1)}$, then \[\qty(\sum_iZ_i)^2=\qty(\sum_iZ_i)\qty(\sum_jZ_j)=\sum_i\sum_jZ_iZ_j.\] Then, \begin{align*}
		\V\qty[\hat\mu(1)]&=\E\qty[\dfrac{\E\qty{\qty[\sum X_i(Y_i-\mu(1)]^2}\mid X_1,\dots,X_n}{\qty(\sum X_i)^2}]\\
		&=\E\qty[\dfrac{\dsst\E\qty{\qty(\sum_iZ_i)^2\mid X_1,\dots,X_n}}{\qty(\sum X_i)^2}]\\
		&=\E\qty[\dfrac{\dsst\E\qty{\sum_i\sum_jZ_iZ_j\mid X_1,\dots,X_n}}{\qty(\sum X_i)^2}]\\
		&=\E\qty[\dfrac{\dsst\sum_i\sum_i\E\qty(Z_iZ_j\mid X_1,\dots,X_n)}{\qty(\sum X_i)^2}]&\text{Linearity}\\
		&=\E\qty[\dfrac{\dsst\sum_i\sum_j\E\qty(Z_iZ_j\mid X_i,X_j)}{\qty(\sum X_i)^2}]&\text{Condition on Irrelevance}
	\end{align*}
	\begin{lem*}
		Suppose $Z_j=Y_j-\mu(X_j)$, then $\E\qty(Z_j\mid X_j)=0$.
		\begin{prf*}
			\begin{align*}
				\E(Z_j\mid X_j)&=\E\qty[X_j(Y_j-\mu(X_j)]\\
				&=\E\qty{\E\qty[X_j(Y_j-\mu(1)\mid X_j]}\\
				&=\E\qty{X_j\E\qty{[Y_j\mid X_j]-\mu(X_j}}\\
				&=\E\qty{X_j\cdot\qty{\mu(X_j)-\mu(X_j}}=0
			\end{align*}
		The lemma is therefore proved as desired.
	\end{prf*}	
	\end{lem*}
	\begin{clm*}
		Suppose $Z_j=Y_j-\mu(X_j)$, then $\E\qty{Z_iZ_j\mid X_i,X_j}=0$.
	\begin{prf*}
		\begin{align*}
			\E\qty{Z_iZ_j\mid X_i,X_j}&=\E\qty{\E\qty{Z_iZ_j\mid Z_i,X_i,X_j}}&\text{Law of Iterated Expectation}\\
			&=\E\qty{Z_i\E\qty{Z_j\mid X_i,X_j}}&\text{Constant}\\
			&=\E\qty{Z_i\E\qty{Z_j\mid X_j}}&\text{Independence}\\
			&=\E\qty{Z_i\cdot0}=0&\text{By Lemma}
		\end{align*}
		Hence, the proof is completed. 
	\end{prf*}
	\end{clm*}
	Therefore, 
	\begin{align*}
		\V\qty(\hat\mu(1)) &=\E\qty[\dfrac{\dsst\sum_i\sum_j\E\qty(Z_iZ_j\mid X_i,X_j)}{\qty(\sum X_i)^2}]&Z_i=Y_i-\mu(X_i)\\
		&=\E\qty[\dfrac{\sum\E\qty(Z_i^2\mid X_i)}{\qty(\sum X_i)^2}]&\text{By Claim, Corss-Terms=0}
	\end{align*}
	\begin{clm*}
		Suppose $Z_i=X_i\qty(Y_i-\mu(X_i))$, then $\E\qty(Z_i^2\mid X_i)=X_i\sigma^2(1)$.
		\begin{prf*}
			\begin{align*}
				\E\qty(Z_i^2\mid X_i)&=\E\qty[X_i\qty{Y_i-\mu(X_i)}^2\mid X_i]\\
				&=X_i\E\qty[\qty(Y_i-\mu(X_i))^2\mid X_i]\\
				&=X_i\V\qty(Y_i\mid X_i)\\
				&=X_i\sigma^2\qty(X_i)\\
				&=X_i\sigma^2(1).
			\end{align*}
			The claim is therefore proved.
		\end{prf*}
	\end{clm*}
	Then, by the Claim and Linearity of Expectation, we have \begin{align*}
		\V\qty(\hat\mu(1))=\E\qty[\dfrac{\sum\E\qty(Z_i^2\mid X_i)}{\qty(\sum X_i)^2}]&=\E\qty[\dfrac{\sum X_i\overbrace{\sigma^2(1)}^\text{constant}}{\qty(\sum X_i)^2}]\\
		&=\E\qty[\dfrac{\sigma^2(1)\sum X_i}{\qty(\sum X_i)^2}]\\
		&=\E\qty[\dfrac{\sigma^2(1)}{\sum X_i}].
	\end{align*}
	Recall that $\sum X_i=N_1$, substitute, by the Linearity of Expectation, we have \[\V\qty(\hat\mu(1))=\E\qty[\dfrac{\overbrace{\sigma^2(1)}^\text{constant}}{N_1}]=\sigma^2(1)\cdot\E\qty[\dfrac{1}{N_1}].\]
\end{prf}
\begin{clm}
	The variance of the difference in subsample means is the \textbf{sum} of the variances of the subsample means themselves. \[\V\qty[\hat\mu(1)-\hat\mu(0)]=\E\qty[\dfrac{\sigma^2(1)}{N_1}+\dfrac{\sigma^2(0)}{N_0}]\quad\text{for } N_x=\sum_{i=1}^n\1_x(X_i),\text{ where }\1_x=\begin{cases}1\quad\text{if } X_i=x\\0\quad\text{o/w}\end{cases}\]	
\end{clm}
\begin{rmk}
	This claim is crucial for power calculations and analysis.
\end{rmk}


\newpage
\section{Causal Inference}
\subsection{Introduction to Randomized Experiments}
\begin{df}{Causal Inference}
	The formal approach to \textit{casual inference} reframes the problem of causal inference in terms of counterfactuals which are treated as missing data. 
\end{df}
\begin{nota} Potential Outcomes and Treatment Effects
\begin{itemize}
	\item Observable Stuff
	\begin{itemize}
		\item $Y$ is an outcome variable
		\item $D$ is a treatment variable
	\end{itemize}
	\item Conceptual Stuff
	\begin{itemize}
		\item Potential Outcomes
		\begin{itemize}
			\item $Y_i(1)$ is the outcome individual $i$ would have attained if they'd received treatment $D_i=1$.
			\item $Y_i(0)$ is the outcome individual $i$ would have attained if they'd not received treatment $D_i=0$.
		\end{itemize}
		\item Individual-level Treatment Effect: It is the \emph{difference} in what would happen with the two treatments. We call this $\tau_i$. Naturally, we can never observe this.
	\end{itemize}
\end{itemize}
\end{nota}
\begin{eg}{}
	\begin{center}
		\begin{tabular}{c|ccc|cc}
			& In & Counterfactual & World & In & Reality \\
			\hline
			$i$ & $Y_i(1)$ & $Y_i(0)$ & $\tau_i$ & $D_i$ & $Y_i$\\
			Individual \# & Vote with Letter& Vote w/o Letter & Effect & Received Letter & Vote\\
			\hline
			$1$ & $1$ & {\color{orange}$0$} & {\color{orange}$1$} & $1$ & $1$\\
			$2$ & $1$ & {\color{orange}$1$} & {\color{orange}$0$} & $1$ & $1$\\
			$3$ & {\color{orange}{$0$}} & $0$ & {\color{orange}$0$} & $0$ & $0$\\
			$4$ & {\color{orange}{$0$}} & $1$ & {\color{orange}$-1$} & $0$ & $1$\\
			\vdots
		\end{tabular}
	\end{center}
	In the table above, black are what is observable (happening in the reality), and orange are what is not observable (happening in the counterfactual world). 
\end{eg}
\begin{rmk}
	A sample without counterfactual information can tell us about averages of quantities involving counterfactual. 	
\end{rmk}
\begin{thm}{Randomized Experiments}
	\begin{description}
		\item[Fundamental Problem of Causal Inference:]	We cannot observe the individuals' outcome under treatment and control at the same time.
		\item[Solution:] Randomized experiments. This means that we can think of the treated and untreated subsamples drawn from our population by a ``coin-flipping'' process. If we ignore the stuff downstream of treatment assignment, we can therefore consider people in those two subsamples have the same distribution. 
	\end{description}
To estimate the treatment effect, \[\ATE=\E\qty[\tau]=\E\qty[Y(1)-Y(0)]=\E\qty[Y(1)]-\E\qty[Y(0)].\] However, here, we have to consider the subpopulations. If we take sample large enough and perform the randomization well enough, the following is true: \[\E\qty[Y_i(1)\mid D_i=1]\approx\E\qty[Y_i(1)]\quad\text{and}\quad\E\qty[Y_i(0)\mid D_i=0]\approx\E\qty[Y_i(0)].\] Therefore, the average treatment effect can be estimated as \[\hat\ATE=\E\qty[\hat\tau]=\E\qty[Y_i(1)\mid D_i=1]-\E\qty[Y_i(0)\mid D_i=0].\]
\end{thm}
\begin{rmk}
	The key of randomization is to make the treated subgroup and untreated subgroup representative of the population treated outcome and untreated outcome.
\end{rmk}
\begin{thm}{Expectation and Variance of Treatment Effect}
	In a randomized experiments, let's use $\hat\mu(1)$	to denote the sample mean of the treated group and $\hat\mu(0)$ to denote the sample mean of the untreated group. Then, \[\E\qty[\hat\tau]=\E\qty[\hat\mu(1)-\hat\mu(0)]=\E\qty[\hat\mu(1)]-\E\qty[\hat\mu(0)].\]
	Further, suppose $\sigma^2(0)$ is the sample variance of the treated group and $\sigma^2(1)$ the sample variance of the untreated group, then \[\V\qty[\hat\tau]=\E\qty[\dfrac{\sigma^2(1)}{N_1}+\dfrac{\sigma^2(0)}{N_0}],\] where $N_1=\dsst\sum_{i=1}^n X_i$, $N_0=\dsst\sum_{i=1}^n(1-X_i)$, and $n=N_1+N_2$.
\end{thm}
\begin{thm}{Condition for Variance of Treatment Effect to be Minimized}
	When $N_1=N_0=\dfrac{n}{2}$, we attain the minimum for the variance of treatment effect. 	
\end{thm}


\end{document}
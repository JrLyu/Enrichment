\documentclass[12pt, a4paper]{article}

\usepackage[framemethod=TikZ]{mdframed}
\usepackage[hidelinks]{hyperref}
\usepackage[T1]{fontenc}
\usepackage{mathtools, amssymb, ntheorem, amsmath, cleveref, fancyhdr, lastpage, geometry, arydshln, url, setspace, framed, pifont, physics, framed}

\geometry{a4paper, left=2cm, right=2cm, bottom=2cm, top=2cm}

\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\rightmark}
\fancyfoot{}
\fancyfoot[C]{\thepage}
%\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\linespread{1.25}

\hypersetup{
	colorlinks = true,
	bookmarks = true,
	bookmarksnumbered = true,
	pdfborder = 001,
	linkcolor = blue
}

\theorembodyfont{\upshape}
\theoremseparator{.}
\newtheorem{thm}{Theorem}[subsection]
\newtheorem{df}{Definition}[subsection]
\newtheorem{eg}{Example}[subsection]
\newtheorem{cor}{Corollary}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{clm}{\indent Claim}[section]
\newtheorem*{rmk}{\indent Remark}
\newenvironment*{sol}{\par\indent\textbf{\textit{Solution. }}}{\hfill{$\square$}\par}

\newenvironment*{prf}{\par\indent\textbf{\textit{Proof. }}}{\hfill $\blacksquare$\par}
\newtheorem*{ext}{\indent Extension}

\def\Z{{\mathbb{Z}}}
\def\R{{\mathbb{R}}}
\def\C{{\mathbb{C}}}
\def\Q{{\mathbb{Q}}}
\def\E{{\mathbb{E}}}
\def\T{{\vb{T}}}
\def\d{{\mathrm{d}}}
\def\i{{\mathrm{i}}}
\def\RE{{\mathrm{Re}}}
\def\IM{{\mathrm{Im}}}
\def\Arg{{\mathrm{Arg}}}
\def\cis{\mathrm{cis}}
\def\rref{\mathrm{rref}}
\def\Span{\mathrm{Span}}
%\def\ker{\mathrm{Ker}}
\def\rank{\mathrm{rank}}
\def\nullity{\mathrm{nullity}}
\def\Proj{\mathrm{Proj}}
\def\Ref{\mathrm{Ref}}
\def\vol{\mathrm{vol}}
\def\almu{\mathrm{almu}}
\def\gemu{\mathrm{gemu}}
\def\qed{\rightline{$\blacksquare$}}
\def\ddx{\frac{\d}{\d x}}
\def\dydx{\frac{\d y}{\d x}}
\def\dx{\d x}
\def\vecx{\va{x}}
\def\vecy{\va{y}}
\def\vecv{\va{v}}
\def\vecw{\va{w}}
\def\vecu{\va{u}}
\def\veca{\va{a}}
\def\vecb{\va{b}}
\def\vece{\va{e}}
\def\vecr{\va{r}}
\def\matrixA{\vb{A}}
\def\matrixB{\vb{B}}
\def\matrixC{\vb{C}}
\def\matrixD{\vb{D}}
\def\matrixI{\vb{I}}
\def\matrixM{\vb{M}}
\def\matrixN{\vb{N}}
\def\matrixS{\vb{S}}
\def\matrixP{\vb{P}}
\def\matrixU{\vb{U}}
\def\matrixV{\vb{V}}
\def\matrixSig{\vb{\Sigma}}
\def\DNE{\mathrm{D.N.E.}}
\def\LI{\mathrm{L.I.}}
\def\dsst{\displaystyle}

\title{Johns Hopkins University\\\textbf{AS.110.201 Linear Algebra}\\Learning Notes}
\author{Jiuru Lyu}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\newpage
\section*{Preface}
These are my personal notes for Johns Hopkins University AS.110.201 Linear Algebra course. I studied this course via Summer @ Hopkins in the summer of 2021. 

As no prerequisite is required (only pre-calculus, basic algebra, and some simple knowledge from Calculus I), this course focuses on matrices. It includes systems of linear equations, basics of matrices, spaces and dimensions, determinants, eigenvalues, and singular value decomposition. The textbook used for this course is \textit{Linear Algebra with Applications, 5th Edition} by Otto Bretscher. Another textbook by Gilbert Strang is also recommended: \textit{Introduction to Linear Algebra, 5th Edition}. 

Throughout this personal note, I use different formats to differentiate different contents, including definitions, theorems, proofs, examples, extensions, and remarks. To be more specific: 
\begin{df}[Terminology]
    This is a \textbf{definition}.	
\end{df}
\begin{thm}[Theorem Name]
    This is a \textbf{theorem}.	
\end{thm}
\begin{eg}
    This is  an \textbf{example}. 
\end{eg}

\begin{sol}
    This is the \textit{answer} part of an \textbf{example}. 
\end{sol}
\begin{rmk}
	This is a \textbf{remark} of a definition, theorem, example, or proof. 
\end{rmk}

\begin{prf}
	This is a \textbf{proof} of a theorem. 
\end{prf}
\begin{ext}
	This is a \textbf{extension} of a theorem, proof, or example. 	
\end{ext}

To better ace this course, it is recommended to do more questions than provided as examples under each section. Although each example is distinctive and representative, more questions and practice is still needed to deepen the understanding of this course. More than doing examples, using visualization tools to visualize some problems or concepts is also helpful in understanding the contents better. Videos made by \textbf{3Blue1Brown} are also recommended as a supplementary source of learning. 

Even though I put efforts into making as few flaws as possible when encoding these learning notes, some errors may still exist in this note. If you find any, please contact me via email: \url{lvjiuru@hotmail.com}. 

I hope you will find my notes helpful when learning Linear Algebra, a fundamental course for other Math and Computer Science courses. 

\rightline{Cheers,}
\rightline{Jiuru Lyu}

\newpage
\section{Systems of Linear Equations}
\subsection{Solving Systems of Linear Equations}

\begin{df}[Linear Equations]
An equation in the unknowns $x$, $y$, $z$, ... is called \textbf{linear} if both sides of the equation are a sum of multiples of $x$, $y$, $z$, ..., plus an optional constant. 
\end{df}
\begin{eg}
Linear equations and nonlinear equations\par
\begin{center}
$\left\{\begin{aligned}3x+4y&=2z\\-x-z&=100\end{aligned}\right.$ are linear equations,  but $\left\{\begin{aligned}3x+yz&=3\\\sin x-\cos y&=2\end{aligned}\right.$ are not.
\end{center}
\end{eg}

\begin{df}[System of Linear Equations]
A \textbf{system} of linear equations is a collection of several linear equations.
\end{df}

\begin{df}[Solution of a System]
A \textbf{solution} of a system of equations is a list of numbers $x$, $y$, $z$, ... that make all of the equations true simultaneously.
\end{df}

\begin{df}[Solution Set of a System]
The \textbf{solution set} of a system of equations is the collection of all solutions.
\end{df}

\begin{df}[Solving a System]
\textbf{Solving} the system means finding all solutions with formulas involving some number of parameters.
\end{df}

\begin{df}[Consistency and Inconsistency of a System]
A system of equations is called \textbf{inconsistent} if it has no solutions. It is called \textbf{consistent} otherwise. 
\end{df}

\begin{eg}
An inconsistent system: \par

\begin{center} $\left\{\begin{aligned}x+2y&=3\\x+2y&=-3\end{aligned}\right.$ has no solutions (the solution set is \textit{empty}).
\end{center}
Thus, the system of equations is \textbf{inconsistent}.
\end{eg}

\begin{rmk}
A solution of equations in $n$ variables is a list of $n$ numbers.
\end{rmk}

\begin{rmk}
We use $\R$ to denote the set of all real numbers.
\end{rmk}

\begin{df}[$\R^n$]
Let $n$ be a positive whole number. We define
$$\R^n=\text{all ordered }n\text{-tuples of real numbers }(x_1,\ x_2,\ x_3,\ ...,\ x_n)$$
An $n$-tuple of real number is called a \textbf{point} of $\R^n$
\end{df}
\begin{eg}
Examples of $\R^n$
\begin{enumerate}
\item $\left[0, \dfrac{3}{2},-\pi\right]$ and $(1, -2, 3)$ are points of $\R^3$
\item When $n=1$, $\R^1=\R$. Geometrically, this is the number line.
\item When $n=2$, $\R^2$. It becomes the $xy$-plane. 
\item When $n=3$, $\R^3$. It is the \textit{space} we live in. 
\end{enumerate}
\end{eg}


\begin{df}[Line]
A \textbf{line} is a ray that is \textit{straight} and \textit{infinite} in both directions.
\end{df}
\begin{df}[Plane]
A \textbf{plane} is a flat sheet that is infinite in all directions.
\end{df}
\begin{thm}
Generally, a single linear equation in $n$ variables defines an $(n-1)$-plane in $n$-space. 
\end{thm}

\begin{eg}
Examples of Lines and Planes. \par
\begin{enumerate}
\item \textbf{Lines}. For $x+y=1$ (implicit equation), the \textbf{parametric form} is $$(x,y)=(t, 1-t)\ \text{for any } t \in \R$$
We call $t$ a \textbf{parameter} in this case.
\item For a system of two linear equations (as implicit equations in $\R^3$) $$\left\{\begin{aligned}x+y+z&=1\\x-z&=0\end{aligned}\right., $$ the parametric form would be $$(x,y,z)=(t,1-2t,t)$$
\item \textbf{Planes}. For $x+y+z=1$ (implicit equation), the \textbf{parametric form} is $$(x,y,z)=(1-t-w,t,w) \text{ for any } t,w \in \mathbb{R}$$
\end{enumerate}
\end{eg}

\begin{thm}[Elementary Operations] Since elementary operations are reversible, the solution set doesn't change:
\begin{enumerate}
\item Switch the order of the equation;
\item Scale the equation by a scale $c\neq 0$; (to reverse, divide equation by $c$)
\item Add a multiple of one equation to another. (to reverse, subtract)
\end{enumerate}
\end{thm}

\subsection{Row Reduction}
\begin{thm}[The Elimination Method] We can use the \textbf{elimination} method to combine the equations in various ways to eliminate as many variables as possible for each equation.
\begin{enumerate}
\item \textbf{Scaling}. We can multiply both sides of an equation by a nonzero number. 
\item \textbf{Replacement}. We can add a multiple of one equation to another, replacing the second equation with the result. 
\item \textbf{Swap}. We can swap two equations.
\end{enumerate}
\end{thm}

\begin{df}[Augmented Matrices and Row Operations]
\textbf{Augmented Matrix} refers to the vertical line, which we draw to remind ourselves where the equals sign belongs.
\end{df}

\begin{df}[Matrix]
A \textbf{matrix} is a grid of numbers without the vertical line. 
\end{df}

\begin{eg}
Augmented Matrix and Row Operations. \par
\begin{center}
$\left[\begin{array}{c:c}\begin{matrix}1&2&3\\2&-3&2\\3&1&-1\end{matrix}&\begin{matrix}6\\14\\-2\end{matrix}\end{array}\right]$ is an augmented matrix. 
\end{center}
The three ways of manipulating our equations become row operations:
\begin{enumerate}
\item \textbf{Scaling}. multiply all entries in a row by a nonzero number. 
$$\left[\begin{array}{c:c}\begin{matrix}1&2&3\\2&-3&2\\3&1&-1\end{matrix}&\begin{matrix}6\\14\\-2\end{matrix}\end{array}\right]\xrightarrow[]{R_1=R_1\times-3} \left[\begin{array}{c:c}\begin{matrix}-3&-6&-9\\2&-3&2\\3&1&-1\end{matrix}&\begin{matrix}-18\\14\\-2\end{matrix}\end{array}\right]$$
\begin{rmk}Here, the notation $R_1$ simply means "the first row."\end{rmk}
\item \textbf{Replacement}. add a multiple of one row to another, replacing the second row with the result. 
$$\left[\begin{array}{c:c}\begin{matrix}1&2&3\\2&-3&2\\3&1&-1\end{matrix}&\begin{matrix}6\\14\\-2\end{matrix}\end{array}\right]\xrightarrow[]{R_2=R_2-2\times R_1} \left[\begin{array}{c:c}\begin{matrix}1&2&3\\0&-7&-4\\3&1&-1\end{matrix}&\begin{matrix}6\\2\\-2\end{matrix}\end{array}\right]$$
\item \textbf{Swap}. Interchange two rows. 
$$\left[\begin{array}{c:c}\begin{matrix}1&2&3\\2&-3&2\\3&1&-1\end{matrix}&\begin{matrix}6\\14\\-2\end{matrix}\end{array}\right]\xrightarrow[]{R_1\leftrightarrow R_3} \left[\begin{array}{c:c}\begin{matrix}-3&-6&-9\\2&-3&2\\3&1&-1\end{matrix}&\begin{matrix}-18\\14\\-2\end{matrix}\end{array}\right]$$
\end{enumerate}
\end{eg}

\begin{df}[Row equivalent]
Two matrices are called \textbf{row equivalent} if one can be obtained from the other by doing some number of row operations. 
\end{df}

\begin{df}[Row Echelon Form (\emph{ref}) of Matrix]
A matrix is in \textbf{row echelon form} if: 
\begin{enumerate}
\item All zero rows are at the bottom.
\item The first nonzero entry of a row is to the \textit{right} of the first nonzero entry of the row above. 
\item Below the first nonzero entry of a row, all entries are zero.
\end{enumerate}
\end{df}

\begin{eg} 
General \emph{ref} of matrices. 
$$\left[\begin{array}{c:c}\begin{matrix}\textcolor{red}{a}&b&b&b\\0&\textcolor{red}{a}&b&b\\0&0&0&\textcolor{red}{a}\\0&0&0&0\end{matrix}&\begin{matrix}b\\b\\b\\0\end{matrix}\end{array}\right], $$

where $b=$ is any number, and $\textcolor{red}{a}=$ is any nonzero number.
\end{eg}
\begin{df}[Pivot]
A \textbf{\textit{pivot}} is the first nonzero entry of a row of a matrix in row echelon form.
\end{df}

\begin{df}[Reduced Row Echelon Form (\emph{rref}) of a Matrix]
A matrix is in \textbf{reduced row echelon form} if it is in row echelon form, and in addition:
\begin{enumerate}
\item[4.] Each pivot is equal to 1. 
\item[5.] Each pivot is the only nonzero entry in its column.
\end{enumerate}
\end{df}

\begin{eg}
Genderal \emph{rref} of matrices
$$\left[\begin{array}{c:c}\begin{matrix}\textcolor{red}{1}&0&b&0\\0&\textcolor{red}{1}&b&0\\0&0&0&\textcolor{red}{1}\\0&0&0&0\end{matrix}&\begin{matrix}b\\b\\b\\0\end{matrix}\end{array}\right], $$

where $b=$ is any number, $\textcolor{red}{1}=$ pivot
$$\left[\begin{array}{c:c}\begin{matrix}1&0&0\\0&1&0\\0&0&1\end{matrix}&\begin{matrix}1\\-2\\3\end{matrix}\end{array}\right]\xrightarrow[]{becomes} \left\{\begin{aligned}x&=1\\y&=-2\\z&=3\end{aligned}\right.$$
\end{eg}

\begin{thm}
Every matrix is row equivalent to one and only one matrix in reduced row echelon form.

\textbf{\textit{Row reduction}} or \textbf{\textit{Gaussian elimination}} demonstrates that every matrix is row equivalent to a least one matrix in reduced row echelon form. 
\begin{enumerate}
\item Swap the 1$^\text{st}$ row with a lower one, so a leftmost nonzero entry is in the 1st row (if necessary).
\item Scale the 1$^\text{st}$ row so that its first nonzero entry equals 1.
\item Use row replacement, so all entries below this 1 are 0.
\item Swap the 2$^\text{nd}$ row with a lower one so that the leftmost nonzero entry is in the 2nd row.
\item Scale the 2$^\text{nd}$ row so that its first nonzero entry equals 1.
\item Use row replacement, so all entries below this 1 are 0.
\item Swap the 3$^\text{rd}$ row with a lower one so that the leftmost nonzero entry is in the 3rd row.
etc.
\item Use row replacement to clear all entries above the pivots, starting with the last pivot.
\end{enumerate}
\end{thm}

\begin{df}[Pivot Position]
A \textbf{pivot position} of a matrix is an entry that is a pivot of a row echelon form of that matrix.
\end{df}
\begin{df}[Pivot Column]
A \textbf{pivot column} of a matrix is a column that contains a pivot position.
\end{df}

\begin{thm}[The Row Echelon Form of an Inconsistent System] An augmented matrix corresponds to an inconsistent system of equations if and only if (\emph{iff}) the last column (i.e., the augmented column) is a pivot column.
\end{thm}

\subsection{Parametric Form}
\begin{df}[Free Variable]
Consider a consistent system of equations in the variables $x_1$, $x_2$,..., $x_n$. Let $A$ be a row echelon form of the augmented matrix for this system. We say that $x_i$ is a \textbf{free variable} if its corresponding column in $A$ is not a pivot column.
\end{df}

\begin{eg} Example of free variables.

In the matrix $\left[\begin{array}{c:c}\begin{matrix}1&0&\textcolor{red}{5}\\0&1&\textcolor{red}{2}\end{matrix}&
\begin{matrix}1\\-2\end{matrix}\end{array}\right]$, the variable $\textcolor{red}{z}$ is the free variable.
\end{eg}

\begin{df}[Implicit Equations]
The line is defined implicitly as the simultaneous solutions to those equations.
\end{df}

\begin{df}[Parameterized Equations]
A \textbf{parameterized equation} is an expression that produces all points of the line in terms of one parameter. 
\end{df}

\begin{eg}
Example of implicit equations.
$\left\{\begin{aligned}2x+y+12z&=1\\x+2y+9z&=-1\end{aligned}\right.$ is an example of implicit equations in $\R^3$. $\left\{\begin{aligned}x&=1-5z\\y&=1-2z\end{aligned}\right.$ can be written as $(x,y,z,)=(1-5z, 1-2z,z),\ z \in \R$, which is a parameterized equation.
\begin{rmk} One should think of a system of equations as an implicit equation for its solution set and of the parametric form as the parameterized equation for the same set. The parametric form is much more explicit: it gives a concrete recipe for producing all solutions.
\end{rmk}
\end{eg}
\begin{thm}[Number of Solutions]
Systems of equations can have different numbers of solutions. 
\begin{enumerate}
\item \textbf{The last column is a pivot column}. In this case, the system is inconsistent. It has zero solutions. 
\item \textbf{Every column except the last column is a pivot column}. The system has a unique solution.
\item \textbf{The last column is not a pivot column, and some other column is not a pivot column either}. The system has many solutions corresponding to the infinite possible values of the free variables.
\end{enumerate}
\end{thm}

\begin{eg} 
Systems with different numbers of solutions. 
\begin{enumerate}
\item  $\left[\begin{array}{c:c}\begin{matrix}1&0\\0&1\\0&0\end{matrix} &\begin{matrix}\textcolor{red}{0}\\\textcolor{red}{0}\\\textcolor{red}{1}\end{matrix}\end{array}\right]$ comes form a linear system with no solutions.
\item For the matrix $\left[\begin{array}{c:c}\begin{matrix}1&0&0\\0&1&0\\0&0&1\end{matrix}&\begin{matrix}a\\b\\c\end{matrix}\end{array}\right]$, it has a unique solution $(x,y,z)=(a,b,c)$
\end{enumerate}
\end{eg}


\newpage
\section{Vector Equations and Linear Transformations}
\subsection{Vectors}

\begin{df}[Vector]
A \textbf{vector} is an array of n numbers: $$\vec{x} (\text{or }\bold{x})=\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix}$$
\end{df}

\begin{df}[$\R^n$]
A set of all vectors of height in n is denoted in $\R^n$.
\end{df}

\begin{thm}[Vector Addition]
$$\begin{bmatrix}a\\b\\c\end{bmatrix}+\begin{bmatrix}x\\y\\z\end{bmatrix}=\begin{bmatrix}a+x\\b+y\\c+z\end{bmatrix}$$
\end{thm}
\begin{thm}[Scalar multiplication]
$$\textcolor{red}{c}\times\begin{bmatrix}x\\y\\z\\ \end{bmatrix}=\begin{bmatrix}\textcolor{red}{c}\times x\\\textcolor{red}{c}\times y\\\textcolor{red}{c}\times z\end{bmatrix}$$
\end{thm}
\begin{ext}The Parallelogram Law for Vector Addition.
\begin{center}
\tikzset{every picture/.style={line width=0.75pt}}
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
\draw  (54,201.27) -- (270.64,201.27)(90.64,53) -- (90.64,228.27) (263.64,196.27) -- (270.64,201.27) -- (263.64,206.27) (85.64,60) -- (90.64,53) -- (95.64,60)  ;
\draw    (90.64,201.27) -- (140.56,123.95) ;
\draw [shift={(141.64,122.27)}, rotate = 122.85] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (90.64,201.27) -- (178.69,181.71) ;
\draw [shift={(180.64,181.27)}, rotate = 167.47] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw  [dash pattern={on 4.5pt off 4.5pt}]  (141.64,122.27) -- (229.69,102.71) ;
\draw [shift={(231.64,102.27)}, rotate = 167.47] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (90.64,201.27) -- (230,103.42) ;
\draw [shift={(231.64,102.27)}, rotate = 144.93] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw (114,122.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{v}$};
\draw (183.64,176.67) node [anchor=north west][inner sep=0.75pt]    {$\vec{u}$};
\draw (178.64,87.67) node [anchor=north west][inner sep=0.75pt]    {$\vec{u}$};
\draw (199.64,130.67) node [anchor=north west][inner sep=0.75pt]    {$\vec{v} +\vec{u}$};
\end{tikzpicture}	
\end{center}\end{ext}

\begin{ext}Vector Subtraction.
\begin{center}
\tikzset{every picture/.style={line width=0.75pt}}
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
\draw  (59.92,201.27) -- (241.56,201.27)(90.64,86.22) -- (90.64,222.22) (234.56,196.27) -- (241.56,201.27) -- (234.56,206.27) (85.64,93.22) -- (90.64,86.22) -- (95.64,93.22)  ;
\draw    (90.64,201.27) -- (159.5,152.92) ;
\draw [shift={(161.14,151.77)}, rotate = 144.93] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (90.64,201.27) -- (230,103.42) ;
\draw [shift={(231.64,102.27)}, rotate = 144.93] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw (126,147.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{v}$};
\draw (234.64,110.67) node [anchor=north west][inner sep=0.75pt]    {$c\vec{v}$};
\end{tikzpicture}	
\end{center}
\end{ext}

\begin{df}[Linear Combinations]
Let $c_1,c_2,...,c_k$ be scalars, and let $v_1,v_2,...,v_k$ be vectors in $\R^2$. The vector in $\R^2$ $$c_1v_1+c_2v_2+...+c_kv_k$$ is called a \textbf{linear combination} of the vectors $v_1,v_2,...,v_k$ with \textbf{weights} or \textbf{coefficients} $c_1,c_2,...,c_k$.
\end{df}

\subsection{Vector Equations}
\begin{df}[Vector Equation]
A \textbf{vector equation} is an equation involving a linear combination of vectors with possibly unknown coefficients.
\end{df}

\begin{eg}
Asking whether or not a vector equation has a solution is the same as asking if a given vector is a linear combination of some other given vector. 

The equation 
$$x\begin{bmatrix}1\\2\\6\end{bmatrix}+y\begin{bmatrix}-1\\-2\\-6\end{bmatrix}=\begin{bmatrix}8\\16\\3\end{bmatrix}$$ 
is asking if the vector $\begin{bmatrix}8\\16\\3\end{bmatrix}$ is a linear combination of the vectors $\begin{bmatrix}1\\2\\6\end{bmatrix}$ and $\begin{bmatrix}-1\\-2\\-6\end{bmatrix}$. 

The equation can be simplified to 
$$\begin{bmatrix}x-y\\2x-2y\\6x-y\end{bmatrix}=\begin{bmatrix}8\\16\\3\end{bmatrix} \ \text{or}\ \left\{\begin{aligned}x-y&=8\\2x-2y&=16\\6x-y&=3\end{aligned}\right.$$.\\Then, one can use augmented matrix to solve it.
\end{eg}
\begin{rmk}Three equivalent ways of thinking about a linear system: 
\begin{enumerate}
\item A system of equations
\item An augmented matrix
\item A vector equation
\end{enumerate}
\end{rmk}

\begin{thm}
A new way to consider linear systems.

Suppose the LHS of a linear system is something we can plug a vector into to produce a list of numbers, and the RHS of a linear system shows the solution out as a vector. 

Thus, The LHS of a system is a function $\textbf{\textit{T}}:\ \R^m \rightarrow \R^n$, where $m$ is the number of variables and $n$ is the number of equations. 

To solve the system, we want to find all vectors that will map to a particular group. We can record the function associated with the LHS of a system as a matrix. 
\end{thm}

\begin{eg}
Example of converting linear systems to matrix equations. 

The linear system $$\left\{\begin{aligned}7x_1+3x_2+4x_3&=25\\2x_1+0x_2+x_3&=5\end{aligned}\right.$$ 
can be recorded as
$$\begin{bmatrix}7&3&4\\2&0&1\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\\\end{bmatrix}=\begin{bmatrix}25\\5\end{bmatrix}$$
\end{eg}

\begin{thm}
Multiplication of a vector by a matrix.
\begin{enumerate}
\item For each row of the matrix, multiply the entries of that row with the corresponding entries of the vector and then add.
\item The output vector is the final output. 
\end{enumerate}
\end{thm}

\begin{eg}
 
$$\begin{bmatrix}7&3&4\\2&0&1\end{bmatrix}\begin{bmatrix}1\\1\\1\\\end{bmatrix}=\begin{bmatrix}7\times 1+3\times 1+4\times 1\\2\times 1+0\times 1+1\times 1\end{bmatrix}=\begin{bmatrix}14\\3\end{bmatrix}$$
\end{eg}

\subsection{Linear Transformation}
\begin{df}[Linear Transformation]
A \textbf{linear transformation} is a function $\T: \R^m\rightarrow\R^n$ so that:
\begin{enumerate}
\item $\T(\vec{x}+\vec{y})=\T(\vec{x})+\T(\vec{y})$
\item $\T(c\times \vec{x})=c\times\T(\vec{x})$
\end{enumerate}
$$\forall \vec{x},\vec{y}\in \R^m\text{, and }c\in\R$$
\end{df}

\begin{df}[Standard Basis Vectors]
The vectors $\vece_1,\vece_2,...,\vece_n \in \R^m$ defined by 
$$\vece_i=\begin{bmatrix}0\\\vdots\\1\\\vdots\\0\end{bmatrix} \rightarrow \ \text{the }i\text{-th entry}$$ are called the \textbf{standard basis vectors}.
\end{df}

\begin{thm}
Let $\T: \R^m\rightarrow\R^n$ be a linear transformation, and $$\matrixA=\begin{bmatrix}\vdots&\vdots&\cdots&\vdots\\\T \vece_1&\T \vece_2&\cdots&\T \vece_n\\\vdots&\vdots&\cdots&\vdots\end{bmatrix}$$ Then, $\T\vecx=\matrixA\vecx$ for all vectors $\vecx$
\end{thm}

\begin{prf}
Assume $\vec{x}=\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}$, then $\vec{x}=x_1\vece_1+x_2\vece_2+...+x_n\vece_n$.\\Thus, $$\T\vec{x}=x_1\boldsymbol{T}\vece_1+x_2\T \vece_2+...+x_n\T \vece_n= \begin{bmatrix}\vdots&\vdots&\cdots&\vdots\\\T{\vece_1}&\T{\vece_2}&\cdots&\T{\vece_n}\\\vdots&\vdots&\cdots&\vdots\end{bmatrix}\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}=\matrixA\vec{x}$$
\end{prf}

\begin{thm}
Given any sequence of elementary raw operations $s_1, s_2,...,s_k$ involving n-rows, there exists a matrix $\matrixB$ such that for all $\vec{v}\in\R^n$, $\matrixB\vec{v}$ equals that vector obtained by applying $s_1, s_2,...,s_k$ to $\vec{v}$.
\end{thm}

\begin{eg}
$$\begin{bmatrix}x\\y\end{bmatrix}\xrightarrow[]{\text{II}-\text{I}}\begin{bmatrix}x\\y-x\end{bmatrix}\xrightarrow[]{\text{I}-\text{II}}\begin{bmatrix}2x-y\\y-x\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}2&-1\\1&-1\end{bmatrix}\begin{bmatrix}x\\y
\end{bmatrix},$$

where $\begin{bmatrix}2&-1\\1&-1\end{bmatrix}$ is the matrix $\matrixB$
\end{eg}

\begin{df}[Geometric Definition of Linear Transformation]
We can also think of linear transformation from a geometric perspective. 

\begin{enumerate}
\item $\T: \R^m\rightarrow\R^n$ implies that the original parallelograms map to the transformed parallelograms
\item $\T(c\times \vec{x})=c\times\T(\vec{x})$ means that the original lines through the origin map to the transformed lines through the origin, and the original maps the ruling defined with fundamental unit $\vec{x}$ to ruling with unit $\T\vec{x}$
\item Rotation around the origin is a linear transformation. 
\item Reflection through a line through the origin is a linear transformation.
\item Translation is not a linear transformation.
\end{enumerate}
\end{df}

\begin{eg}
Fix $\theta \in [0, 2\pi )$. Consider the map $\text{Rot}_\theta: \R^2 \rightarrow\R^2$, which rotates a vector by angle $\theta$ around the origin counterclockwise. $\text{Rot}_\theta$ is a linear transformation. Find the matrix associated with this transformation.\\
\begin{sol}
Let $\vece_1=\begin{bmatrix}1\\0\end{bmatrix},\ \vece_2=\begin{bmatrix}0\\1\end{bmatrix}$. \\
The matrix of $\text{Rot}_\theta$ is 
$$\begin{bmatrix}|&|\\ \text{Rot}_{\theta}\vece_1& \text{Rot}_{\theta}\vece_2\\|&|\end{bmatrix}$$
\begin{enumerate}
\item If $\theta = \dfrac{\pi}{2}$, i.e. we rotate by 90° counterclockwise. The matrix for rotation is $$\begin{bmatrix}|&|\\ \text{Rot}_{\theta}\vece_1& \text{Rot}_{\theta}\vece_2\\|&|\end{bmatrix}=\begin{bmatrix}|&|\\ \vece_2& -\vece_2\\|&|\end{bmatrix}=\begin{bmatrix}0&-1\\1&0\end{bmatrix}$$
\item General case: 
$e_1=\begin{bmatrix}1\\0\end{bmatrix};e_2=\begin{bmatrix}0\\1\end{bmatrix}$. Thus, $$\text{Rot}_{\theta}\vece_1=\begin{bmatrix}\cos{\theta}\\\sin{\theta}\end{bmatrix};\text{Rot}_{\theta}\vece_2=\begin{bmatrix}0&-1\\1&0\end{bmatrix}\begin{bmatrix}\cos{\theta}\\\sin{\theta}\end{bmatrix}=\begin{bmatrix}-\sin{\theta}\\\cos{\theta}\end{bmatrix}.$$
$$\Longrightarrow\begin{bmatrix}|&|\\ \text{Rot}_{\theta}\vece_1& \text{Rot}_{\theta}\vece_2\\|&|\end{bmatrix}=\begin{bmatrix}\cos{\theta}&-\sin{\theta}\\\sin{\theta}&\cos{\theta}\end{bmatrix}$$
\begin{center}
\tikzset{every picture/.style={line width=0.75pt}}
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
\draw  (59.92,175.27) -- (241.56,175.27)(141.64,86.22) -- (141.64,222.22) (234.56,170.27) -- (241.56,175.27) -- (234.56,180.27) (136.64,93.22) -- (141.64,86.22) -- (146.64,93.22)  ;
\draw    (141.64,175.27) -- (196.64,175.27) ;
\draw [shift={(198.64,175.27)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (141.64,175.27) -- (141.64,125.27) ;
\draw [shift={(141.64,123.27)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (141.64,175.27) -- (183.74,139.88) ;
\draw [shift={(185.27,138.6)}, rotate = 139.95] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (141.64,175.27) -- (109.47,137) ;
\draw [shift={(108.18,135.47)}, rotate = 49.95] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw (200.64,178.67) node [anchor=north west][inner sep=0.75pt]    {$e_{1}$};
\draw (123.64,111.67) node [anchor=north west][inner sep=0.75pt]    {$e_{2}$};
\draw (185.64,135.67) node [anchor=north west][inner sep=0.75pt]    {$\text{Rot\_} e_{1}$};
\draw (60.64,131.67) node [anchor=north west][inner sep=0.75pt]    {$\text{Rot\_} e_{2}$};
\end{tikzpicture}	
\end{center}
\end{enumerate}
\end{sol}
\end{eg}

\begin{eg}
The map $\text{Ref}_L:\R^2\rightarrow\R^2$ is a linear transformation that reflects a vector over the line $L: y=2x$. Find the matrix for $\text{Ref}_L$.\\
\begin{sol}
Key idea: express $\vece_i=\vece_i^\parallel+\vece_i^\perp$, and $\text{Ref}(\vece_i)=\text{Ref}(\vece_i^\parallel)+\text{Ref}(\vece_i^\perp)$. \\
Choose $\begin{bmatrix}1\\2\end{bmatrix} \in L$, then every parallel vector is $c\begin{bmatrix}1\\2\end{bmatrix}$.\\
Rotate $\begin{bmatrix}1\\2\end{bmatrix}$ by 90°: $$\begin{bmatrix}0&-1\\1&0\end{bmatrix}\begin{bmatrix}1\\2\end{bmatrix}=\begin{bmatrix}-2\\1\end{bmatrix}, $$
then are perpendicular vector is $d\begin{bmatrix}-2\\1\end{bmatrix}$. \\
Take $\vece_1=\begin{bmatrix}1\\0\end{bmatrix}$ and $\vece_2=\begin{bmatrix}0\\1\end{bmatrix}$, then we get
$$\begin{cases}\begin{bmatrix}1\\0\end{bmatrix}&=c\begin{bmatrix}1\\2\end{bmatrix}+d\begin{bmatrix}-2\\1\end{bmatrix}\\\begin{bmatrix}0\\1\end{bmatrix}&=c^{'}\begin{bmatrix}1\\2\end{bmatrix}+d^{'}\begin{bmatrix}-2\\1\end{bmatrix}\end{cases}\Longrightarrow\begin{cases}c=\frac{1}{5}\\d=-\frac{2}{5}\\c^{'}=\frac{2}{5}\\d^{'}=\frac{1}{5}\end{cases}\Longrightarrow\begin{cases} e_1=\frac{1}{5}\begin{bmatrix}1\\2\end{bmatrix}-\frac{2}{5}\begin{bmatrix}-2\\1\end{bmatrix}\\e_2=\frac{2}{5}\begin{bmatrix}1\\2\end{bmatrix}+\frac{1}{5}\begin{bmatrix}-2\\1\end{bmatrix}\end{cases};$$
$$\xrightarrow{\ \ \ \ \ \ \ \ \text{Ref}_L\ \ \ \ \ \ \ \ }\begin{cases}\text{Ref}_L(\vece_1)=\frac{1}{5}\begin{bmatrix}1\\2\end{bmatrix}+\frac{2}{5}\begin{bmatrix}-2\\1\end{bmatrix}=\begin{bmatrix}^{-3}/_5\\^4/_5\end{bmatrix}\\\text{Ref}_L(\vece_2)=\frac{2}{5}\begin{bmatrix}1\\2\end{bmatrix}-\frac{1}{5}\begin{bmatrix}-2\\1\end{bmatrix}=\begin{bmatrix}^4/_5\\^3/_5\end{bmatrix}\end{cases}.$$
Thus, the matrix is 
$$\left[\begin{array}{cc}^{-3}/_{5}&^4/_{5}\\ ^{4}/_{5}&^{3}/_{5}\end{array}\right].$$
\end{sol}
\end{eg}

\newpage
\section{Matrices}
\subsection{Matrix Multiplication}
\begin{thm}[Procedure of Matrix Multiplication]
Matrix multiplication is very different from other formats of multiplication.
\begin{itemize}
	\item Input: a pair of matrices $\matrixA$ and $\matrixB$. \\
	*The number of rows of $\matrixA$ equals the number of columns of $\matrixB$.
	\item Output: The product $\matrixB\matrixA$
	\item Procedure: 
	\begin{enumerate}
		\item View $\matrixA$ as a list of its column vectors: 
		\[\matrixA=\begin{bmatrix}|& &|\\v_1&\cdots &v_n\\|& &|\end{bmatrix}\]
		\item Multiply each column by $\matrixB$: 
		\[\matrixB\matrixA =\begin{bmatrix}|& &|\\\matrixB v_1&\cdots &\matrixB v_n\\|& &|\end{bmatrix}\]
	\end{enumerate}
\end{itemize}	
\end{thm}

\begin{eg}
Examples of matrix multiplication.
\begin{enumerate}
	\item Let $\matrixA=\begin{bmatrix}1&2\\-1&1\end{bmatrix}$ and $\matrixB=\begin{bmatrix}1&2\\0&1\\3&5\end{bmatrix}$. Find $\matrixB\matrixA$.\\
	\begin{sol}\[\matrixB\matrixA=\begin{bmatrix}1&2\\0&1\\3&5\end{bmatrix}\begin{bmatrix}1&2\\-1&1\end{bmatrix}=\begin{bmatrix}1\times1+2\times(-1)&2\times1+2\times1\\1\times0+1\times(-1)&0\times2+1\times1\\3\times1+5\times(-1)&3\times1+5\times1\end{bmatrix}=\begin{bmatrix}-1&4\\-1&1\\-2&11\end{bmatrix}\]\end{sol}
	\item Let $\matrixA=\begin{bmatrix}1&2\\0&1\\3&5\end{bmatrix}$ and $\matrixB=\begin{bmatrix}1&2\\-1&1\end{bmatrix}$. Find $\matrixB\matrixA$.\\
	\begin{sol} Because 2 columns is not equal to three rows, the product does not exist. \end{sol}
	\item Let $\matrixA=\begin{bmatrix}1&2\\-1&1\end{bmatrix}$ and $\matrixB=\begin{bmatrix}1&0&3\\2&1&5\end{bmatrix}$. Find $\matrixA\matrixB$.\\
	\begin{sol}\[	\matrixA\matrixB=\begin{bmatrix}1&2\\-1&1\end{bmatrix}\begin{bmatrix}1&0&3\\2&1&5\end{bmatrix}=\begin{bmatrix}1\times1+2\times2&0\times1+2\times1&3\times1+2\times5\\-1\times1+2\times1&0\times(-1)+1\times1&3\times(-1)+5\times1\end{bmatrix}\\=\begin{bmatrix}5&2&13\\1&1&2\end{bmatrix}\]\end{sol}
\end{enumerate}	
\end{eg}

\begin{rmk}[Conceptualizing Matrix Multiplication]
There are many ways to understand matrix multiplication: 
\begin{enumerate}
	\item A matrix encodes a linear transformation: 
	\[ \matrixA: \R^m\longrightarrow\R^n\text{ is a }m\times n\text{ matrix.}\]
	\[ \matrixB: \R^n\longrightarrow\R^k\text{ is a }n\times k\text{ matrix.}\]
	We can compass these maps: 
	\begin{center}
	\tikzset{every picture/.style={line width=0.75pt}}  
	\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
	\draw    (150.74,156.35) -- (199.74,156.35) ;
	\draw [shift={(201.74,156.35)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (229.74,157.35) -- (278.74,157.35) ;
	\draw [shift={(280.74,157.35)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (131,167) .. controls (169.35,203.97) and (252.06,198.46) .. (292.52,169.24) ;
	\draw [shift={(293.74,168.35)}, rotate = 143.13] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw (122,145.4) node [anchor=north west][inner sep=0.75pt]    {$\mathbb{R}^{m}$};
	\draw (206,146.4) node [anchor=north west][inner sep=0.75pt]    {$\mathbb{R}^{n}$};
	\draw (284,146.4) node [anchor=north west][inner sep=0.75pt]    {$\mathbb{R}^{k}$};
	\draw (161,134.4) node [anchor=north west][inner sep=0.75pt]    {$\matrixA$};
	\draw (248,133.4) node [anchor=north west][inner sep=0.75pt]    {$\matrixB$};
	\draw (196,174.4) node [anchor=north west][inner sep=0.75pt]    {$\matrixB\matrixA$};
	\end{tikzpicture}
	\end{center}
	The product $\matrixB\matrixA$ encodes the composition of those transformations. 
	\begin{eg} Rotation by $90^\circ$ counterclockwise: \[\matrixB=\matrixA=\begin{bmatrix}0&-1\\1&0\end{bmatrix}\]
	Thus, \[\matrixB\matrixA=\begin{bmatrix}0&-1\\1&0\end{bmatrix}\begin{bmatrix}0&-1\\1&0\end{bmatrix}=\begin{bmatrix}-1&0\\0&-1\end{bmatrix}\text{ encodes a rotation by }180^\circ\]
	\end{eg}
	\item The composition $\matrixB\matrixA$ is linear: 
	\begin{itemize}
		\item $\matrixB\matrixA(\vecx+\vecy)=\matrixB(\matrixA\vecx+\matrixA\vecy)=\matrixB\matrixA\vecx+\matrixB\matrixA\vecy$
		\item $\matrixB\matrixA(c\vecx)=\matrixB(c\matrixA\vecx)=c\matrixB\matrixA\vecx$
	\end{itemize}
	\item The matrix for the composition is: 
	\[\matrixB\matrixA=\begin{bmatrix}|&|&&|\\\matrixB v_1&\matrixB v_2&\cdots&\matrixB v_n\\|&|&&|\end{bmatrix},\text{ where }\matrixA=\begin{bmatrix}|&|&&|\\v_1&v_2&\cdots&v_n\\|&|&&|\end{bmatrix}\]
	\begin{prf}
		Suppose \[\matrixA=\begin{bmatrix}|&&|\\v_1&\cdots&v_n\\|&&|\end{bmatrix}=\begin{bmatrix}|&&|\\\matrixA e_1&\cdots&\matrixA e_n\\|&&|\end{bmatrix}\]
		Then, \[\matrixB\matrixA=\begin{bmatrix}|&&|\\\matrixB\matrixA e_1&\cdots &\matrixB\matrixA e_n\\|&&|\end{bmatrix}=\begin{bmatrix}|&&|\\\matrixB v_1&\cdots &\matrixB v_n\\|&&|\end{bmatrix}\]	
	\end{prf}
\end{enumerate}	
\end{rmk}
\begin{eg}[Application: Double Angle Formulae]
Find an expression for $\sin{2\theta}$ and $\cos{2\theta}$ in terms of $\sin\theta$ and $\cos\theta$.\\
\begin{sol}
	For angle $\theta$, we have rotation by $\theta$ is a linear transformation, and the matrix is: \[\matrixA=\begin{bmatrix}\cos\theta &-\sin\theta\\\sin\theta &\cos\theta\end{bmatrix}\]
	Geometrically, $\matrixA\dot\matrixA$ is rotation by $2\theta$: \[\matrixA\cdot\matrixA=\begin{bmatrix}\cos2\theta &-\sin2\theta\\\sin2\theta &\cos2\theta\end{bmatrix}\]
	Algebraically, we have \[\matrixA\cdot\matrixA=\begin{bmatrix}\cos\theta & -\sin\theta\\\sin\theta &\cos\theta\end{bmatrix}\begin{bmatrix}\cos\theta &-\sin\theta\\\sin\theta &\cos\theta\end{bmatrix}=\begin{bmatrix}\cos^2\theta-\sin^2\theta &-2\sin\theta\cos\theta\\2\sin\theta\cos\theta &\cos^2\theta-\sin^2\theta\end{bmatrix}\]
	Since these are equal: 
	\begin{framed}
	\[\cos2\theta=\cos^2\theta-\sin^2\theta\]
	\[\sin2\theta=2\sin\theta\cos\theta\]
	\end{framed}
\end{sol}
\begin{rmk}
	Generalization: 
	$\matrixA^3$: triple angle formulae; $\matrixA^n$: multiple angle formulae	
\end{rmk}
\end{eg}

\begin{thm}
Algebraic properties of matrix multiplication: 
\begin{enumerate}
	\item Matrix multiplication is associated: 
	\[(\matrixA\matrixB)\textbf{\emph{C}}=\matrixA(\matrixB\textbf{\emph{C}}),\text{ assuming the products }\matrixA\matrixB,\ \matrixB\textbf{\emph{C}},\ \matrixA\matrixB)\textbf{\emph{C}}\text{ exists.}\]
	\item Matrix multiplication is generally NOT communitive: 
	\begin{enumerate}
		\item If $\matrixA$ and $\matrixB$ are matrices $n$ rows and $n$ columns, $\matrixA\matrixB\neq\matrixB\matrixA$ in general. 
		*View matrix multiplication as a type of function composition. 
		\item In other words, \textbf{the order matters}.
		\begin{eg}
		\begin{itemize}
			\item Exception: $\begin{bmatrix}3\end{bmatrix}\begin{bmatrix}6\end{bmatrix}=\begin{bmatrix}18\end{bmatrix}=\begin{bmatrix}6\end{bmatrix}=\begin{bmatrix}3\end{bmatrix}$	
			\item Consider
			\[\begin{bmatrix}1&3\\2&4\end{bmatrix}\begin{bmatrix}5&7\\6&8\end{bmatrix}=\begin{bmatrix}5+18&7+24\\10+24&14+32\end{bmatrix}=\begin{bmatrix}23&31\\34&46\end{bmatrix}\]
			\[\begin{bmatrix}5&7\\6&8\end{bmatrix}\begin{bmatrix}1&3\\2&4\end{bmatrix}=\begin{bmatrix}5+14&15+28\\6+16&18+32\end{bmatrix}=\begin{bmatrix}19&43\\22&50\end{bmatrix}\]
			Thus, \[\begin{bmatrix}1&3\\2&4\end{bmatrix}\begin{bmatrix}5&7\\6&8\end{bmatrix}\neq\begin{bmatrix}5&7\\6&8\end{bmatrix}\begin{bmatrix}1&3\\2&4\end{bmatrix}\]
		\end{itemize}
		\end{eg}
	\end{enumerate}
\end{enumerate}	
\end{thm}


\subsection{Invertible Matrices}
\begin{eg}[Guiding Question]
	Let $\vecb=\begin{bmatrix}b_1\\b_2\end{bmatrix}\in\R^2$ be a fixed, arbitrary vector. Let $\matrixA=\begin{bmatrix}2&1\\1&1\end{bmatrix}.$ Find all solutions $\vecx\in\R^2$ to the matrix equation $\matrixA\vecx=\vecb$ (as a function of $b_1$ and $b_2.$)
	
	\begin{sol}
		Observe: $\vecx=\begin{bmatrix}x\\y\end{bmatrix}$	, $\matrixA\vecx=\begin{bmatrix}2x+y\\x+y\end{bmatrix}.$ Then we want to solve \[\begin{cases}2x+y=b_1\\x+y=b_2\end{cases}\]
		\[\begin{aligned}\Rightarrow\left[\begin{array}{c:c}\begin{matrix}2&1\\1&1\end{matrix}&\begin{matrix}b_1\\b_2\end{matrix}\end{array}\right]&\xrightarrow[]{I\leftrightarrow II}\left[\begin{array}{c:c}\begin{matrix}1&1\\2&1\end{matrix}&\begin{matrix}b_2\\b_1\end{matrix}\end{array}\right]\xrightarrow[]{II-2I}\left[\begin{array}{c:c}\begin{matrix}1&1\\0&-1\end{matrix}&\begin{matrix}b_2\\b_1-2b_2\end{matrix}\end{array}\right]\\&\xrightarrow[]{II/(-1)}\left[\begin{array}{c:c}\begin{matrix}1&1\\0&1\end{matrix}&\begin{matrix}b_2\\2b_2-b_1\end{matrix}\end{array}\right]\xrightarrow[]{I-II}\left[\begin{array}{c:c}\begin{matrix}1&0\\0&1\end{matrix}&\begin{matrix}-b_2+b_1\\2b_2-b_1\end{matrix}\end{array}\right]\end{aligned}\]
		\[\therefore\vecx=\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}-b_2+b_1\\2b_2-b_1\end{bmatrix}=\begin{bmatrix}1&-1\\-1&2\end{bmatrix}\begin{bmatrix}b_1\\b_2\end{bmatrix}\]
	\end{sol}
\end{eg}
\begin{df}[Inverse of a Matrix]
	Let $\matrixA$ be a square ($n\times n$ matrix). Assume $\matrixA\vecx=\vecb$ has unique solution for each $\vecb\in\R^n$. Then the map $\vecb\longmapsto\vecx$, the unique solution to $\matrixA\vecx=\vecb$, is a linear transformation and the matrix of this map is called the \textbf{inverse of $\matrixA$}. We denote it as $\matrixA^{-1}.$
	\begin{rmk}
		The matrix $\begin{bmatrix}1&-1\\-1&2\end{bmatrix}$ in the guiding question is the inverse of $\begin{bmatrix}2&1\\1&1\end{bmatrix}.$	
	\end{rmk}
\end{df}
\begin{thm}
	Computing the inverse for a matrix.
	\begin{itemize}
		\item $\matrixA^{-1}$ does not always exist. 
		\item There are square matrices such that $\matrixA\vecx=\vecb$ has infinite solutions.
		\item Process:
		\[\left[\begin{array}{c:c}\quad\matrixA\quad &\begin{matrix}b_1\\\vdots\\b_n\end{matrix}\end{array}\right]\xrightarrow{\text{Row reduce}}\left[\begin{array}{c:c}\rref(\matrixA) &\begin{matrix}\text{Linear expressions}\\\text{in terms of }b_i\end{matrix}\end{array}\right]\]
		Check pivot over each row of $\rref(\matrixA)$, and the coefficient matrix is $\matrixA^{-1}.$
	\end{itemize}	
\end{thm}
\begin{df}[Identity matrix]
	For an $n\times n$ matrix, if it is \[\matrixI_n=\begin{bmatrix}1&0&0&\cdots&0\\0&1&0&\cdots&0\\0&0&1&\cdots&0\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\cdots&1\end{bmatrix},\] we call it the \textbf{identity matrix}. 
	\begin{rmk}
		$\matrixI_n$ encodes the linear transformation $\matrixI_n:\ \R^n\rightarrow\R^n$ ($\vecx\longmapsto\vecx$)
	\end{rmk}
\end{df}
\begin{thm}
	Procedure for finding $\matrixA^{-1}$:
	\begin{enumerate}
		\item Form augmented matrices: \[\left[\begin{array}{c:c}\matrixA&\matrixI_n\end{array}\right]=\left[\begin{array}{c:c}\matrixA&\begin{matrix}1&0&0&\cdots&0\\0&1&0&\cdots&0\\0&0&1&\cdots&0\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\cdots&1\end{matrix}\end{array}\right]\]
		\item Row reduce: \[\left[\begin{array}{c:c}\text{rref}(\matrixA)&\matrixB\end{array}\right],\] if rref($\matrixA$)=$\matrixI_n$, $\matrixB=\matrixA^{-1}$
	\end{enumerate}
\end{thm}
\begin{eg}
\[\begin{aligned}\left[\begin{array}{c:c}\begin{matrix}2&1\\1&1\end{matrix}&\begin{matrix}1&0\\0&1\end{matrix}\end{array}\right]\xrightarrow[]{I\leftrightarrow II}\left[\begin{array}{c:c}\begin{matrix}1&1\\2&1\end{matrix}&\begin{matrix}0&1\\1&0\end{matrix}\end{array}\right]&\xrightarrow[]{II-2I}\left[\begin{array}{c:c}\begin{matrix}1&1\\0&-1\end{matrix}&\begin{matrix}0&1\\1&-2\end{matrix}\end{array}\right]\\&\xrightarrow[]{II/(-1)}\left[\begin{array}{c:c}\begin{matrix}1&1\\0&1\end{matrix}&\begin{matrix}0&1\\-1&2\end{matrix}\end{array}\right]\xrightarrow[]{I-II}\left[\begin{array}{c:c}\begin{matrix}1&0\\0&1\end{matrix}&\begin{matrix}1&-1\\-1&2\end{matrix}\end{array}\right]\end{aligned}\]
	\[\therefore\matrixA^{-1}=\begin{bmatrix}1&-1\\-1&2\end{bmatrix}\]
\end{eg}
\begin{thm}[Function theoretic definition of $\matrixA^{-1}$]
	When $\matrixA^{-1}$ exists, $matrixA^{-1}$ is the matrix encoding the inverse function of $\matrixA$. Hence, $\matrixA$ and $\matrixA^{-1}$ always commute: \[\matrixA^{-1}\cdot\matrixA=\matrixI_n=\matrixA\cdot\matrixA^{-1}\]	
\end{thm}
\begin{eg}
	Let \[\matrixA=\begin{bmatrix}2&1\\1&1\end{bmatrix},\ \matrixA^{-1}=\begin{bmatrix}1&-1\\-1&2\end{bmatrix}.\]
	\[\matrixA\cdot\matrixA^{-1}=\begin{bmatrix}2&1\\1&1\end{bmatrix}\begin{bmatrix}1&-1\\-1&2\end{bmatrix}=\begin{bmatrix}2-1&-2+2\\1-1&-1+2\end{bmatrix}=\begin{bmatrix}1&0\\0&1\end{bmatrix}=\matrixI_2.\]
\end{eg}
\begin{thm}[A new way to find $\matrixA^{-1}$]
	Solving $\matrixA\matrixA^{-1}=\matrixI_n$
	$$\begin{aligned}
		\matrixA^{-1}&=\begin{bmatrix}|&&|\\v_1&\cdots& v_n\\|&&|\end{bmatrix}\\
		\matrixA\matrixA^{-1}&=\begin{bmatrix}|&&|\\\matrixA v_1&\cdots& \matrixA v_n\\|&&|\end{bmatrix}=\begin{bmatrix}|&&|\\e_1&\cdots& e_n\\|&&|\end{bmatrix}=\matrixI_n \\
		\matrixA v_1&=e_1,\ \matrixA v_2=e_2,\cdots,\matrixA v_n=e_n\\
		&\left[\begin{array}{c:c}\matrixA v_1&e_1\end{array}\right],\  \left[\begin{array}{c:c}\matrixA v_2&e_2\end{array}\right],\cdots,\ \left[\begin{array}{c:c}\matrixA v_n&e_n\end{array}\right]\\
		\xrightarrow[]{\text{Row reduce}}&\left[\begin{array}{c:c}\text{rref}(\matrixA) &v_1\end{array}\right],\  \left[\begin{array}{c:c}\text{rref}(\matrixA)  &v_2\end{array}\right],\cdots,\ \left[\begin{array}{c:c}\text{rref}(\matrixA)  &v_n\end{array}\right]
	\end{aligned}$$
	$\therefore$ To find $\matrixA^{-1}$:
	\begin{framed}\[\left[\begin{array}{c:c}\matrixA&\matrixI_n\end{array}\right]\xRightarrow{\text{Row reduce}}\left[\begin{array}{c:c}\matrixI&\matrixA^{-1}\end{array}\right]\]\end{framed}
\end{thm}
\begin{eg}[Problems concerning inverting matrices]
	Let $\matrixA=\begin{bmatrix}1&1&1\\1&2&3\\1&4&9\end{bmatrix}.$ Compute $\matrixA^{-1}$ and use it to find all solutions to $\matrixA\vecx=\begin{bmatrix}1\\-1\\1\end{bmatrix}$	.\\
	\begin{sol}
		$$\begin{aligned}
			\left[\begin{array}{c:c}\begin{matrix}1&1&1\\1&2&3\\1&4&9\end{matrix}&\begin{matrix}1&0&0\\0&1&0\\0&0&1\end{matrix}\end{array}\right]&\xrightarrow[II-I]{III-I}\left[\begin{array}{c:c}\begin{matrix}1&1&1\\0&1&2\\0&3&8\end{matrix}&\begin{matrix}1&0&0\\-1&1&0\\-1&0&1\end{matrix}\end{array}\right]\xrightarrow[I-II]{III-3II}\left[\begin{array}{c:c}\begin{matrix}1&0&1\\0&1&2\\0&0&2\end{matrix}&\begin{matrix}2&-1&0\\-1&1&0\\2&-3&1\end{matrix}\end{array}\right]\\
			&\xrightarrow[]{III/2}\left[\begin{array}{c:c}\begin{matrix}1&0&1\\0&1&2\\0&0&1\end{matrix}&\begin{matrix}2&-1&0\\-1&1&0\\1&-^3/_2&^1/_2\end{matrix}\end{array}\right]\xrightarrow[I+III]{II-2III}\left[\begin{array}{c:c}\begin{matrix}1&0&0\\0&1&0\\0&0&1\end{matrix}&\begin{matrix}3&-^5/_2&^1/_2\\-3&4&-1\\1&-^3/_2&^1/_2\end{matrix}\end{array}\right]
		\end{aligned}$$
		\[\therefore\matrixA^{-1}=\begin{bmatrix}3&-^5/_2&^1/_2\\-3&4&-1\\1&-^3/_2&^1/_2\end{bmatrix}\]
		To solve $\matrixA\vecx=\vecb,$ apply $\matrixA^{-1}$ on both sides: 
		$$\begin{aligned}
		\matrixA^{-1}(\matrixA\vecx)&=\matrixA^{-1}\vecb\\
		\vecx&=\matrixA^{-1}\vecb\\
		\therefore\vecx&=\begin{bmatrix}3&-^5/_2&^1/_2\\-3&4&-1\\1&-^3/_2&^1/_2\end{bmatrix}\begin{bmatrix}1\\-1\\1\end{bmatrix}=\begin{bmatrix}3+^5/_2+^1/_2\\-3-4-1\\1+^3/_2+^1/_2\end{bmatrix}=\begin{bmatrix}6\\-8\\3\end{bmatrix}
		\end{aligned}$$
	\end{sol}
\end{eg}


\subsection{Kernel of a Matrix}

As $\matrixA\vecx=\vecb$ encodes a system of linear equation, one key question of linear algebra is to find how would the solution to $\matrixA\vecx=\vecb$ change as $\vecb$ varies. 

\begin{thm}
	Let $f:\R^m\to\R^n$ be a function, then: 
	\begin{enumerate}
		\item If $\vecb_1,\ \vecb_2\in\R^n,$ and $\vecb_1\neq\vecb_2,$ then the sets $\left\{\vecx: f(\vecx)=\vecb_1\right\}$ and $\left\{\vecx: f(\vecx)=\vecb_2\right\}$ do not intersect. 
		\item Every $\vecx$ in the domain is an element of the solution set $\left\{\vecx: f(\vecx)=\vecb\right\}$ for some $\vecb$.
	\end{enumerate}
\end{thm}
\begin{eg}
	Let $\matrixA=\begin{bmatrix}2&1\end{bmatrix}$. Then solving $\matrixA\vecx=\vecb$ gives $2x+y=\vecb$, which encodes a line of slope$=-2$ that has a $y-$intercept of $\vecb$.	
\end{eg}
\begin{df}[Zero Vector]
	The \textbf{zero vector} $\vec{0}\in\R^n$ (sometimes denoted as $\vec{0}_n$ if the context is unclear) is the vector all of whose entries are $0$.
\end{df}
\begin{eg}
	\[\vec{0}_2=\begin{bmatrix}0\\0	\end{bmatrix},\ \vec{0}_3=\begin{bmatrix}0\\0\\0\end{bmatrix}\]	
\end{eg}
\begin{thm}
	Let $\matrixA$ be an $n\times m$ matrix (i.e., encoding a linear transformation $\matrixA: \R^m\to\R^n$) and $\vecb\in\R^n$ such that (\emph{s.t.}) $\matrixA\vecx=\vecb$ has a solution. Suppose $\vecx_0$ to be any fixed solution. Then, the solution set to $\matrixA\vecx=\vecb$ is $\left\{\vecx_0+\vecx'\ |\ \matrixA\vecx'=0\right\}$
	
	Interpretation: The solution set to $\matrixA\vecx=\vecb$ is the translation of the solution set to $\matrixA\vecx=\vec{0}$ by $\vecx_0$.
	\begin{prf}
	We need to prove two parts: 1. Any solution to $\matrixA\vecx=\vecb$ is of the form $\vecx_0+\vecx'$, where $\matrixA\vecx'=\vec{0}$, and 2. $\vecx_0+\vecx$ are solutions to $\matrixA\vecx=\vecb.$
		\begin{enumerate}
			\item Any solution to $\matrixA\vecx=\vecb$ is of the form $\vecx_0+\vecx'$, where $\matrixA\vecx'=\vec{0}$.\\
			Let $\vecx$ be such a solution, then $\vecx'\coloneqq\vecx-\vecx_0$, then $$\begin{aligned}\matrixA\vecx'&=\matrixA(\vecx-\vecx_0)\\&=\matrixA\vecx-\matrixA\vecx_0\\&=\vecb-\vecb\\&=\vec{0}.\end{aligned}$$
			So, $\vecx=\vecx_0+\vecx'.$
			\item $\vecx_0+\vecx$ are solutions to $\matrixA\vecx=\vecb.$
			$$\begin{aligned}
				\matrixA(\vecx_0+\vecx')&=\matrixA\vecx_0+\matrixA\vecx'\\
				&=\vecb+\vec(0)=\vecb.
			\end{aligned}$$

		\end{enumerate}
	\end{prf}
\end{thm}
\begin{df}[Kernel of a Matrix]
	The \textbf{kernel} of a linear transformation or a matrix is the solution set to $\matrixA\vecx=\vec{0}.$
	\[\text{i.e., }\ker(\matrixA)=\left\{\vecx\in\R^m;\  \matrixA\vecx=\vec{0}\right\}.\]
\end{df}
\begin{thm}
	\[\ker(\matrixA)=\ker(\text{rref}(\matrixA)).\]	
\end{thm}

\begin{thm}\label{thm3.3.4}
	Procedure of computing the kernel of a matrix:
	\begin{enumerate}
		\item Row reduce $\matrixA$ to rref($\matrixA$), compute $\ker(\text{rref}(\matrixA))$.
		\item Unpack the equations encoded by matrix equation rref($\matrixA)=0$, solve for pivot variables in terms of free variables.
		\item Parameterize the solution set for $\rref(\matrixA)\vecx=0$ as $\left\{t_1\vecv_1+t_2\vecv_2+\cdots+t_d\vecv_d:\ t_i\in\R\right\}$ and $\vecv_i$ tracks the coefficient of the $i$-th free variable.
	\end{enumerate}
\end{thm}
\begin{eg}
	Let $\matrixA=\begin{bmatrix}1&2&3&4\\5&6&7&8\\9&10&11&12\end{bmatrix}.$  Compute $\ker(\matrixA).$\\
	\begin{sol}
		$\ker(\matrixA)$ is the solution set to $\matrixA\vecx=\vec{0}$: 
		$$\begin{aligned}
			\left[\begin{array}{c:c}\begin{matrix}1&2&3&4\\5&6&7&8\\9&10&11&12\end{matrix}&\begin{matrix}0\\0\\0\end{matrix}\end{array}\right]\xrightarrow[II-5I]{III-9I}\left[\begin{array}{c:c}\begin{matrix}1&2&3&4\\0&-4&-8&-12\\0&-8&-16&-24\end{matrix}&\begin{matrix}0\\0\\0\end{matrix}\end{array}\right]&\xrightarrow[]{II/-4}\left[\begin{array}{c:c}\begin{matrix}1&2&3&4\\0&1&2&3\\0&-8&-16&-24\end{matrix}&\begin{matrix}0\\0\\0\end{matrix}\end{array}\right]\\
			&\xrightarrow[I-2II]{III+8II}\left[\begin{array}{c:c}\begin{matrix}1&0&-1&-2\\0&1&2&3\\0&0&0&0\end{matrix}&\begin{matrix}0\\0\\0\end{matrix}\end{array}\right]
		\end{aligned}$$
		\[\therefore\begin{cases}x_1-x_3-2x_4=0\\x_2+2x_3+3x_4=0\end{cases}\]
		$$\therefore\text{ Solution set: }\begin{bmatrix}x_1\\x_2\\x_3\\x_4\end{bmatrix}=\begin{bmatrix}x_3+2x_4\\-2x_3-3x_4\\x_3\\x_4\end{bmatrix}.$$
		Thus, $$\begin{aligned}\ker(\matrixA)&=\left\{\begin{bmatrix}x_3+2x_4\\-2x_3-3x_4\\x_3\\x_4\end{bmatrix}:\ x_3,\ x_4\in\R\right\}=\left\{x_3\begin{bmatrix}1\\-2\\1\\0\end{bmatrix}+x_4\begin{bmatrix}2\\-3\\0\\1\end{bmatrix}\right\}\end{aligned}$$
	\end{sol}
\end{eg}
\begin{df}[Span]
	Let $\vecv_1,\vecv_2,\cdots,\vecv_d\in\R$, the \textbf{span} of $\vecv_1,\vecv_2,\cdots,\vecv_d$ is the set: 
	\[\Span(\vecv_1,\vecv_2,\cdots,\vecv_d)=\left\{t_1\vecv_1+t_2\vecv_2+\cdots+t_d\vecv_d;\  t_i\in\R\right\}\]
\end{df}
\begin{eg}
	Our procedure of finding kernels finds vectors $\vecv_1,\vecv_2,\cdots,\vecv_d$ which spans the kernel of the matrix.	
\end{eg}
\begin{df}[Image of a Matrix]
	Let $\matrixA$ be an $n\times m$ matrix (i.e., encoding a linear transformation $\matrixA:\R^m\to\R^n$), the \textbf{image} of $\matrixA$	 is the set: 
	\[\IM(\matrixA)=\left\{\matrixA\vecx\ |\ \vecx\in\R^m\right\}.\]
	
	Interpretation: $\IM(\matrixA)$ is the set of $\vecb$ \emph{s.t.} $\matrixA\vecx=\vecb$ has a solution.
\end{df}
\begin{thm}
	Let $\matrixA$ be an $n\times m$ matrix, and let	 $\vecw_1,\cdots,\vecw_m$ be the columns of $\matrixA$: $\matrixA=\begin{bmatrix}|&&|\\\vecw_1&\cdots&\vecw_m\\|&&|\end{bmatrix}$. The image of $\matrixA$ is the span of $\vecw_1,\cdots,\vecw_m$: 
	\[\IM(\matrixA)=\Span(\vecw_1,\cdots,\vecw_m)=\left\{t_1\vecw_1+\cdots+t+m\vecw_m;\ t_m\in\R\right\}\]
	\begin{rmk}
		\[\begin{aligned}\ker(\matrixA)&\subseteq\R^m\quad\text{(domain)}\\\Span(\matrixA)&\subseteq\R^n\quad\text{(range)}\end{aligned}\]	
	\end{rmk}
	\begin{prf}
		We know that the columns of a matrix form $\matrixA\vecx$, namely the $i$-th column of the matrix $\matrixA$ is $\matrixA\vec{e}_i$, where $\vece_i=\begin{bmatrix}0\\\vdots\\1\\\vdots\\0\end{bmatrix} \rightarrow \ \text{the }i\text{-th entry}$.
		
		Hence, $\vecx=\begin{bmatrix}x_1\\vdots\\x_m\end{bmatrix}=x_1\vece_1+x_2\vece_2+\cdots+x_m\vece_m$.
		$$\begin{aligned}
			\matrixA\vecx&=x_1\matrixA\vece_1+x_2\matrixA\vece_2+\cdots+x_m\matrixA\vece_m\\
			&=x_1\vecw_1+x_2\vecw_2+\cdots+x_m\vecw_m\\
			&\in\Span(\vecw_1,\cdots,\vecw_m).
		\end{aligned}$$
	\end{prf}
\end{thm}


\newpage
\section{Spaces and Dimensions}
\subsection{Subspaces and Bases}
\begin{thm}
Spans of Sets of Vectors: 
	\begin{enumerate}
		\item In general, for $\vecv\in\R^n$, if $\vecv\neq0$, then $\Span(\vecv)$ is the line through the origin containing $\vecv$.
		\item If $\vecv=\vec{0}$, then $\Span(\vecv)$ is also the zero vector.
		\item For vectors $\vecv_1,\ \vecv_2\in\R^n$, if $\vecv_1\neq\vecv_2$ and $\vecv_1,\ \vecv_2\neq0$, then $\Span(\vecv_1,\vecv_2)$ is a plane through the origin containing $\vecv_1$ and $\vecv_2$. 
		\item If $\vecv_1$ and $\vecv_2$ are co-linear with each other, then $\Span(\vecv_1,\vecv_2)$ is a line through the containing origin of $\vecv_1$ and $\vecv_2$.
	\end{enumerate}
\end{thm}
\begin{df}[Redundancy]
	A vector $\vecv_k$ is called \textbf{redundant} in a list of vectors $\vecv_1,\cdots,\vecv_k\in\R^n$ if \[\vecv_k\in\Span(\vecv_1,\cdots,\vecv_{k-1})\]	
\end{df}
\begin{df}[Span of an Empty Set]
	The span of the \textbf{empty set of vectors} is $\{\vec{0}\}$. 	
\end{df}
\begin{df}[Linear Independence]
	Let $\vecv_1,\cdots,\vecv_k\in\R^n$. Then vectors $\vecv_1,\cdots,\vecv_k$ are called \textbf{linearly independent} if $\vecv_i$ is not redundant in the list of $\vecv_1,\cdots,\vecv_i\quad\forall i\in[1,k]$. 
	\begin{eg}
		$\vece_1,\cdots,\vece_k$ are linearly independent ($\LI$) in $\R^n\quad\forall n\geq k$. ($\vece_1,\cdots,\vece_k$ are the standard basis vectors.	
	\end{eg}
\end{df}
\begin{thm}
	Span and Linear Independency.
	\begin{enumerate}
		\item The span of the empty set is a point $\{\vec{0}\}$.
		\item The span of a single linear independent vector is a line through the origin.
		\item The span of two linear independent vectors is a plane through the origin. 
	\end{enumerate}	
\end{thm}
\begin{df}[Subspace]
	Let $V$ be a subset of $\R^n$. $V$ is called	 a \textbf{subspace} if: 
	\begin{enumerate}
		\item $\vec{0}\in V$\\
		Interpretation: Origin is in $V$.
		\item If $\vecv\in V$, then $c\vecv\in V\quad\forall c\in\R$.\\
		Interpretation: If $\vecv\in V$, then the line through the origin containing $\vecv$ is in $V$.
		\item If $\vecv_1,\ \vecv_2\in V$, then $\vecv_1+\vecv_2\in V$.\\
		Interpretation: If $\vecv_1$ and $\vecv_2$ are not co-linear and contained in $V$, then the plane through $\vecv_1,\ \vecv2$ and $\vec{0}$ is in $V$. 
	\end{enumerate}
	\begin{eg}
	Examples of subspaces. 
	\begin{enumerate}
		\item $\{\vec{0}\}$ is a subspace.
		\item $\R^n$ is a subspace.
		\item If $\vecv_1,\cdots,\vecv_k\in\R^n$, then $\Span(\vecv_1,\cdots,\vecv_k)$ is a subspace. \\
		\begin{prf}
			\begin{enumerate}
				\item $\vec{0}=0\vecv_1+0\vecv_2+\cdots+\vecv_k$
				\item $\vecv=t_1\vecv_1+t_2\vecv_2+\cdots+t_k\vecv_k$\[\Longrightarrow c\vecv=ct_1\vecv_1+ct_2\vecv_2+\cdots+ct_k\vecv_k\in\Span(\vecv_1,\cdots,\vecv_k)\]
				\item $\vecv'=t_1'\vecv_1+t_2'\vecv_2+\cdots+t_k'\vecv_k$\[\Longrightarrow \vecv+\vecv'=(t_1+t_1')\vecv_1+(t_2+t_2')\vecv_2+\cdots+(t_k+t_k')\vecv_k\in\Span(\vecv_1,\cdots,\vecv_k)\]
			\end{enumerate}	
		\end{prf}
		\item A line through origin is a subspace.
		\item A plane through the origin is a subspace.
		\begin{framed}
			In two and three dimensions, examples 1, 2, 4, and 5 are the only examples of subspaces.	
		\end{framed}
		For examples 6 and 7, consider an $n\times m$ matrix $\matrixA$, which maps a linear transformation from $\R^m$ to $\R^n$ (i.e., $\matrixA:\R^m\to\R^n$). Let $\ker(\matrixA)$ and $\IM(\matrixA)$ be the kernel and image of $\matrixA$, respectively. 
		\item $\ker(\matrixA)$ is a subspace.\\
		\begin{prf}
			\begin{enumerate}
				\item $\matrixA\vec{0}_n=\vec{0}_n$\[\Longrightarrow\vec{0}_n\text{ is in }\ker(\matrixA).\]
				\item $\matrixA\vecv=\vec{0}$, then $\matrixA(c\vecv)=c\matrixA\vecv=c\vec{0}=\vec{0}$\[\Longrightarrow\text{ If }\vecv\in\ker(\matrixA)\text{, then }c\vecv\in\ker(\matrixA).\]
				\item If $\matrixA\vecv_1=\vec{0}$ and $\matrixA\vecv_2=\vec{0}$, then $\matrixA(\vecv_1+\vecv_2)=\matrixA\vecv_1+\matrixA\vecv_2=\vec{0}$\[\Longrightarrow\text{ If }\vecv_1,\vecv_2\in\ker(\matrixA),\text{ then }\vecv_1+\vecv_2\in\ker(\matrixA).\]
			\end{enumerate}	
		\end{prf}
		\item $\IM(\matrixA)$ is a subspace.\\
		\begin{prf}
			\begin{enumerate}
				\item $\vec{0}_n\in\IM(\matrixA)$
				\item If $\vecb\in\IM(\matrixA)$, then $\vecb=\matrixA\vecx$\[\Longrightarrow c\vecb=\matrixA(c\vecx)\in\IM(\matrixA)\]
				\item If $\vecb_1,\vecb_2\in\IM(\matrixA)$, then $\vecb_1=\matrixA\vecx_1$ and $\vecb_2=\matrixA\vecx_2$\[\Longrightarrow\vecb_1+\vecb_2=\matrixA\vecx_1+\matrixA\vecx_2=\matrixA(\vecx_1+\vecx_@)\in\IM(\matrixA).\]
			\end{enumerate}	
		\end{prf}
	\end{enumerate}
	\end{eg}
	\begin{rmk}
		The same subspace can be spanned by \textbf{many} sets of vectors.	
	\end{rmk}
\end{df}
\begin{df}
	Let $V$ be a subspace of $\R^n$. A \textbf{basis} for $V$ is a set of vectors $\vecv_1,\cdots,\vecv_k\in V$, which: 
	\begin{enumerate}
		\item Span $V$, and
		\item Are linearly independent. 
	\end{enumerate}
	\begin{eg}
		The vectors $\vece_1,\cdots,\vece_n$ are a basis for $\R^n$\\
		\begin{prf}
			\begin{enumerate}
				\item $\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}=x_1\vece_1+x_2\vece_2+\cdots+x_n\vece_n$
				\item $e_i\notin\Span(\vece_1,\cdots\vece_{i-1})\rightarrow\LI$	
			\end{enumerate}
		\end{prf}
	\end{eg}
\end{df}
\begin{thm}[Computing a basis for $\IM(\matrixA)$]
	Let $\matrixA$ be an $n\times m$ matrix with columns $\vecv_1,\cdots,\vecv_m$: \[\matrixA=\begin{bmatrix}|&&|\\\vecv_1&\cdots&\vecv_m\\|&&|\end{bmatrix}\] The columns of $\matrixA	$ which contain a pivot upone row reduction to $\rref(\matrixA)$ are a basis for $\IM(\matrixA)$.
	\begin{eg}
		Let $\matrixA=\begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix}\xrightarrow[]{\text{Row Reduction}}\begin{bmatrix}\boxed{1}&0&-1\\0&\boxed{1}&2\\0&0&0\end{bmatrix}$	
		\[\Longrightarrow\begin{bmatrix}1\\4\\7\end{bmatrix}\text{ and }\begin{bmatrix}2\\5\\8\end{bmatrix}\text{ are the basis of }\IM(\matrixA).\]
		\begin{rmk}
			The coefficients $-1$ and $2$ on the third column of $\matrixA$ indicates that \[\begin{bmatrix}3\\6\\9\end{bmatrix}=-1\begin{bmatrix}1\\4\\7\end{bmatrix}+2\begin{bmatrix}2\\5\\8\end{bmatrix}.\]
		\end{rmk}
	\end{eg}
	
	\begin{prf}
		We know: $\IM(\matrixA)=\Span(\vecv_1,\cdots\vecv_m).$ To produce basis, remove redundant columns. 
		Hence, we want to show: the $i$-th column does not contain a pivot on row reduction \emph(iff) $\vecv_i$ is redundant: 
		\[\vecv_i=t_1\vecv_1+\cdots+t_i\vecv_{i-1}=\Span(\vecv_1,\cdots,\vecv_{i-1})=\begin{bmatrix}|&&|\\\vecv_1&\cdots&\vecv_{i-1}\\|&&|\end{bmatrix}\begin{bmatrix}t_1\\\vdots\\t_{i-1}\end{bmatrix}=\matrixA_{i-1}\vecx\]
		$\Rightarrow$ We want to show: when $\vecv_i=\matrixA_{i-1}\vecx$ has solutions. To solve $\vecv_i=\matrixA_{i-1}\vecx$: 
		\[\left[\begin{array}{c:c}\matrixA_{i-1}&\vecv_i\end{array}\right]\xrightarrow[]{\text{Row Reduce}}\begin{cases}\text{Consistent}\Rightarrow\text{Redundant}\Rightarrow\text{Do not contain pivot in }i\text{-th column}\\\text{Inconsistent}\Rightarrow\text{Not redundant}\Rightarrow\text{Contain a pivot}\end{cases}\]
		\begin{eg}
			\[\matrixA=\begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix}\Rightarrow\rref(\matrixA)=\begin{bmatrix}\boxed{1}&0&-1\\0&\boxed{1}&2\\0&0&0\end{bmatrix}\]
			\[\left[\begin{array}{c:c}\begin{matrix}1\\4\\7\end{matrix}&\begin{matrix}2\\5\\8\end{matrix}\end{array}\right]\Rightarrow\left[\begin{array}{c:c}\begin{matrix}1\\0\\0\end{matrix}&\begin{matrix}2\\-3\\-6\end{matrix}\end{array}\right]\Rightarrow\text{Inconsistent}\Rightarrow\text{Not redundant}\]
			\[\left[\begin{array}{c:c}\begin{matrix}1&2\\4&5\\7&8\end{matrix}&\begin{matrix}3\\6\\9\end{matrix}\end{array}\right]\Rightarrow\left[\begin{array}{c:c}\begin{matrix}1&0\\0&1\\0&0\end{matrix}&\begin{matrix}-1\\2\\0\end{matrix}\end{array}\right]\Rightarrow\text{Consistent}\Rightarrow\text{Redundant}\]
		\end{eg}\
	\end{prf}
\end{thm}
\begin{thm}[Computing a basis for $\ker(\matrixA)$]
	Recall \textbf{Theorem \ref{thm3.3.4}} Procedure to find $\ker(\matrixA)$.
	\begin{enumerate}
		\item The spanning set produced by ``computing the kernel of $\matrixA$'' is a basis for $\ker(\matrixA)$.
		\item Procedure:
		\begin{enumerate}
			\item Row reduce $\matrixA$ to $\rref(\matrixA)$, and then compute $\ker(\rref(\matrixA))$.
			\item Unpack the equations encoded by matrix equation $\rref(\matrixA)=0$. Solve for pivot variables in terms of free variables.
			\item Parametrize the solution set for $\rref(\matrixA)\vecx=0$ as $\{t_1\vecv_1+t_2\vecv_2+\cdots+t_d\vecv_d;\ t_i\in\R\}$ and $\vecv_i$ tracks the coefficient of the $i$-th free variable.
		\end{enumerate}
	\end{enumerate}	
	\begin{prf}
		Look at the free variables $x_{i_1}, x_{i_2},\cdots,x_{i_d}.$ Then $\vecv_{i_j}$ is $0$ if $j\neq k$; $\vecv_{i_j}$ is $1$ if $j=k$. Thus, \[c_1\vecv_1+c_2\vecv_2+\cdots+c_{k-1}\vecv_{k-1}\neq\vecv_k.\]	
	\end{prf}
	\begin{eg}
		Let $\vecv_3=\begin{bmatrix}1\\-2\\1\\0\end{bmatrix}$ and $\vecv_4=\begin{bmatrix}2\\-3\\0\\1\end{bmatrix}$. Then, $c\vecv_3\neq\vecv4$ since the $4$-th position of $\vecv_3$ is $0$, whereas that of $\vecv_4$ is $1$.	
	\end{eg}
\end{thm}

\subsection{The Rank-Nullity Theorem}
\begin{thm} 
	If $V$ is a subspace of $\R^n$, then $V$ has a basis, and all bases have the same size. 
\end{thm}
\begin{df}[The Dimension of a Subspace]
	Let $V$ be a subspace, the \textbf{dimension} of $V$ is the size of any bases. We denote it as $\dim(V)$. 
\end{df}
\begin{df}[Rank of $\matrixA$]
	Let $\matrixA$ be an $n\times m$ matrix (i.e., $\matrixA:\R^m\to\R^n$). The \textbf{rank} of $\matrixA$ is the dimension of the image of A. We denote it as $\rank(\matrixA)$. \[\rank(\matrixA)=\dim(\IM(\matrixA))\]
\end{df}
\begin{df}[Nullity of $\matrixA$]
	Let $\matrixA$ be an $n\times m$ matrix (i.e., $\matrixA:\R^m\to\R^n$). The \textbf{nullity} of $\matrixA$ is the dimension of the kernel of A. We denote it as $\nullity(\matrixA)$. \[\nullity(\matrixA)=\dim(\ker(\matrixA))\]
\end{df}
\begin{thm}[The Rank-Nullity Theorem]
	Suppose $\matrixA$ to be an $n\times m$ matrix: \[\boxed{\rank(\matrixA)+\nullity(\matrixA)=\dim(\text{domain of }\matrixA)=m}.\]	
	\begin{eg}
		Let $\matrixA=\begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix}$. To find basis for $\IM(\matrixA)$ and $\ker(\matrixA)$: \[\matrixA\xRightarrow[\text{Reduce}]{\text{Row}}\begin{bmatrix}\boxed{1}&0&-1\\0&\boxed{1}&2\\0&0&0\end{bmatrix}\]
		\begin{enumerate}
			\item To find a basis for $\IM(\matrixA)$, we take the columns of $\matrixA$ which contain a pivot upon row reduction: \[\IM(\matrixA)=\Span\left(\begin{bmatrix}1\\4\\7\end{bmatrix},\begin{bmatrix}2\\5\\8\end{bmatrix}\right).\]\[\therefore\dim(\IM(\matrixA))=2.\]
			\item 	To find a basis for $\ker(\matrixA)$, unpack the equation: \[\begin{cases}x_1-x_3=0\\x_2+2x_3=0\end{cases},\ \Longrightarrow\begin{cases}x_1=x_3\\x_2=-2x_3\end{cases}.\]
			\[\therefore \ker(\matrixA)=\left\{\begin{bmatrix}x_3\\-2x_3\\x_3\end{bmatrix};\ x_3\in\R\right\}=\Span\left(\begin{bmatrix}1\\-2\\1\end{bmatrix}\right).\]
			\[\therefore\dim(\ker(\matrixA))=1.\]
			\item $\rank(\matrixA)=\dim(\IM(\matrixA))=2;\ \nullity(\matrixA)=\dim(\ker(\matrixA))=1;\ \dim(\text{domain})=3$\[\therefore\rank(\matrixA)+\nullity(\matrixA)=3=\dim(\text{domain}).\]
		\end{enumerate}
	\end{eg}
	
	\begin{prf}
		\begin{enumerate}
			\item $\rank(\matrixA)=\dim(\IM(\matrixA))=$ number of rectors in a basis of $\IM(\matrixA)=$ number of pivots in $\rref(\matrixA)$.
			\item  $\nullity(\matrixA)=\dim(\ker(\matrixA))=$number of rectors in a basis of $\ker(\matrixA)=$ number of free variables $=$ number of non-pivot columns in $\rref(\matrixA)$.
			\item $\therefore\rank(\matrixA)+\nullity(\matrixA)=$ number of columns of $\rref(\matrixA)$ or, simply, $\matrixA=\dim(\text{domain of }\matrixA).$
		\end{enumerate}
	\end{prf}
\end{thm}
\begin{eg}[Geometric Perspective of Rank-Nullity Theorem]
	Let $\matrixM=\begin{bmatrix}1&3\\2&6\end{bmatrix}$.\[\therefore\rref(\matrixM)=\begin{bmatrix}1&3\\0&0\end{bmatrix}.\]
	\[\therefore\IM(\matrixM)=\Span\left(\begin{bmatrix}1\\2\end{bmatrix}\right) (\text{Line of slope }2\text{ through the origin})\Longrightarrow\dim(\IM(\matrixM))=1;\]
	\[\ker(\matrixM)=\Span\left(\begin{bmatrix}-3\\1\end{bmatrix}\right)(\text{Line of slope }-\frac{1}{3}\text{ through origin})\Longrightarrow\dim(\ker(\matrixM))=1.\] 
	If we consider the domain of $\matrixM$ to be the inputs for the transformation, and range of $\matrixM$ ($\IM(\matrixM)$) to be the outputs of the linear transformation, then the rank-nullity theorem denotes that \[\dim(\text{Inputs})=\dim(\text{Outputs})+\text{Information Loss}.\]
	The ``information loss'' is given by $\dim(\ker(\matrixM)).$ In this specific example, $\dim(\text{inputs})=2$ and $\dim(\text{outputs})=1$, so the information loss of the linear transformation $\matrixM$ is $2-1=1.$
	\begin{center}
		\tikzset{every picture/.style={line width=0.75pt}}       
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		\draw  (32.01,149.24) -- (264.01,149.24)(142.01,47) -- (142.01,242.24) (257.01,144.24) -- (264.01,149.24) -- (257.01,154.24) (137.01,54) -- (142.01,47) -- (147.01,54)  ;
		\draw    (110.5,96.01) -- (210.5,196.01) ;
		\draw    (120,80) -- (220,180) ;
		\draw    (92.01,99.24) -- (192.01,199.24) ;
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (160.01,149.5) .. controls (160.01,147.57) and (161.57,146.01) .. (163.5,146.01) .. controls (165.44,146.01) and (167,147.57) .. (167,149.5) .. controls (167,151.44) and (165.44,153) .. (163.5,153) .. controls (161.57,153) and (160.01,151.44) .. (160.01,149.5) -- cycle ;
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (139.01,103.5) .. controls (139.01,101.57) and (140.57,100.01) .. (142.5,100.01) .. controls (144.44,100.01) and (146,101.57) .. (146,103.5) .. controls (146,105.44) and (144.44,107) .. (142.5,107) .. controls (140.57,107) and (139.01,105.44) .. (139.01,103.5) -- cycle ;
		\draw  (324.01,147.24) -- (556.01,147.24)(434.01,45) -- (434.01,240.24) (549.01,142.24) -- (556.01,147.24) -- (549.01,152.24) (429.01,52) -- (434.01,45) -- (439.01,52)  ;
		\draw    (489.01,77.24) -- (390.26,203.93) ;
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (467.01,100.5) .. controls (467.01,98.57) and (468.57,97.01) .. (470.5,97.01) .. controls (472.44,97.01) and (474,98.57) .. (474,100.5) .. controls (474,102.44) and (472.44,104) .. (470.5,104) .. controls (468.57,104) and (467.01,102.44) .. (467.01,100.5) -- cycle ;
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (447.01,127.5) .. controls (447.01,125.57) and (448.57,124.01) .. (450.5,124.01) .. controls (452.44,124.01) and (454,125.57) .. (454,127.5) .. controls (454,129.44) and (452.44,131) .. (450.5,131) .. controls (448.57,131) and (447.01,129.44) .. (447.01,127.5) -- cycle ;
		\draw    (206,113) .. controls (235.71,61.75) and (366.37,62.45) .. (388.38,120.46) ;
		\draw [shift={(389.01,122.24)}, rotate = 251.51] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw (97,243) node [anchor=north west][inner sep=0.75pt]   [align=left] {Input/Domain};
		\draw (388,242) node [anchor=north west][inner sep=0.75pt]   [align=left] {Output/Range};
		\draw (351,184.4) node [anchor=north west][inner sep=0.75pt]    {$\text{Im}(\matrixM)$};
		\draw (291,54.4) node [anchor=north west][inner sep=0.75pt]    {$\matrixM$};
	\end{tikzpicture}
	\end{center}
\end{eg}
\begin{thm}[Invertibility Criteria]
	Let $\matrixA$ be an $n\times m$ matrix: 
	\begin{enumerate}
		\item $\matrixA$ is invertible \emph{iff} $\matrixA\vecx=\vecb$ has a unique solution $\forall\vecb\in\R^n$.
		\[\Longleftrightarrow\IM(\matrixA)=\R^n\quad\text{and}\quad\ker(\matrixA)=\{\vec{0}\}.\]
		\[\Longleftrightarrow\rank(\matrixA)=n\quad\text{and}\quad\nullity(\matrixA)=0.\]
		\item If $\matrixA$ is an $n\times m$ matrix, then the following are equivalent: 
		\begin{enumerate}
			\item $\matrixA\vecx=\vecb$ has a unique solution for all $\vecb$ in $\R^n.$
			\item $\rank(\matrixA)=n$
			\item $\nullity(\matrixA)=0$
			\item $\IM(\matrixA)=\R^n$
			\item $\ker(\matrixA)=\{\vec{0}\}$
			\item $\rref(\matrixA)=\matrixI_n$
			\item The columns of $\matrixA$ form a basis for $\R^n$
			\item The columns of $\matrixA$ span $\R^n$
			\item The columns of $\matrixA$ are $\LI$
			\item There is a matrix $\matrixB$ \emph{s.t.} \[\matrixB\matrixA=\matrixI=\matrixA\matrixB\qquad(\matrixB\coloneqq\matrixA^{-1})\] 
		\end{enumerate}
	\end{enumerate}
\end{thm}

\subsection{Coordinates}
\begin{rmk}[Goal of Coordinates]
	To describe the location of a vector within a subspace. 	
\end{rmk}
\begin{df}[Standard coordinates on $\R^n$]
	We can write $\vecx$ as a linear combination of the standard basis vectors. \[\text{i.e., }\vecx=a_1+\vece_1+a_2\vece_2+\cdots+a_n\vece_n;\ a_i\in\R.\]
	\begin{eg}
		Suppose $\vecx=\begin{bmatrix}1\\2\end{bmatrix}\in\R^2.$ Then $\vecx=\vece_1+2\vece_2.$	
		\begin{center}
		\tikzset{every picture/.style={line width=0.75pt}} 
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		\draw  (51,229.55) -- (240.01,229.55)(69.9,73) -- (69.9,246.95) (233.01,224.55) -- (240.01,229.55) -- (233.01,234.55) (64.9,80) -- (69.9,73) -- (74.9,80)  ;
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (110.01,149.77) -- (110,230) ;
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (110.01,149.77) -- (70.01,149.77) ;
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (105.51,149.77) .. controls (105.51,147.28) and (107.53,145.27) .. (110.01,145.27) .. controls (112.49,145.27) and (114.5,147.28) .. (114.5,149.77) .. controls (114.5,152.25) and (112.49,154.26) .. (110.01,154.26) .. controls (107.53,154.26) and (105.51,152.25) .. (105.51,149.77) -- cycle ;
		\draw    (69.9,229.55) -- (108,229.98) ;
		\draw [shift={(110,230)}, rotate = 180.64] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw    (69.9,229.55) -- (70.01,151.77) ;
		\draw [shift={(70.01,149.77)}, rotate = 90.08] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw (104,236.4) node [anchor=north west][inner sep=0.75pt]    {$\vece_{1}$};
		\draw (39,144.4) node [anchor=north west][inner sep=0.75pt]    {$2\vece_{2}$};
		\draw (122,126.4) node [anchor=north west][inner sep=0.75pt]    {$\vecx =\vece_{1} +2\vece_2$};
		\end{tikzpicture}
		\end{center}
	\end{eg}
\end{df}
\begin{thm}
	Let $V\subseteq\R^n$ be a subspace and $\beta=(\vecx_1,\cdots,\vecx_m)$ be a basis. Then every $\vecx\in V$ may be written as $\vecx=a_1\vecx_1+a_2\vecx_2+\cdots+a_m\vecx_m$ for some unique scalars $a_1,\cdots,a_m\in\R.$
	\begin{eg}
		Suppose $V$ is a subspace and $\beta=(\vecx_1,\vecx_2)$: 
		\begin{center}
		\tikzset{every picture/.style={line width=0.75pt}}       
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=0.07 ] (279.99,102) -- (538.1,102) -- (372.01,202.71) -- (113.9,202.71) -- cycle ;
		\draw    (168,170) -- (426.01,170) ;
		\draw    (192.01,202.71) -- (358.1,102) ;
		\draw    (246.01,169.71) -- (302.01,169.71) ;
		\draw [shift={(304.01,169.71)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw    (246.01,169.71) -- (281.29,148.73) ;
		\draw [shift={(283.01,147.71)}, rotate = 149.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (409.51,131.5) .. controls (409.51,129.02) and (411.53,127.01) .. (414.01,127.01) .. controls (416.49,127.01) and (418.5,129.02) .. (418.5,131.5) .. controls (418.5,133.99) and (416.49,136) .. (414.01,136) .. controls (411.53,136) and (409.51,133.99) .. (409.51,131.5) -- cycle ;
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (353.01,169.71) -- (416.5,131.5) ;
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (309.01,131.5) -- (414.01,131.5) ;
		\draw (527,113.4) node [anchor=north west][inner sep=0.75pt]    {$V$};
		\draw (299,173.4) node [anchor=north west][inner sep=0.75pt]    {$\vecx_{1}$};
		\draw (265,122.4) node [anchor=north west][inner sep=0.75pt]    {$\vecx_{2}$};
		\draw (382,103.4) node [anchor=north west][inner sep=0.75pt]    {$\vecx =a_{1}\vecx_1 +a_{2}\vecx_{2}$};
		\end{tikzpicture}
		\end{center}
	\end{eg}
\end{thm}
\begin{df}[$\beta$ coordinates]
	Let	$V\subseteq\R^n$ be a subspace and $\beta$ be a basis for $V$. Let $\vecx\in V$. The $\beta$\textbf{-coordinates} for $\vecx$ in $V$ is the following vector: \[\left[\vecx\right]_\beta=\begin{bmatrix}a_1\\\vdots\\a_m\end{bmatrix}\] \emph{s.t.} $\vecx=a_1\vecx_1+\cdots+a_m\vecx_m.$
	\begin{eg}
		Suppose $V=\Span\left(\begin{bmatrix}1\\1\\0\end{bmatrix},\begin{bmatrix}0\\1\\1\end{bmatrix}\right)$ and $\beta=\left(\begin{bmatrix}1\\1\\0\end{bmatrix}, \begin{bmatrix}0\\1\\1\end{bmatrix}\right)$.\\
		Let $\vecx=2\begin{bmatrix}1\\1\\0\end{bmatrix}+1\begin{bmatrix}0\\1\\1\end{bmatrix}=\begin{bmatrix}2\\3\\1\end{bmatrix}$. 
		Then, $[\vecx]_\beta=\begin{bmatrix}2\\1\end{bmatrix}$.
	\end{eg}
	\begin{rmk} 
		$V$ in general has many basis. The $\beta$-coordinates depend on the basis. Also, in general, coordinate axes are not perpendicular. 
	\end{rmk}
\end{df}
\begin{eg}
	Let $V\subseteq\R^3$ be the subspace spanned by $\vecv_1=\begin{bmatrix}1\\2\\1\end{bmatrix},\ \vecv_2=\begin{bmatrix}-3\\2\\3\end{bmatrix}.$ Let $\beta=(\vecv_1,\vecv_2)$ and $\vecx=\begin{bmatrix}-1\\2\\2\end{bmatrix}$. Find $[\vecx]_\beta.$	\\
	\begin{sol}
		Find $[\vecx]_\beta=\begin{bmatrix}c_1\\c_2\end{bmatrix}$ \emph{s.t.} $\vecx=c_1\vecv_1+c_2\vecv_2.$ (Find an expression for $\vecx$ in the span of $\vecv_1$ and $\vecv_2$, which is the image of $\matrixS=\begin{bmatrix}|&|\\\vecv_1&\vecv_2\\|&|\end{bmatrix}$. Hence, we need to find $\vecx=\matrixS\begin{bmatrix}c_1\\c_2\end{bmatrix}$ (i.e., solve $\matrixS\vec{c}=\vecx$).\\
		Form augmented matrix $\left[\begin{array}{c:c}\matrixS&\vecx\end{array}\right]$: 
		$$\begin{aligned}
			\left[\begin{array}{c:c}\matrixS&\vecx\end{array}\right]&=\left[\begin{array}{c:c}\begin{matrix}|&|\\\vecv_1&\vecv_2\\|&|\end{matrix}&\vecx\end{array}\right]=\left[\begin{array}{c:c}\begin{matrix}1&-3\\2&2\\1&3\end{matrix}&\begin{matrix}-1\\2\\2\end{matrix}\end{array}\right]\\
			&\xRightarrow[\text{reduce}]{\text{Row}}\left[\begin{array}{c:c}\begin{matrix}1&0\\0&1\\0&0\end{matrix}&\begin{matrix}^1/_2\\^1/_2\\0\end{matrix}\end{array}\right]
		\end{aligned}$$
		\[\therefore[\vecx]_\beta=\begin{bmatrix}^1/_2\\^1/_2\\\end{bmatrix}.\]
	\end{sol}
	\begin{rmk} If $(\vecv_1,\cdots,\vecv_m)=\beta$ is a basis for a subspace $V$, and $\matrixS\coloneqq\begin{bmatrix}|&&|\\\vecv_1&\cdots&\vecv_m\\|&&|\end{bmatrix}$, then $\matrixS$ converts $\beta$-coordinates to standard coordinates. \[\text{i.e., }\matrixS[\vecx]_\beta=\vecx.\]
	\end{rmk}
\end{eg}
\begin{eg}[$\beta$-coordinates Under Linear Transformation]
	Consider $\T: \R^2\to\R^2$ defined by matrix $\begin{bmatrix}1&3\\2&2\end{bmatrix}$. Let $\vecx\in\R^2$ be the vector whose $\beta$-coordinates are $[\vecx]_\beta=\begin{bmatrix}1\\3\end{bmatrix}$, where $\beta=\left(\begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}1\\-1\end{bmatrix}\right)$. Find $[\T\vecx]_\beta$.\\
	\begin{sol}
		First, unpack the question: 
		\begin{center}
		\tikzset{every picture/.style={line width=0.75pt}} 
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		\draw  (44,147.46) -- (245.01,147.46)(140.5,57.34) -- (140.5,238.18) (238.01,142.46) -- (245.01,147.46) -- (238.01,152.46) (135.5,64.34) -- (140.5,57.34) -- (145.5,64.34)  ;
		\draw    (211.46,76.5) -- (69.55,218.42) ;
		\draw    (84.01,91.18) -- (208.01,215.18) ;
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (172,116) -- (234.18,178.18) ;
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (203.01,209.35) -- (234.18,178.18) ;
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (229.43,178.18) .. controls (229.43,175.56) and (231.56,173.43) .. (234.18,173.43) .. controls (236.8,173.43) and (238.93,175.56) .. (238.93,178.18) .. controls (238.93,180.8) and (236.8,182.93) .. (234.18,182.93) .. controls (231.56,182.93) and (229.43,180.8) .. (229.43,178.18) -- cycle ;
		\draw  (375,145.46) -- (576.01,145.46)(471.5,55.34) -- (471.5,236.18) (569.01,140.46) -- (576.01,145.46) -- (569.01,150.46) (466.5,62.34) -- (471.5,55.34) -- (476.5,62.34)  ;
		\draw    (542.46,74.5) -- (400.55,216.42) ;
		\draw    (415.01,89.18) -- (539.01,213.18) ;
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (503,114) -- (565.18,176.18) ;
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (534.01,207.35) -- (565.18,176.18) ;
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (560.43,176.18) .. controls (560.43,173.56) and (562.56,171.43) .. (565.18,171.43) .. controls (567.8,171.43) and (569.93,173.56) .. (569.93,176.18) .. controls (569.93,178.8) and (567.8,180.93) .. (565.18,180.93) .. controls (562.56,180.93) and (560.43,178.8) .. (560.43,176.18) -- cycle ;
		\draw    (238,122) .. controls (250.88,77.72) and (362.75,64.79) .. (393.11,120.56) ;
		\draw [shift={(394.01,122.27)}, rotate = 243.34] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw    (140.5,147.46) -- (140.5,101.73) ;
		\draw [shift={(140.5,99.73)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw    (140.5,147.46) -- (182.01,147.28) ;
		\draw [shift={(184.01,147.27)}, rotate = 179.75] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw    (471.5,145.46) -- (557.08,122.78) ;
		\draw [shift={(559.01,122.27)}, rotate = 165.15] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw    (471.5,145.46) -- (483.34,112.15) ;
		\draw [shift={(484.01,110.27)}, rotate = 109.56] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw (157,92.4) node [anchor=north west][inner sep=0.75pt]    {$\vecv_{1}$};
		\draw (181,211.4) node [anchor=north west][inner sep=0.75pt]    {$3\vecv_{2}$};
		\draw (242,165.4) node [anchor=north west][inner sep=0.75pt]    {$[\vecx]_{\beta } =\begin{bmatrix}1\\3\end{bmatrix}$};
		\draw (512,103.4) node [anchor=north west][inner sep=0.75pt]    {$\vecv_{1}$};
		\draw (512,209.4) node [anchor=north west][inner sep=0.75pt]    {$\vecv_{2}$};
		\draw (573,163.4) node [anchor=north west][inner sep=0.75pt]    {$[ \T \vec{x}]_{\beta } =?$};
		\draw (293,30) node [anchor=north west][inner sep=0.75pt]    {$\begin{bmatrix}1 & 3\\2 & 2\end{bmatrix}$};
		\draw (310,93.4) node [anchor=north west][inner sep=0.75pt]    {$\T $};
		\draw (186.01,150.67) node [anchor=north west][inner sep=0.75pt]    {$a_1\vece_{1}$};
		\draw (122.01,96.67) node [anchor=north west][inner sep=0.75pt]    {$a_2\vece_{2}$};
		\draw (560.01,114.67) node [anchor=north west][inner sep=0.75pt]    {$\T \vec{e}_{1}$};
		\draw (473.01,88.67) node [anchor=north west][inner sep=0.75pt]    {$\T \vec{e}_{2}$};
		\end{tikzpicture}
		\end{center}
		To solve this question: 
		\begin{enumerate}
			\item Find standard coordinates for $\vecx$: 
				\[\matrixS=\begin{bmatrix}1&1\\1&-1\end{bmatrix}\Longrightarrow[\vecx]_{st}=\matrixS[\vecx]_\beta=\begin{bmatrix}1&1\\1&-1\end{bmatrix}\begin{bmatrix}1\\3\end{bmatrix}=\begin{bmatrix}4\\-2\end{bmatrix}\]
			\item Multiply $[\vecx]_{st}$ by $\T$: 
				\[\T[\vecx]_{st}=\begin{bmatrix}1 & 3\\2 & 2\end{bmatrix}\begin{bmatrix}4\\-2\end{bmatrix}=\begin{bmatrix}-2\\-4\end{bmatrix}\]
			\item Compute $[\T\vecx]_\beta$.
				\[\T[\vecx]_{st}=\matrixS[\T\vecx]_\beta\Longrightarrow\begin{bmatrix}-2\\-4\end{bmatrix}=\begin{bmatrix}1&1\\1&-1\end{bmatrix}[\T\vecx]_\beta\Longrightarrow[\T\vecx]_\beta=\begin{bmatrix}1\\3\end{bmatrix}.\]
		\end{enumerate}
	\end{sol}
\end{eg}
\begin{thm}
	Let $\T: \R^n\to\R^m$ be a linear transformation and $\beta=(\vecv_1,\cdots,\vecv_n)$ be a basis for $\R^n$. Let $\vecx\in\R^n$: \[[\T\vecx]_\beta=\matrixS^{-1}\T\matrixS[\vecx]_\beta,\qquad\text{where}\quad\matrixS=\begin{bmatrix}|&&|\\\vecv_1&\cdots&\vecv_n\\|&&|\end{bmatrix}.\] 	\[[\T\vecx]_\beta=[\T]_\beta[\vecx]_\beta.\]
\end{thm}
\begin{thm}
	The matrix for $\T$ with respect to the basis $\beta$ is \[[\T]_\beta=\matrixS^{-1}\T\matrixS.\]	
	\begin{eg}
		Let $\T=\begin{bmatrix}1&3\\2&2\end{bmatrix}$ and $\beta=\left(\begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}1\\-1\end{bmatrix}\right)$. Then \[[\T]_\beta=\begin{bmatrix}^1/_2&^1/_2\\^1/_2&-^1/_2\end{bmatrix}\begin{bmatrix}1&3\\2&2\end{bmatrix}\begin{bmatrix}1&1\\1&-1\end{bmatrix}=\begin{bmatrix}4&-1\\0&-1\end{bmatrix}.\]
	\end{eg}
\end{thm}

\newpage
\section{Approx. Solution of $\matrixA\vecx=\vecb$}
\subsection{Lengths and Angles in $\R^n$}
\begin{df}[Dot Product]
	Let $\vecx=\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}$ and $\vecy=\begin{bmatrix}y_1\\\vdots\\y_n\end{bmatrix}\in\R^n$. The \textbf{dot product} of $\vecx$ and $\vecy$ is the following number: \[\vecx\cdot\vecy=\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}\cdot\begin{bmatrix}y_1\\\vdots\\y_n\end{bmatrix}=x_1y_1+x_2y_2+\cdots+x_ny_n=\begin{bmatrix}x_1&\cdots&x_n\end{bmatrix}\begin{bmatrix}y_1\\\vdots\\y_n\end{bmatrix}\]	
	\begin{eg}
		\[\begin{bmatrix}1\\2\\3\end{bmatrix}\cdot\begin{bmatrix}7\\5\\2\end{bmatrix}=1\times7+2\times5+3\times2=23.\]	
	\end{eg}
\end{df}
\begin{thm}
	Algebraic property of dot products:
	\begin{enumerate}
		\item $\vecx\cdot\vecy=\vecy\cdot\vecx$
		\item $\vecx\cdot(\vecy_1+\vecy_2)=\vecx\cdot\vecy_1+\vecx\cdot\vecy_2$\\
			$(\vecx_1+\vecx_2)\cdot\vecy=\vecx_1\cdot\vecy+\vecx_2\cdot\vecy$
		\item $\vecx\cdot(c\vecy)=c(\vecx\cdot\vecy)=(c\vecx)\cdot\vecy$
	\end{enumerate}
\end{thm}
\begin{df}[Length]
	Let $\vecx\in\R^n$. The \textbf{length} of $\vecx$ is the following number: 
	\[\|\vecx\|\coloneqq\sqrt{\vecx\cdot\vecx}=\sqrt{x_1^2+x_2^2+\cdots+x_n^2},\text{ where }\vecx=\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}\]	
	\begin{eg}
		\[\left\|\begin{bmatrix}4\\3\end{bmatrix}\right\|=\sqrt{4^2+3^2}=5\]
		\begin{rmk}
			In $\R^2$, the definition of length is the Pythagorean theorem.	
		\end{rmk}
	\end{eg}
\end{df}
\begin{thm}[Angle Between Vectors]
	Let $\theta$ be the angle between $\vecx$ and $\vecy$. We then have \[\cos\theta=\frac{\|\vecx\|^2+\|\vecy\|^2-\|\vecy-\vecx\|^2}{2\|\vecx\|\|\vecy\|}\]
	\begin{prf}
		Assume vectors $\vecx$ and $\vecy$ are drawn as below. 
		\begin{center}
		\tikzset{every picture/.style={line width=0.75pt}} 
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		\draw    (148.62,95.54) -- (36.01,212.1) ;
		\draw [shift={(150.01,94.1)}, rotate = 134.01] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw    (171.14,264.38) -- (36.01,212.1) ;
		\draw [shift={(173.01,265.1)}, rotate = 201.15] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw    (173.01,265.1) -- (150.01,94.1) ;
		\draw (50,199.4) node [anchor=north west][inner sep=0.75pt]    {$\theta $};
		\draw (69,135.4) node [anchor=north west][inner sep=0.75pt]    {$\| \vec{x} \| $};
		\draw (79,238.4) node [anchor=north west][inner sep=0.75pt]    {$\| \vec{y} \| $};
		\draw (137,75.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{x}$};
		\draw (175.01,268.5) node [anchor=north west][inner sep=0.75pt]    {$\vec{y}$};
		\draw (167,173.4) node [anchor=north west][inner sep=0.75pt]    {$L=\| \vec{y} -\vec{x} \| $};
		\end{tikzpicture}
		\end{center}
		By the cosine rule, we have:
		\[L^2=\|\vecx\|^2+\|\vecy\|^2-2\|\vecx\|\|\vecy\|\cos\theta\]
		So, \[\cos\theta=\frac{\|\vecx\|^2+\|\vecy\|^2-L^2}{2\|\vecx\|\|\vecy\|}=\frac{\|\vecx\|^2+\|\vecy\|^2-\|\vecy-\vecx\|^2}{2\|\vecx\|\|\vecy\|}.\]
	\end{prf}
\end{thm}
\begin{thm}
	Relationship of angle and dot products:
	\begin{enumerate}
		\item $\vecx\cdot\vecy>0$ if $\theta<90^\circ$
		\item $\vecx\cdot\vecy=0$ if $\theta=90^\circ$	
		\item $\vecx\cdot\vecy<0$ if $\theta>90^\circ$
	\end{enumerate}
	\begin{prf}
		$$\begin{aligned}
			\|\vecy-\vecx\|^2&=(\vecy-\vecx)\cdot(\vecy-\vecx)\\
			&=(\vecy-\vecx)\cdot\vecy-(\vecy-\vecx)\cdot\vecx\\
			&=\vecy\cdot\vecy-\vecx\cdot\vecy-\vecy\cdot\vecx+\vecx\cdot\vecx\\
			&=\vecy\cdot\vecy-2\vecx\cdot\vecy+\vecx\cdot\vecx\\
			&=\|\vecy\|^2-2\vecx\cdot\vecy+\|\vecx\|^2
		\end{aligned}$$
		Think of Pythagonean theorem: 
		\begin{center}
			\tikzset{every picture/.style={line width=0.75pt}}
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			\draw    (211,180) -- (411.01,180) ;
			\draw    (211,96.35) -- (211,180) ;
			\draw   (127.35,180) .. controls (127.35,133.8) and (164.8,96.35) .. (211,96.35) .. controls (257.2,96.35) and (294.65,133.8) .. (294.65,180) .. controls (294.65,226.2) and (257.2,263.65) .. (211,263.65) .. controls (164.8,263.65) and (127.35,226.2) .. (127.35,180) -- cycle ;
			\draw    (157.01,115.77) -- (211,180) ;
			\draw    (254.01,108.77) -- (211,180) ;
			\draw    (254.01,108.77) -- (411.01,180) ;
			\draw    (211,96.35) -- (411.01,180) ;
			\draw    (157.01,115.77) -- (411.01,180) ;
			\draw (222,161.4) node [anchor=north west][inner sep=0.75pt]    {$\theta $};
			\draw (285,182.4) node [anchor=north west][inner sep=0.75pt]    {$\| \vec{y} \| $};
			\draw (163,146.4) node [anchor=north west][inner sep=0.75pt]    {$\| \vec{x} \| $};
			\draw (300,111.4) node [anchor=north west][inner sep=0.75pt]    {$L=\| \vec{y} -\vec{x} \| $};
			\end{tikzpicture}
		\end{center}
		\begin{itemize}
			\item If $\theta<90^\circ,\ \|\vecy-\vecx\|^2<\|\vecy\|+\|\vec\|^2\Longrightarrow\vecx\cdot\vecy>0.$	
			\item If $\theta=90^\circ,\ \|\vecy-\vecx\|^2=\|\vecy\|+\|\vec\|^2\Longrightarrow\vecx\cdot\vecy=0.$	
			\item If $\theta>90^\circ,\ \|\vecy-\vecx\|^2>\|\vecy\|+\|\vec\|^2\Longrightarrow\vecx\cdot\vecy<0.$	
		\end{itemize}
	\end{prf}
\end{thm}
\begin{df}[Perpendicular]
	Let $\vecx,\vecy\in\R^n$. Then, $\vecx$ and $\vecy$ are \textbf{perpendicular} \emph{iff} $\vecx\cdot\vecy=0$. (Equivalently: orthogonal)	
\end{df}
\begin{thm}
	Suppose $\matrixA$ is an $1\times n$ matrix \emph{s.t.} $\matrixA=\begin{bmatrix}a_1&\cdots&a_n\end{bmatrix}.$ Then, $\matrixA^\T=\begin{bmatrix}a_1\\\vdots\\a_n\end{bmatrix}=\vecv$. Thus, 	$\matrixA\vecx=\vecv\cdot\vecx$.
\end{thm}
\begin{thm}
	\[\vecv\perp\vecx\Longleftrightarrow\vecv\cdot\vecx=0\Longleftrightarrow\matrixA\vecx=0\Longrightarrow\vecx\in\ker(\matrixA).\]	
	\begin{itemize}
		\item Let $\vecv\neq\vec{0}$. The set $\{\vecx\ |\ \vecx\perp\vecv\}$ is a subspace of dimension $m-1$.
		\item Let $\matrixA: \R^m\to\R^n$ be a linear transformation. Then, the kernel of $\matrixA$ is the set of all vectors $\vecx\in\R^m$, which are perpendicular to the row of the matrix for $\matrixA$.
	\end{itemize}
\end{thm}
\begin{thm}
	\[\vecx\cdot\vecv=\|\vecx\|\|\vecv\|\cos\theta\]
	\begin{prf}
		\begin{enumerate}
			\item $\vecv\cdot\vecx$ is constant along translates of the subspace perpendicular to the line spanned by $\vecv$: \[\vecv\cdot\vecx=\matrixA\vecx=\vecb\]
			\item Project $\vecx$ into the line spanned by $\vecv$: 
			\begin{center}
			\tikzset{every picture/.style={line width=0.75pt}} 
			\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
			\draw    (152.62,63.54) -- (40.01,180.1) ;
			\draw [shift={(154.01,62.1)}, rotate = 134.01] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
			\draw    (175.14,232.38) -- (40.01,180.1) ;
			\draw [shift={(177.01,233.1)}, rotate = 201.15] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
			\draw    (177.01,233.1) -- (154.01,62.1) ;
			\draw    (32.01,32.76) -- (45.01,294.76) ;
			\draw    (151.01,27) -- (164.01,289) ;
			\draw [line width=2.25]    (40.01,180.1) -- (157.28,225.32) ;
			\draw [shift={(161.01,226.76)}, rotate = 201.09] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=2.25]    (17.49,-5.26) .. controls (11.12,-2.23) and (5.29,-0.48) .. (0,0) .. controls (5.29,0.48) and (11.12,2.23) .. (17.49,5.26)   ;
			\draw (73,103.4) node [anchor=north west][inner sep=0.75pt]    {$\| \vec{x} \| $};
			\draw (90,220.4) node [anchor=north west][inner sep=0.75pt]    {$\| \vec{v} \| $};
			\draw (141,43.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{x}$};
			\draw (179.01,236.5) node [anchor=north west][inner sep=0.75pt]    {$\vec{v}$};
			\draw (50,276.4) node [anchor=north west][inner sep=0.75pt]    {$\ker(\matrixA)$};
			\draw (169,273.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{x} +\ker(\matrixA)$};
			\draw (48,169.4) node [anchor=north west][inner sep=0.75pt]    {$\theta $};
			\draw (130,197.4) node [anchor=north west][inner sep=0.75pt]    {$c\vec{v}$};
			\end{tikzpicture}
			\end{center}
			\item Use trigonometry to calculate the projection: 
				\[c\vecv=(\|\vecx\|\cos\theta)\left(\frac{\vecv}{\|\vecv\|}\right)\]
				$$\begin{aligned}
					\vecv\cdot\vecx&=\vecv\cdot c\vecv\\
					&=\frac{\|\vecx\|\cos\theta}{\|\vecv\|}\|\vecv\|^2=\|\vecx\|\cdot\|\vecv\|\cos\theta\\
					&\Rightarrow\ \theta=\arccos\left(\frac{\vecx\vecv}{\|\vecx\|\|\vecv\|}\right)
				\end{aligned}$$
		\end{enumerate}
	\end{prf}
\end{thm}
\begin{thm}
	Projection of $\vecx$ into line spanned by $\vecv$ is given by the following formula: 
	$$\begin{aligned}
		\text{Projection}=c\vecv&=\frac{\|\vecx\|\cos\theta}{\|\vecv\|}\vecv\\
		&=\frac{\vecx\cdot\vecv}{\|\vecv\|^2}\vecv\\
		&=\frac{\vecx\cdot\vecv}{\vecv\cdot\vecv}\vecv.
	\end{aligned}$$	
\end{thm}
\begin{df}[Orthogonal Complement]
	Let $V\subseteq\R^n$ be a subspace. The \textbf{orthogonal complement} of $V$ is the set of vectors perpendicular to all vectors in $V$: 
	\[V^\perp=\left\{\vecx\in\R^n;\ \vecv\cdot\vecx=0\quad\forall\vecv\in\R^n\right\}.\]	
	\begin{eg} The orthogonal complement of a line with a slope $m$ through the origin is a line through the origin with a slop of $-\dfrac{1}{m}$. \end{eg}
\end{df}
\begin{thm}
	Let $V$ be a subspace. If $\vecv_1,\cdots,\vecv_k\in V$ is a spanning set, (i.e., $V=\Span(\vecv_1,\cdots,\vecv_k)$), then $\vecx\in V^\perp$ \emph{iff} $\vecv_1\cdot\vecx=0,\ \vecv_2\cdot\vecx=0,\cdots,\vecv_k\cdot\vecx=0.$\\
	\begin{prf}
		``Perpendicular to everything'' implies $\vecv_i\cdot\vecx=0\quad\forall\vecv\in V$, then $\vecv=c_1\vecv_1+\cdots+c_k\vecv_k\Longrightarrow\vecx\cdot\vecv=c_1(\vecx\cdot\vecv_1)+\cdots+c_k(\vecx\cdot\vecv_k)=0\Longrightarrow\therefore\vecx\perp\vecv.$
	\end{prf}
\end{thm}
\begin{thm}
	Let $V\subseteq\R^n$ be a subspace, $V^\perp$ is a subspace. Specifically, if $V=\Span(\vecv_1,\cdots,\vecv_k)$, then \[V^\perp=\ker\left(\begin{bmatrix}-&\vecv_1&-\\-&\vecv_2&-\\&\vdots&\\-&\vecv_k&-\end{bmatrix}\right)\]
	\begin{prf}
		\[\begin{bmatrix}-&\vecv_1&-\\-&\vecv_2&-\\&\vdots&\\-&\vecv_k&-\end{bmatrix}\begin{bmatrix}|\\\vecx\\|\end{bmatrix}=\begin{bmatrix}\vecx\cdot\vecv_1\\\vdots\\\vecx\cdot\vecv_k\end{bmatrix}=\begin{bmatrix}0\\\vdots\\0\end{bmatrix}\]	
	\end{prf}
	\begin{eg}
		Let $V=\Span\left(\begin{bmatrix}1\\0\\1\\1\end{bmatrix},\begin{bmatrix}1\\-1\\0\\1\end{bmatrix}\right)$. Compute $V^\perp$.	
	\begin{sol}
		\[\begin{bmatrix}1&0	&1&1\\1&-1&0&1\end{bmatrix}\xRightarrow[\text{reduce}]{\text{Row}}\begin{bmatrix}1&0&1&1\\0&-1&-1&0\end{bmatrix}\Longrightarrow\rref=\begin{bmatrix}1&0&1&1\\0&1&1&0\end{bmatrix}\]
		Unpack, we have \[\begin{cases}x_1=-x_3-x_4\\x_2=-x_3\end{cases}\]
		\[\therefore V^\perp=\text{Kernel}=\left\{\begin{bmatrix}-x_3-x_4\\-x_3\\x_3\\x_4\end{bmatrix};\ x_{3,4}\in\R\right\}=\Span\left(\begin{bmatrix}-1\\-1\\1\\0\end{bmatrix},\begin{bmatrix}-1\\0\\0\\1\end{bmatrix}\right)\]
	\end{sol}
	\end{eg}
\end{thm}

\subsection{Orthogonal Projection}
\begin{thm}
	Let $V\subseteq\R^n$ be a subspace and $\vecx\in\R^n$. Then, $\vecx$ can be written uniquely as \[\vecx=\vecx^\parallel+\vecx^\perp, \] 	when $\vecx^\parallel\in V$ and $\vecx^\perp\in V$.
\end{thm}
\begin{df}[Orthogonal Projection]
	Let $V\subseteq\R^n$ be a subspace. The \textbf{orthogonal projection} of $\vecx$ into $V$ is the vector $\vecx^\parallel$. The map $\vecx\mapsto\vecx^\parallel$ is denoted as $\Proj_V:\R^n\to\R^n$.	
\end{df}
\begin{thm}
	Computing $\Proj_V(\vecx)\coloneqq\vecx^\parallel$: 
	\begin{enumerate}
		\item Let $\vecv_1,\cdots,\vecv_k$ be a basis for $V$: \[\matrixA^\T=\begin{bmatrix}-&\vecv_1&-\\&\vdots&\\-&\vecv_k&-\end{bmatrix}\quad\text{and}\quad\matrixA=\begin{bmatrix}|&&|\\\vecv_1&cdots&\vecv_k\\|&&|\end{bmatrix}\]
		\[\boxed{\matrixA^\T\vecx^\parallel=\matrixA^\T\vecx}\]
		\begin{center}
		\tikzset{every picture/.style={line width=0.75pt}} 
		\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		\draw   (252.01,11) -- (381.01,11) -- (345,109.05) -- (216,109.05) -- cycle ;
		\draw   (111.01,148) -- (240.01,148) -- (204,246.05) -- (75,246.05) -- cycle ;
		\draw    (202.01,70.05) .. controls (128.38,42.19) and (40.9,109.42) .. (79.42,166.19) ;
		\draw [shift={(80.01,167.05)}, rotate = 234.92] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (321.01,78.5) .. controls (321.01,75.47) and (323.47,73.01) .. (326.5,73.01) .. controls (329.54,73.01) and (332,75.47) .. (332,78.5) .. controls (332,81.54) and (329.54,84) .. (326.5,84) .. controls (323.47,84) and (321.01,81.54) .. (321.01,78.5) -- cycle ;
		\draw   (424.01,151) -- (553.01,151) -- (517,249.05) -- (388,249.05) -- cycle ;
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (492.01,217.5) .. controls (492.01,214.47) and (494.47,212.01) .. (497.5,212.01) .. controls (500.54,212.01) and (503,214.47) .. (503,217.5) .. controls (503,220.54) and (500.54,223) .. (497.5,223) .. controls (494.47,223) and (492.01,220.54) .. (492.01,217.5) -- cycle ;
		\draw    (125.01,287.05) -- (125.01,105.05) ;
		\draw [shift={(125.01,103.05)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw    (125.01,215.05) -- (169.56,172.43) ;
		\draw [shift={(171.01,171.05)}, rotate = 136.27] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (125.01,171.05) -- (171.01,171.05) ;
		\draw    (125.01,215.05) -- (168.01,215.05) ;
		\draw [shift={(170.01,215.05)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw  [dash pattern={on 4.5pt off 4.5pt}]  (171.01,171.05) -- (170.01,215.05) ;
		\draw    (125.01,215.05) -- (125.01,173.05) ;
		\draw [shift={(125.01,171.05)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw    (229,200) .. controls (264.65,244.6) and (363.02,242.1) .. (397.97,202.27) ;
		\draw [shift={(399.01,201.05)}, rotate = 129.67] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
		\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (164.51,215.05) .. controls (164.51,212.01) and (166.98,209.55) .. (170.01,209.55) .. controls (173.04,209.55) and (175.5,212.01) .. (175.5,215.05) .. controls (175.5,218.08) and (173.04,220.54) .. (170.01,220.54) .. controls (166.98,220.54) and (164.51,218.08) .. (164.51,215.05) -- cycle ;
		\draw (133,12.4) node [anchor=north west][inner sep=0.75pt]    {$V=\IM(\matrixA)$};
		\draw (72,70.4) node [anchor=north west][inner sep=0.75pt]    {$\matrixA$};
		\draw (309,53.4) node [anchor=north west][inner sep=0.75pt]    {$\matrixA^{\T}\vec{x}$};
		\draw (480,189.4) node [anchor=north west][inner sep=0.75pt]    {$\matrixA^{\T}\vec{x}$};
		\draw (128,95.4) node [anchor=north west][inner sep=0.75pt]    {$V^{\perp }$};
		\draw (174,157.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{x}$};
		\draw (172.01,218.45) node [anchor=north west][inner sep=0.75pt]    {$\vec{x}^{\parallel }$};
		\draw (143,261.4) node [anchor=north west][inner sep=0.75pt]    {$\mathbb{R}^{n}$};
		\draw (276,111.4) node [anchor=north west][inner sep=0.75pt]    {$\mathbb{R}^{k}$};
		\draw (455,257.4) node [anchor=north west][inner sep=0.75pt]    {$\mathbb{R}^{k}$};
		\draw (104.01,185.45) node [anchor=north west][inner sep=0.75pt]    {$\vec{x}^{\perp }$};
		\draw (296,238.4) node [anchor=north west][inner sep=0.75pt]    {$\matrixA^{\T}$};
		\draw (261,195.4) node [anchor=north west][inner sep=0.75pt]    {$\ker\left( \matrixA^{\T}\right) =V^{\perp }$};
		\end{tikzpicture}
		\end{center}
		\item Slove $\matrixA^\T\matrixA\vec{c}=\matrixA^\T\vecx$ for $\vec{c}$.
		\item $\vecx^\parallel=\matrixA\vec{c}$
	\end{enumerate}	
	\begin{eg}
		Let $V=\Span\left(\begin{bmatrix}1\\1\\0\end{bmatrix},\begin{bmatrix}0\\1\\1\end{bmatrix}\right)$ and $\vecx=\begin{bmatrix}1\\2\\3\end{bmatrix}$. Compute the projection of $\vecx$ onto $V$.\\
		\begin{sol}
			\begin{enumerate}
				\item \[\matrixA=\begin{bmatrix}1&0\\1&1\\0&1\end{bmatrix}\quad\text{and}\quad\matrixA^\T=\begin{bmatrix}1&1&0\\0&1&1\end{bmatrix}\]
				\item Compute $\matrixA^\T\matrixA$ and $\matrixA^\T\vecx$: \[\matrixA^\T\matrixA=\begin{bmatrix}1&1&0\\0&1&1\end{bmatrix}\begin{bmatrix}1&0\\1&1\\0&1\end{bmatrix}=\begin{bmatrix}2&1\\1&2\end{bmatrix}\]\[\matrixA^\T\vecx=\begin{bmatrix}1&1&0\\0&1&1\end{bmatrix}\begin{bmatrix}1\\2\\3\end{bmatrix}=\begin{bmatrix}3\\5\end{bmatrix}\]
				\item Solve $\matrixA^\T\matrixA\vec{c}=\matrixA^\T\vecx$ for $\vec{c}$: 
				\[\begin{bmatrix}2&1\\1&2\end{bmatrix}\vec{c}=\begin{bmatrix}3\\5\end{bmatrix}\Longrightarrow\left[\begin{array}{c:c}\begin{matrix}2&1\\1&2\end{matrix}&\begin{matrix}3\\5\end{matrix}\end{array}\right]\xrightarrow[\text{reduce}]{\text{Row}}\left[\begin{array}{c:c}\begin{matrix}1&0\\0&1\end{matrix}&\begin{matrix}^1/_3\\^7/_3\end{matrix}\end{array}\right]\]
				\[\therefore\vec{c}=\begin{bmatrix}^1/_3\\^7/_3\end{bmatrix}\]
				\item Compute $\matrixA\vec{c}=\vecx^\parallel$
				\[\vecx^\parallel=\begin{bmatrix}1&0\\1&1\\0&1\end{bmatrix}\begin{bmatrix}^1/_3\\^7/_3\end{bmatrix}=\begin{bmatrix}^1/_3\\^8/_3\\^7/_3\end{bmatrix}\]
				\[\therefore\vecx^\perp=\vecx-\vecx^\parallel=\begin{bmatrix}1\\2\\3\end{bmatrix}-\begin{bmatrix}^1/_3\\^8/_3\\^7/_3\end{bmatrix}=\begin{bmatrix}^2/_3\\-^2/_3\\^2/_3\end{bmatrix}\]
			\end{enumerate}
		\end{sol}
	\end{eg}
\end{thm}
\begin{df}[Transpose of a Matrix]
	Let $\matrixA$ be an $n\times m$ matrix. The \textbf{transpose} of $\matrixA$ is the $m\times n$ matrix $\matrixA^\T$ whose rows are the columns of $\matrixA$: \[\matrixA=\begin{bmatrix}|&&|\\\vecv_1&\cdots&\vecv_k\\|&&|\end{bmatrix};\ \matrixA^\T=\begin{bmatrix}-&\vecv_1&-\\&\vdots&\\=&\vecv_k&-\end{bmatrix}\]	
	Equivalently, the $ij$-entry of $\matrixA$ is the $ji$-entry of $\matrixA^\T$.\\
	Equivalently, whose columns are rows of $\matrixA$.
\end{df}
\begin{thm}
	\[\ker(\matrixA^\T)=\IM(\matrixA)^\perp\]	
\end{thm}
\begin{rmk}
	In general, if $\rank(\matrixA)$ is less than the dimension of range, small perturbations of any $\vecb\in\IM(\matrixA)$ lie outside the image of $\matrixA$. In such cases, rather than try to find $\vecx$ \emph{s.t.} $\matrixA\vecx=\vecb$, try to find $\vecx$ \emph{x.t.} $\matrixA\vecx$ is as close as to $\vecb$ as possible. 	
	\begin{framed}
		Problem: Find $\vecx$ \emph{s.t.} $\|\matrixA\vecx-\vecb\|$ is as small as possible (minimized).
		\begin{itemize}
			\item The solution agrees with solving $\matrixA\vecx=\vecb$ when there are solutions.
			\item This question always has solutions.
		\end{itemize}	
	\end{framed}
	\begin{sol}
		\begin{enumerate}
			\item Find $\vecb*\in\IM(\matrixA)$ which are as close as to $\vecb$ as possible. \\
			\begin{thm} Let $\matrixA$ be an $n\times m$ matrix and $\vecb\in\R^m$. The closest vector to $\vecb$ in $\IM(\matrixA)$ is $\vecb*=\Proj_{\IM(\matrixA)}(\vecb)=\vecb^\parallel$ \end{thm}
			\item Solve $\matrixA\vecx=\vecb*$
		\end{enumerate}
	\end{sol}
	\begin{sol}(Advanced approach).
		\begin{enumerate}
			\item Approximate solutions to 	$\matrixA\vecx=\vecb$\\$\Longleftrightarrow$ Solutions $\matrixA\vecx=\vecb^\parallel$ where $\vecb^\parallel\in\IM(\matrixA)$\\$\longrightarrow\matrixA\vecx-\vecb=\vecb^\perp$ equivalently $\matrixA\vecx-\vecb$ is perpendicular to $\IM(\matrixA)$\\$\longrightarrow\matrixA^\T(\matrixA\vecx-\vecb)=0$\[\boxed{\text{ i.e., }\matrixA^\T\matrixA\vecx=\matrixA^T\vecb}\]
			\item The approximate solutions to $\matrixA\vecx=\vecb$ are exactly the solutions to $\matrixA^\T\matrixA\vecx=\matrixA^\T\vecb$.
		\end{enumerate}
	\end{sol}
	\begin{eg}
		Let $\matrixA=\begin{bmatrix}1&1\\1&2\\1&1\end{bmatrix}$ and $\vecb=\begin{bmatrix}1\\0\\0\end{bmatrix}$. Find all approximate solutions to $\matrixA\vecx=\vecb$.\\ 
		\begin{sol}
			$$\begin{aligned}
				\begin{bmatrix}1&1&1\\1&2&1\end{bmatrix}\begin{bmatrix}1&1\\1&2\\1&1\end{bmatrix}\vecx&=\begin{bmatrix}1&1&1\\1&2&1\end{bmatrix}\begin{bmatrix}1\\0\\0\end{bmatrix}\\
				\begin{bmatrix}3&4\\4&6\end{bmatrix}\vecx=\begin{bmatrix}1\\1\end{bmatrix}
			\end{aligned}$$
			Solve the equation, we have $\vecx=\begin{bmatrix}1\\-\dfrac{1}{2}\end{bmatrix}$ as the unique approximate solution to $\matrixA\vecx=\vecb$.
		\end{sol}
	\end{eg}
\end{rmk}

\subsection{Graph Fitting}
\begin{eg}
	Consider the following data set: \begin{center}\begin{tabular}{c|c}$x$&$y$\\\hline0&0\\1&0\\2&1\end{tabular}\end{center} Find a quadratic polynomial $f(x)=Ax^2+Bx+C$ (i.e., find $A,B,C\in\R$) \emph{s.t.} $f(x)=y\quad\forall x$ in the data set.\\ 
	\begin{sol}
		Plug-in data points to $f(x)=Ax^2+Bx+C$ to obtain algebraic relations between $A,\ B,$ and $C$.
		\[\begin{cases}0A+0B+C=f(0)=0\\1A+1B+C=f(1)=0\\4A+2B+C=f(2)=1\end{cases}\]	
		We can form a system of linear equations: 
		\[\begin{bmatrix}0&0&1\\1&1&1\\4&2&1\end{bmatrix}\begin{bmatrix}A\\B\\C\end{bmatrix}=\begin{bmatrix}0\\0\\1\end{bmatrix}\Longrightarrow\left[\begin{array}{c:c}\begin{matrix}0&0&1\\1&1&1\\4&2&1\end{matrix}&\begin{matrix}0\\0\\1\end{matrix}\end{array}\right]\xrightarrow[\text{reduce}]{\text{Row}}\rref=\left[\begin{array}{c:c}\begin{matrix}1&0&0\\0&1&0\\0&0&1\end{matrix}&\begin{matrix}\dfrac{1}{2}\\-\dfrac{1}{2}\\0\end{matrix}\end{array}\right]\]
		\[\therefore A=\frac{1}{2}, B=-\frac{1}{2}, C=0\]
		\[\therefore f(x)=\frac{1}{2}x^2-\frac{1}{2}x=\frac{1}{2}x(x-1)\]
	\end{sol}
\end{eg}
\begin{thm}[Fundamental Problem of Graph fitting]
	Given some data set $(x_1,y_1),\cdots,(x_m,y_m)\in\R^2$ and functions $f_1,\cdots,f_n: \R\to\R$. Find a function $f:\R\to\R$ \emph{s.t.}: 1. $f(x_i)=y_i$, and 2. $f=A_1f_1+\cdots+A_nf_n$.\\
	To solve this, plug-in data points and get a matrix equation as following: 
	\[\begin{bmatrix}f_1(x_1)&\cdots&f_n(x_1)\\\vdots&\ddots&\vdots\\f_1(x_m)&\cdots&f_n(x_m)\end{bmatrix}\begin{bmatrix}A_1\\\vdots\\A_n\end{bmatrix}=\begin{bmatrix}y_1\\\vdots\\y_m\end{bmatrix}\]	
\end{thm}
\begin{eg}\label{eg5.3.2}
	Consider the following data set: 
	\begin{center}\begin{tabular}{c|c}$x$&$y$\\\hline0&0\\1&0\\2&0\\3&1\end{tabular}\end{center} Find a quadratic polynomial $f(x)=Ax^2+Bx+C$ (i.e., find $A,B,C\in\R$) \emph{s.t.} $f(x)=y\quad\forall x,y$ in the data set.\\ 
	\begin{sol}
		Plug-in data points: \[\begin{cases}0A+0B+C=f(0)=0\\1A+1B+C=f(1)=0\\4A+2B+C=f(2)=0\\9A+3B+C=f(3)=1\end{cases}\]
		Form a matrix equation: \[\begin{bmatrix}0&0&1\\1&1&1\\4&2&1\\9&3&1\end{bmatrix}\begin{bmatrix}A\\B\\C\end{bmatrix}=\begin{bmatrix}0\\0\\0\\1\end{bmatrix}\Rightarrow\left[\begin{array}{c:c}\begin{matrix}0&0&1\\1&1&1\\4&2&1\\9&3&1\end{matrix}&\begin{matrix}0\\0\\0\\1\end{matrix}\end{array}\right]\xrightarrow[\text{reduce}]{\text{Row}}\left[\begin{array}{c:c}\begin{matrix}1&0&0\\0&1&0\\0&0&1\\0&0&0\end{matrix}&\begin{matrix}0\\0\\0\\1\end{matrix}\end{array}\right]\]
		$\therefore$ There's no solution.
	\end{sol}
\end{eg}
\begin{eg}
	Using the same data set from Example \ref{eg5.3.2}, find a quadratic polynomial \emph{s.t.} the distance between $\begin{bmatrix}0\\0\\0\\1\end{bmatrix}$ and $\begin{bmatrix}f(0)\\f(1)\\f(2)\\f(3)\end{bmatrix}$ is minimized. \\
	\begin{sol}
		This problem is equivalent to the least squares problems (finding the best approximate solution to $\matrixA\vecx=\vecb$). Solve $\matrixA^\T\matrixA\vecx=\matrixA^\T\vecb.$
		\[\matrixA=\begin{bmatrix}0&0&1\\1&1&1\\4&2&1\\9&3&1\end{bmatrix};\ \matrixA^\T=\begin{bmatrix}0&1&4&9\\0&1&2&3\\1&1&1&1\end{bmatrix}\]
		\[\matrixA^\T\matrixA=\begin{bmatrix}0&1&4&9\\0&1&2&3\\1&1&1&1\end{bmatrix}\begin{bmatrix}0&0&1\\1&1&1\\4&2&1\\9&3&1\end{bmatrix}=\begin{bmatrix}98&36&14\\36&24&6\\14&6&4\end{bmatrix}\]
		\begin{rmk}
			$\matrixA^\T\matrixA$ is \textbf{symmetric across diagonal}, meaning $a_ij$ entry is equal to $a_ji$ entry. 
		\end{rmk}
		\[\vecb=\begin{bmatrix}0\\0\\0\\1\end{bmatrix}\Longrightarrow\matrixA^\T\vecb=\begin{bmatrix}0&1&4&9\\0&1&2&3\\1&1&1&1\end{bmatrix}\begin{bmatrix}0\\0\\0\\1\end{bmatrix}=\begin{bmatrix}9\\3\\1\end{bmatrix}\]
		Form a matrix equation: 
		\[\left[\begin{array}{c:c}\begin{matrix}98&36&14\\36&24&6\\14&6&4\end{matrix}&\begin{matrix}9\\3\\1\end{matrix}\end{array}\right]\xrightarrow[\text{reduce}]{\text{Row}}\left[\begin{array}{c:c}\begin{matrix}1&0&0\\0&1&0\\0&0&1\end{matrix}&\begin{matrix}\frac{1}{4}\\-\frac{9}{20}\\\frac{1}{20}\end{matrix}\end{array}\right]\]
		\[\therefore f(x)=\frac{1}{4}x^2-\frac{9}{20}x+\frac{1}{20}\]
		\[\therefore \begin{bmatrix}f(0)\\f(1)\\f(2)\\f(3)\end{bmatrix}=\begin{bmatrix}0.05\\-0.15\\0.15\\0.95\end{bmatrix}\]
		The distance between these vectors is minimized: 
		\[d=\sqrt{(0-0.05)^2+(0+0.15)^2+(0-0.15)^2+(1-0.95)^2}\approx0.2236\]
		That is, \textbf{error}$\approx0.2236.$
	\end{sol}
\end{eg}
\begin{thm}[General Problem of Graph Fitting] 
	Given a data set $(x_1,y_1),\cdots,(x_m,y_m)\in\R^2$ and functions $f_1,\cdots,f_n:\R\to\R$. Find a function $f:\R\to\R$ \emph{s.t.}: 1. $f=A_1f_1+\cdots+A_nf_n$, and 2. $\begin{bmatrix}f(x_1)\\\vdots\\f(x_m)\end{bmatrix}$ and $\begin{bmatrix}y_1\\\vdots\\y_m\end{bmatrix}$ are as close as possible.\\
	To solve this problem, form a matrix equation and solve for its best approximate solutions: 
	\[\underbrace{\begin{bmatrix}f_1(x_1)&\cdots&f_n(x_1)\\\vdots&\ddots&\vdots\\f_1(x_m)&\cdots&f_n(x_m)\end{bmatrix}}_{\matrixA}\underbrace{\begin{bmatrix}A_1\\\vdots\\A_n\end{bmatrix}}_{\vecx}=\underbrace{\begin{bmatrix}y_1\\vdots\\y_m\end{bmatrix}}_{\vecb}\]	
	Solve for the normal equation \[\matrixA^\T\matrixA\vecx=\matrixA^\T\vecb\]
\end{thm}

\subsection{Orthogonal Linear Transformation}
\begin{df}[Orthogonal Transformation]
	Let $\T:\R^n\to\R^n$ be a linear transformation. $\T$ is called an \textbf{orthogonal transformation} if \[\T(\vecx)\cdot\T(\vecy)=\vecx\cdot\vecy\quad\forall\vecx,\vecy\in\R^n.\]	
	Equivalently, $\T$ is orthogonal \emph{iff} $\T$ preserves lengths and angles. 
	\begin{eg}
		Rotations and reflections in $\R^2$ are orthogonal. Reflections through a subspace $V\subseteq\R^n$ is also orthogonal. 	
	\end{eg}
\end{df}
\begin{df}
	Let $V\subseteq\R^n$ be a subspace and $\Proj_V:\R^n\to\R^n$ and $\Proj_{V^\perp}:\R^\to\R^n$ be the orthogonal projections into $V$ and $V^\perp$, respectively. We define $\Ref_V:\R^n\to\R^n$ by \[\Ref_V(\vecx)=\Proj_V(\vecx)-\Proj_{V^\perp}(\vecx)\]	
\end{df}
\begin{thm}[Property of $\Ref_V$]
	$\Ref_V$ is an orthogonal linear transformation. \\
	\begin{prf}
		\begin{enumerate}
			\item It's linear because the projections are linear: 
			$$\begin{aligned}
				\Ref_V(\vecx+\vecy)&=\Proj_V(\vecx+\vecy)-\Proj_{V^\perp}(\vecx+\vecy)\\
				&=\Proj_V(\vecx)+\Proj_V(\vecy)-\Proj_{V^\perp}(\vecx)-\Proj_{V^\perp}(\vecy)=\Ref_V(\vecx)+\Ref_V(\vecy)
			\end{aligned}$$	
			\[\Ref_V(c\vecx)=\Proj_V(c\vecx)-\Proj_{V^\perp}(c\vecx)=c\Proj_V(\vecx)-c\Proj_{V^\perp}(\vecx)=c\Ref_V(\vecx)\]
			\item It's orthogonal $\longleftrightarrow$ preserve lengths and angles
			$$\begin{aligned}
				\vecx\cdot\vecy&=(\vecx^\parallel+\vecx^\perp)\cdot(\vecy^\parallel)+\vecy^\perp)\\
				&=\vecx^\parallel\cdot\vecy^\parallel+\underbrace{\vecx^\perp\cdot\vecy^\parallel}_{0}+\underbrace{\vecx^\parallel\cdot\vecy^\perp}_{0}+\vecx^\perp\cdot\vecy^\perp=\vecx^\parallel\cdot\vecy^\parallel+\vecx^\perp\cdot\vecy^\perp
			\end{aligned}$$
			$$\begin{aligned}
				\Ref_V(\vecx)\cdot\Ref_V(\vecy)&=(\vecx^\parallel)-\vecx^\perp)\cdot(\vecy^\parallel-\vecy^\perp)\\
				&=\vecx^\parallel\cdot\vecy^\parallel-\underbrace{\vecx^\parallel\cdot\vecy^\perp}_{0}-\underbrace{\vecx^\perp\cdot\vecy^\parallel}_{0}+\vecx^\perp\cdot\vecy^\perp=\vecx^\parallel\cdot\vecy^\parallel+\vecx^\perp\cdot\vecy^\perp
			\end{aligned}$$
			\[\therefore\Ref_V(\vecx)\cdot\Ref_V(\vecy)=\vecx\cdot\vecy.\]
		\end{enumerate}
	\end{prf}
\end{thm}
\begin{df}[Orthogonal Matrices]
	\textbf{Orthogonal Matrices} are matrices encoding orthogonal linear transformations.	
\end{df}
\begin{thm}
	If $\T:\R^n\to\R^n$ is orthogonal, the matrix for $\T$ is $\begin{bmatrix}|&&|\\\T(\vece_1)&\cdots&\T(\vece_n)\\|&&|\end{bmatrix}$. The lengths and angles of these vectors are the same as $\vece_1,\cdots,\vece_n$ if $\T$ is orthogonal. 
\end{thm}
\begin{thm}
	\[\vece_i\cdot\vece_j=\begin{cases}1,\quad i=j\\0,\quad i\neq j\end{cases}.\]
	Equivalently, $\vece_i\perp\vece_j$ if $i\neq j$ and $\|\vece_i\|=\sqrt{\vece_i\cdot\vece_i}=1$.
	\begin{ext}
		Let $\vecv_1,\cdots,\vecv_k$	 be vectors in $\R^n$, we say $\vecv_1,\cdots,\vecv_k$	 are orthogonal if $\vecv_i\cdot\vecv_j=\begin{cases}1,\quad i=j\\0,\quad i\neq j\end{cases}$.
	\end{ext}
\end{thm}
\begin{thm}
	A matrix $\matrixA$ is orthogonal \emph{iff} its columns are an orthogonal set of vectors. \\
	\begin{prf}
		Suppose $\matrixA=\begin{bmatrix}|&&|\\\vecu_1&\cdots&\vecu_n\\|&&|\end{bmatrix},$ in which $\vecu_1,\cdots,\vecu_n$ are orthogonal. \\
		Let $\vecx=\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}$ and $\vecy=\begin{bmatrix}y_1\\\vdots\\y_n\end{bmatrix}$.
		$$\begin{aligned}
			\therefore\matrixA(\vecx)\cdot\matrixA(\vecy)&=\matrixA(x_1\vece_1+\cdots+x_n\vece_n)\cdot\matrixA(y_1\vece_1+\cdots+y_n\vece_n)\\
			&=(x_1\vecu_1+\cdots+x_n\vecu_n)\cdot(y_1\vecu_1+\cdots+y_n\vecu_n)\\
			&=\sum_{1\leq i,j\leq n}(x_i\vecu_i)\cdot (y_j\vecu_j)\\
			&=\sum_{1\leq i,j\leq n}x_iy_j(\vecu_i\cdot\vecu_j)\\
			&=\sum_{1\leq i,j\leq n}x_iy_j\qquad\left[\vecu_i\cdot\vecu_j=\begin{cases}1,\quad i=j\\0,\quad i\neq j\end{cases}\right]\\
			&=\vecx\cdot\vecy.
		\end{aligned}$$
	\end{prf}
	\begin{eg}
		Consider $\matrixA=\begin{bmatrix}\frac{2}{3}&-\frac{2}{3}&\frac{1}{3}\\\frac{1}{3}&\frac{2}{3}&\frac{2}{3}\\\frac{2}{3}&\frac{1}{3}&-\frac{2}{3}\end{bmatrix}$. Is $\matrixA$ orthogonal?\\
		\begin{sol}
			$$\begin{aligned}
				\vecv_1\cdot\vecv_1&=\left(\frac{2}{3}\right)^2+\left(\frac{1}{3}\right)^2+\left(\frac{2}{3}\right)^2=1	\\
				\vecv_1\cdot_vecv_2&=\left(\frac{2}{3}\right)\left(-\frac{2}{3}\right)+\left(\frac{1}{3}\right)\left(\frac{2}{3}\right)+\left(\frac{1}{3}\right)\left(\frac{2}{3}\right)=0\\
				\vecv_1\cdot_vecv_3&=\left(\frac{2}{3}\right)\left(\frac{1}{3}\right)+\left(\frac{1}{3}\right)\left(\frac{2}{3}\right)+\left(\frac{2}{3}\right)\left(-\frac{2}{3}\right)=0\\
				\vecv_2\cdot\vecv_2&=\left(-\frac{2}{3}\right)^2+\left(\frac{2}{3}\right)^2+\left(\frac{1}{3}\right)^2=1	\\
				\vecv_2\cdot_vecv_3&=\left(-\frac{2}{3}\right)\left(\frac{1}{3}\right)+\left(\frac{2}{3}\right)\left(\frac{2}{3}\right)+\left(\frac{1}{3}\right)\left(-\frac{2}{3}\right)=0\\
				\vecv_3\cdot\vecv_3&=\left(\frac{1}{3}\right)^2+\left(\frac{2}{3}\right)^2+\left(-\frac{2}{3}\right)^2=1	\\
			\end{aligned}$$
			\[\therefore\matrixA\text{ is orthogonal.}\]
		\end{sol}
	\end{eg}
\end{thm}
\begin{thm}
	To compute lots of dot products, we can encode them as a matrix product: 
	\[\begin{bmatrix}-&\vecu_1&-\\&\vdots&\\-&\vecu_n&-\end{bmatrix}\begin{bmatrix}|&&|\\\vecu_1&\cdots&\vecu_n\\|&&|\end{bmatrix}=\begin{bmatrix}\vecu_1\cdot\vecu_1&\cdots&\vecu_n\cdot\vecu_1\\\vdots&\ddots&\vdots\\\vecu_1\cdot\vecu_n&\cdots&\vecu_n\cdot\vecu_n\end{bmatrix}\]
	\begin{ext}
		An $n\times n$ matrix $\matrixA$ is orthogonal \emph{iff} $\matrixA^\T\matrixA=\matrixI$. Consequently, all orthogonal matrices are invertible, and $\matrixA^{-1}=\matrixA^\T.$
	\end{ext}
\end{thm}
\begin{thm}
	\[(\matrixA\matrixB)^\T=\matrixB^\T\cdot\matrixA^\T.\]	
	\begin{eg}
		Consider $\matrixA=\begin{bmatrix}1&1&1\end{bmatrix}$ and $\matrixB=\begin{bmatrix}1\\2\\3\end{bmatrix}.$
		\[(\matrixA\matrixB)^\T=\left(\begin{bmatrix}1&1&1\end{bmatrix}\begin{bmatrix}1\\2\\3\end{bmatrix}\right)^\T=\begin{bmatrix}6\end{bmatrix}^\T=\begin{bmatrix}6\end{bmatrix},\quad\matrixB^\T\matrixA^\T=\begin{bmatrix}1&2&3\end{bmatrix}\begin{bmatrix}1\\1\\1\end{bmatrix}=\begin{bmatrix}6\end{bmatrix}.\]	
		\[\therefore(\matrixA\matrixB)^\T=\matrixB^\T\matrixA^\T.\]
	\end{eg}
	\begin{prf}
		Suppose $\matrixA=\begin{bmatrix}-&\veca_1&-\\&\vdots&\\-&\veca_n&-\end{bmatrix}$ and $\matrixB=\begin{bmatrix}|&&|\\\vecb_1&\cdots&\vecb_m\\|&&|\end{bmatrix}$
		\[\matrixA\matrixB=\begin{bmatrix}-&\veca_1&-\\&\vdots&\\-&\veca_n&-\end{bmatrix}\begin{bmatrix}|&&|\\\vecb_1&\cdots&\vecb_m\\|&&|\end{bmatrix}=\begin{bmatrix}\vecb_1\cdot\veca_1&\cdots&\vecb_m\cdot\veca_1\\\vdots&\ddots&\vdots\\\vecb_1\cdot\veca_n&\cdots&\vecb_m\cdot\veca_n\end{bmatrix}\]
		\[\matrixB^\T\matrixA^\T=\begin{bmatrix}-&\vecb_1&-\\&\vdots&\\-&\vecb_m&-\end{bmatrix}\begin{bmatrix}|&&|\\\veca_1&\cdots&\veca_n\\|&&|\end{bmatrix}=\begin{bmatrix}\veca_1\cdot\vecb_1&\cdots&\veca_n\cdot\vecb_1\\\vdots&\ddots&\vdots\\\veca_1\cdot\vecb_m&\cdots&\veca_n\cdot\vecb_m\end{bmatrix}\]
		\[\therefore(\matrixA\matrixB)^\T=\matrixB^\T\cdot\matrixA^\T.\]
	\end{prf}
\end{thm}
\begin{thm}
	Properties of orthogonal matrices:
	\begin{enumerate}
		\item The inverse $\matrixA^{-1}=\matrixA^\T$ of an orthogonal matrix $\matrixA$ is orthogonal. 
		\item The product $\matrixA\matrixB$ of orthogonal matrices is orthogonal. 	
	\end{enumerate}
	Consequences: 
	\begin{itemize}
		\item $\matrixA$ is orthogonal $\Longleftrightarrow$ columns of $\matrixA$ are an orthogonal basis.
		\item $\matrixA^\T$ is orthogonal $\Longleftrightarrow$ rows of $\matrixA$ are an orthogonal basis. 
	\end{itemize}
	\begin{prf}
		We know if $\matrixA$ is orthogonal, then $\matrixA^\T\matrixA=\matrixI.$
		\begin{enumerate}
			\item To show $\matrixA^\T$ is orthogonal, we need to show $(\matrixA^\T)^\T\matrixA^\T=\matrixI.$ \[(\matrixA^\T)^\T=\matrixA\Rightarrow(\matrixA^\T)^\T\matrixA^\T=\matrixA\matrixA^\T=\matrixA\matrixA^{-1}=\matrixI.\]
			\item To show $\matrixA\matrixB$ is orthogonal, we need to show $(\matrixA\matrixB)^\T(\matrixA\matrixB)=\matrixI$ \[(\matrixA\matrixB)^\T=\matrixB^\T\matrixA^\T\Rightarrow(\matrixA\matrixB)^\T(\matrixA\matrixB)=\matrixB^\T\matrixA^\T(\matrixA\matrixB)=\matrixB^\T(\matrixA^\T\matrixA)\matrixB=\matrixB^\T\matrixI\matrixB=\matrixB^\T\matrixB=\matrixI.\]
		\end{enumerate}
	\end{prf}
\end{thm}


\subsection{Gram-Schmidt Process, QR Factorization}
\begin{rmk}[Orthogonal Coordinate System]In general, a vector cannot be represented by summation of its projects, but when we have orthogonal ones, we can.\end{rmk}
\begin{thm}
	Let $V=\Span\qty(\vecu_1,\cdots,\vecu_k)$ and $\vecx\in V,$ then there exists unique scalars $c_1,\cdots,c_k$ such that $\vecx=c_1\vecu_1+c_2\vecu_2+\cdots+c_k\vecu_k.$ The constants $c_1,\cdots,c_k$ equal: \[c_i=\vecx\cdot\vecu_i.\]
	\begin{prf}
		Since $\vecx\in V,$ $\vecx=c_1\vecu_!+\cdots+c_k\vecu_k$ for some constants.\[\begin{aligned}\therefore \vecu_i\cdot\vecx&=\vecu_i\cdot\qty(c_1\vecu_1+\cdots+c_k\vecu_k)\\&=c_1\qty(\vecu_i\cdot\vecu_1)+\cdots+c_k\qty(\vecu_i\cdot\vecu_k)\end{aligned}\]\par Since $\vecu_1,\cdots,\vecu_k$ are orthogonal, \[\vecu_i\cdot\vecu_j=\begin{cases}0,\qquad i\neq j\\1,\qquad i=j\end{cases}\]\[\therefore\vecu_i\cdot\vecx=c_i\qty(\vecu_i\cdot\vecu_i)=c_i.\]
	\end{prf}
\end{thm}
\begin{thm}
	Let $u_1,\cdots,\vecu_k$ be orthogonal vectors and $V=\Span\qty(u_1,\cdots,\vecu_k.)$ The projection of $\vecx\in\R^n$ to $V$ is given by \[\Proj_V\qty(\vecx)=\qty(\vecu_1\cdot\vecx)\vecu_1+\cdots+\qty(\vecu_k\cdot\vecx)\vecu_k\]\par In particular, projections into a line spanned by $\vecu$ is given by \[\Proj_L\qty(\vecx)=\qty(\vecu\cdot\vecx)\vecu.\]
	\begin{prf}
		Write $\vecx=\vecx^\parallel+\vecx^\perp$ such that $\vecx^\parallel\in V$ and $\vecx^\perp\in V^\perp,$ and $\vecx^\parallel=c_1\vecu_1+\cdots+c_k\vecu_k.$\par Note that $\vecu_i\cdot\vecx=\vecu_i\cdot\vecx^\parallel+\vecu_i\cdot\vecx^\perp.$\par Since $\vecu_i\in V,\ \vecx^\perp\in V^\perp,$ and $\vecu_i\cdot\vecx^\perp=0,$ \[\begin{aligned}\vecu_i\cdot\vecx&=\vecu_i\cdot\vecx^\parallel\\&=\vecu_i\qty(c_1\vecu_1+\cdots+c_k\vecu_k)\\&=c_1\qty(\vecu_i\cdot\vecu_1)+\cdots+c_k\qty(\vecu_i\cdot\vecu_k)\\&=c_i\qty(\vecu_i\cdot\vecu_i)\\&=c_i.\end{aligned}\]
	\end{prf}	
\end{thm}
\begin{framed}
\textbf{The Gram-Schmidt Process:}
\begin{enumerate}
	\item Input: $V\subseteq\R^n$ is a subspace with basis $\vecv_1,\cdots,\vecv_k.$
	\item Output: $\vecu_1,\cdots,\vecu_k$ are orthogonal and span $V.$
	\item Procedure: 
	\begin{enumerate}
		\item $\vecu_1=\dfrac{\vecv_1}{\|\vecv_1\|}$
		\item $\vecv_k^\perp=\vecv_k-\vecv_k^\parallel$ relative to $V_{k-1}=\Span\qty(\vecv_1,\cdots,\vecv_{k-1})$
		\item $\vecu_k^\perp=\dfrac{\vecv_k^\perp}{\|\vecv_k^\perp\|}$
		\item Compute the last $\vecv_i^\perp$: \par $\begin{aligned}\vecv_i^\perp&=\vecv_i-\vecv_i^\parallel\\&=\vecv_i-\qty(\vecu_1\cdot\vecv_i)\vecu_1-\cdots-\qty(\vecu_{i-1}\cdot\vecv_i)\vecu_{i-1}\\&=\vecv_i-\sum_{j=1}^{i-1}\qty(\vecu_j\cdot\vecv_i)\vecu_j\end{aligned}$
	\end{enumerate}
\end{enumerate}
\end{framed}
\begin{eg}
	Consider $V=\Span\qty(\begin{bmatrix}1\\1\\1\\1\end{bmatrix},\begin{bmatrix}1\\0\\0\\1\end{bmatrix}, \begin{bmatrix}0\\2\\1\\-1\end{bmatrix}).$ Apply the Gram-Schmidt process to these vectors to find a set of vectors that are orthogonal and span $V$. 
	\begin{sol}
	\begin{enumerate}
		\item $\vecu_1=\dfrac{\vecv_1}{\|\vecv_1\|}$\par Since $\|\vecv_1\|=\sqrt{1^2+1^2+1^2+1^2}=2,\ \vecu_1=\begin{bmatrix}^1/_2\\^1/_2\\^1/_2\\^1/_2\end{bmatrix}$
		\item Find $\vecv_2^\perp$ and $\vecu_2$ \[\begin{aligned}\vecv_2^\perp&=\vecv_2-\vecv_2^\parallel\\&=\vecv_2-\qty(\vecu_1\cdot\vecv_2)\vecu_1\\\vecu_1\cdot\vecv_2&\dfrac{1}{2}+\dfrac{1}{2}=1\\\therefore\vecv_2^\perp&=\begin{bmatrix}1\\0\\0\\1\end{bmatrix}-\begin{bmatrix}^1/_2\\^1/_2\\^1/_2\\^1/_2\end{bmatrix}=\begin{bmatrix}^1/_2\\-^1/_2\\-^1/_2\\^1/_2\end{bmatrix}\\\vecu_2&=\dfrac{\vecv_2^\perp}{\|\vecv_2^\perp\|}=\begin{bmatrix}^1/_2\\-^1/_2\\-^1/_2\\^1/_2\end{bmatrix}\end{aligned}\]
		\item Find $\vecv_3^\perp$ and $\vecu_3^\perp$\[\begin{aligned}\vecv_3^\perp&=\vecv_3-\vecv_3^\parallel\\&=\vecv_3-\qty(\vecu_1\cdot\vecv_3)\vecu_1-\qty(\vecu_2\cdot\vecv_3)\vecu_2\\\begin{bmatrix}\vecu_1\cdot\vecv_3\\\vecu_2\cdot\vecv_3\end{bmatrix}&=\begin{bmatrix}\dfrac{1}{2}&\dfrac{1}{2}&\dfrac{1}{2}&\dfrac{1}{2}\\\\\dfrac{1}{2}&-\dfrac{1}{2}&-\dfrac{1}{2}&\dfrac{1}{2}\end{bmatrix}\begin{bmatrix}0\\2\\1\\-1\end{bmatrix}=\begin{bmatrix}1\\-2\end{bmatrix}\\\therefore\vecv_3^\perp&=\begin{bmatrix}0\\2\\1\\-1\end{bmatrix}-1\cdot\begin{bmatrix}^1/_2\\^1/_2\\^1/_2\\^1/_2\end{bmatrix}-\qty(-2)\cdot\begin{bmatrix}^1/_2\\-^1/_2\\-^1/_2\\^1/_2\end{bmatrix}\\&=\begin{bmatrix}0\\2\\1\\-1\end{bmatrix}-\begin{bmatrix}^1/_2\\^1/_2\\^1/_2\\^1/_2\end{bmatrix}-\begin{bmatrix}-1\\1\\1\\-1\end{bmatrix}=\begin{bmatrix}^1/_2\\^1/_2\\-^1/_2\\-^1/_2\end{bmatrix}\\\therefore\vecu_3&=\dfrac{\vecv_3^\perp}{\|\vecv_3^\perp\|}=\begin{bmatrix}^1/_2\\^1/_2\\-^1/_2\\-^1/_2\end{bmatrix}\end{aligned}\]
	\end{enumerate}\par Therefore, $\begin{bmatrix}^1/_2\\^1/_2\\^1/_2\\^1/_2\end{bmatrix}, \begin{bmatrix}^1/_2\\-^1/_2\\-^1/_2\\^1/_2\end{bmatrix}, \begin{bmatrix}^1/_2\\^1/_2\\-^1/_2\\-^1/_2\end{bmatrix}$ are orthogonal and span $V$.
	\end{sol}
\end{eg}
\begin{thm}[QR-Decomposition]
	Let $\matrixA=\mqty[{|&&|\\\vecv_1&\cdots&\vecv_k\\|&&|}]$ be a matrix and assume $\matrixA$ has linearly independent columns. Then, \[\matrixA=\vb Q\vb R,\] where \[\vb Q=\mqty[{|&&|\\\vecu_1&\cdots&\vecu_k\\|&&|}]\] has orthogonal columns and \[\vb R=\mqty[{\vecu_1\cdot\vecv_1&\vecu_1\cdot\vecv_2&\cdots&\vecu_1\cdot\vecv_k\\0&0&\vdots\\\vdots&\ddots&\ddots&\vdots\\0&\cdots&0&\vecu_k\cdot\vecv_k}]\]	is upper triangular. In particular, if $\matrixA$ is a square, invertible matrix, $\vb A=\vb Q\vb R,$ where $\vb Q$ is orthogonal and $\vb R$ is upper triangular. 
	\begin{prf}
		Run G.S. process in $V=\Im(A)$ with basis $\vecv_1,\cdots,\vecv_k,$ and we get $\vecu_1,\cdots,\vecu_k$ that are orthogonal and span $V$. \[\vecv_i=\sum_{j=1}^i\qty(\vecv_i\cdot\vecu_j)\vecu_j.\]\[\therefore\mqty[{|&&|\\\vecu_1&\cdots&\vecu_k\\|&&|}]\mqty[{\vecu_1\cdot\vecv_1&\vecu_1\cdot\vecv_2&\cdots&\vecu_1\cdot\vecv_k\\0&\vecu_2\cdot\vecv_2&\cdots&\vdots\\\vdots&\vdots&\ddots&\vdots\\0&0&\cdots&\vecu_k\cdot\vecv_k}]=\mqty[{|&&|\\\vecv_1&\cdots&\vecv_k\\|&&|}].\]
	\end{prf}
\end{thm}
\begin{framed}
	\textbf{Find G.S. Process and QR factorization via row reduction.}
	\begin{enumerate}
		\item General Idea: \par Input: A matrix $\vb A$ with linearly independent columns\par Output: a factorization $\vb A=\vb Q\vb R,$ where $\vb Q$ has orthogonal columns and $\vb R$ is upper triangular.
		\item General Procedure: 
		\begin{enumerate}
			\item Compute $\matrixA^\T\matrixA$ and form the augmented matrix $\mqty[{\matrixA^\T\matrixA&\vdots&\matrixA^\T}].$ 
			\item Row reduce the left hand side until an upper triangular only by subtracting multiples of rows from rows below them. At the conclusion of this step, left hand side is upper triangular.
			\item Divide each row by the square root of the leading diagonal entry.
			\item The final output is $\mqty[\vb R&\vdots&\vb Q^\T].$
		\end{enumerate}
	\end{enumerate}	
\end{framed}
\begin{eg}
	Find the QR factorization of $\matrixA=\mqty[{1&1&1\\0&1&2\\1&0&3}].$
	\begin{sol}
		\begin{enumerate}
			\item Compute $\matrixA^\T\matrixA.$\[\matrixA^\T\matrixA=\mqty[{1&0&1\\1&1&0\\1&2&3}]\mqty[{1&1&1\\0&1&2\\1&1&3}]=\mqty[{2&1&4\\1&2&3\\4&3&14}].\]
			\item Row reduce $\mqty[{\matrixA^\T\matrixA&\vdots&\matrixA^\T}].$ \[\left[\begin{array}{c:c}\mqty{2&1&4\\1&2&3\\4&3&14}&\mqty{1&0&1\\1&1&0\\1&2&3}\end{array}\right]\xrightarrow[\text{Reduction}]{\text{Row}}\left[\begin{array}{c:c}\mqty{\sqrt{2}&1/\sqrt{2}&4/\sqrt{2}\\0&\sqrt{3/2}&\sqrt{2/3}\\0&0&2/\sqrt{3}}&\mqty{1/\sqrt{2}&0&1/\sqrt{2}\\^1/_2\sqrt{2/3}&\sqrt{2/3}&-^1/_2\sqrt{2/3}\\-^1/_2\sqrt{3}/3&^1/_2\sqrt{3}/3&^1/_2\sqrt{3}/3}\end{array}\right]\]
			\item So, \[\vb R=\mqty[{\sqrt{2}&1/\sqrt{2}&4/\sqrt{2}\\0&\sqrt{3/2}&\sqrt{2/3}\\0&0&2/\sqrt{3}}]\]\[\vb Q=\mqty[{1/\sqrt{2}&0&1/\sqrt{2}\\^1/_2\sqrt{2/3}&\sqrt{2/3}&-^1/_2\sqrt{2/3}\\-^1/_2\sqrt{3}/3&^1/_2\sqrt{3}/3&^1/_2\sqrt{3}/3}]\]
		\end{enumerate}
	\end{sol}
\end{eg}


\newpage
\section{Determinant}
\subsection{The Definition of the Determinant}
\begin{rmk} Dot products encode lengths and angles of vectors. Determinant encodes volume and orientations of subspaces. \end{rmk}
\begin{df}[Determinant]
	Let $\matrixA=\mqty[{a&b\\c&d}]$ be a $2\times2$ matrix, the \textbf{determinant} of $\matrixA$ is the quantity \[\det(\matrixA)=ad-bc.\]	
\end{df}
\begin{thm}
	The image of the unit square under $\matrixA$ is $\qty|\det(\matrixA)|.$	
\end{thm}
\begin{thm}
	A matrix $\matrixA$ is invertible $\iff$ $\det(\matrixA)\neq0.$ 	
\end{thm}
\begin{prf}
	$\matrixA$ is invertible $\implies\rank(\matrixA)=2,$	i.e., $\vecv_1$ and $\vecv_2$ (columns of $\matrixA$) are not co-linear. $\implies$ The area of the parallelogram spanned by $\vecv_1$ and $\vecv_2$ does not have an area of $0$.
\end{prf}
\begin{thm}
	Let $\matrixA=\mqty[{a&b\\c&d}]$ be a $2\times2$ matrix. The sign of the determinant of $\matrixA$ satisfies \[\text{sign}(\det(\matrixA))=\begin{cases}0&\text{if }\vecv_1\text{ and }\vecv_2\text{ are colinear}\\+&\text{if }\vecv_2^\perp\text{ is a positive multiple of }\vecv_1^\text{rot}=\mqty[{-c\\a}]\\\\-&\text{if }\vecv_2^\perp\text{ is a negative multiple of }\vecv_1^\text{rot}=\mqty[{-c\\a}]\end{cases}.\]
	\begin{prf}
		Consider the projection of $\vecv_2$ into the line spanned by $\vecv_1^\text{rot}=\mqty[{-c\\a}].$ \[\begin{aligned}\vecv_2^\perp=\dfrac{\vecv_1^\text{rot}\cdot\vecv_2}{\vecv_1^\text{rot}\cdot\vecv_1^\text{rot}}\cdot\vecv_1^\text{rot}&=\dfrac{\mqty[{-c\\a}]\cdot\mqty[{b\\d}]}{\mqty[{-c\\a}]\cdot\mqty[{-c\\a}]}\cdot\mqty[{-c\\a}]\\&=\dfrac{\det(\matrixA)}{a^2+c^2}\cdot\vecv_1^\text{rot}\end{aligned}\] \[\therefore\det(\matrixA)>0\implies\vecv_2^\perp>0\]\[\quad\det(\matrixA)<0\implies\vecv_2^\perp<0.\]
	\end{prf}
\end{thm}
\begin{rmk}
	The sign of the determinant describes the orientation of $\vecv_1$ and $\vecv_2.$	
\end{rmk}
\begin{df}[Parallelogram]
	A parallelogram is defined by the set $\qty{c_1\vecv_1+c_2\vecv_2\mid0\leq c_2\leq1}$
	\begin{center}
	\tikzset{every picture/.style={line width=0.75pt}} 
	\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
	\draw    (100,270) -- (237.64,270) ;
	\draw [shift={(239.64,270)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (100,270) -- (159.23,210.77) ;
	\draw [shift={(160.64,209.36)}, rotate = 135] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (239.64,270) -- (300.28,209.36) ;
	\draw    (160.64,209.36) -- (300.28,209.36) ;
	\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.32 ] (159.94,209.27) -- (299.64,209.27) -- (239.7,270) -- (100,270) -- cycle ;
	\draw    (102.18,270) -- (167.82,270) ;
	\draw [shift={(169.82,270)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (100,270) -- (128.91,241.09) ;
	\draw [shift={(130.32,239.68)}, rotate = 135] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw (143,184.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{v}_{1}$};
	\draw (241.64,273.4) node [anchor=north west][inner sep=0.75pt]    {$\vec{v}_{2}$};
	\draw (96,215.4) node [anchor=north west][inner sep=0.75pt]    {$c_{1}\vec{v}_{1}$};
	\draw (171.82,273.4) node [anchor=north west][inner sep=0.75pt]    {$c_{2}\vec{v}_{2}$};
	\end{tikzpicture}
	\end{center}	
\end{df}
\begin{ext}[K-Parallelepiped] Let $\vecv_1,\cdots,\vecv_k\in\R^n.$ The \textbf{k-parallelepiped} spanned by $\vecv_1,\cdots,\vecv_k$ is the set \[\qty{c_1\vecv_1+c_2\vecv_2+\cdots+c_k\vecv_k\mid c_i\in\qty[0,1]}.\]\end{ext}
\begin{ext}[Unit Cube/n-parallelepiped] The \textbf{unit cube} is $\R^n$ is the \textbf{n-parallelepiped} spanned by $\vece_1,\cdots\vecv_n$\[\qty{c_1\vece_1+\cdots+c_n\vece_n\mid c_i\in\qty[0,1]}.\]\end{ext}
\begin{thm}
	Let $\matrixA$ be a linear transformation, then $\matrixA$ maps parallelepipeds to parallelepipeds. The image of the unit cube under $\matrixA$ is the parallelepipeds spanned by the columns of $\matrixA.$	
\end{thm}
\begin{thm}[Volume]
	The \textbf{volume} of a k-parallelepiped spanned by $\vecv_1,\cdots.\vecv_k$ is \[\vol\qty(\vecv_1,\cdots,\vecv_k)=\vol\qty(\vecv_1,\cdots,\vecv_{k-1})\|\vecv_k^\perp\|,\] where the $\vecv_k^\perp$ is the perpendicular part of $\vecv_k$ in the decomposition $\vecv_k=\vecv_k^\parallel+\vecv_k^\perp,$ where $\vecv_k^\parallel\in\Span\qty(\vecv_1,\cdots,\vecv_{k-1}),$ and $\vecv_k^\perp$ is perpendicular. 
\end{thm}
\begin{thm}
	The volume of the k-parallelepiped spanned by $\vecv_1,\cdots,\vecv_k$ equals \[\vol\qty(\vecv_1,\cdots,\vecv_k)=\|\vecv_1\|\cdot\|\vecv_2^\perp\|\cdot\|\vecv_3^\perp\|\cdots\|\vecv_k^\perp\|,\]	where $\vecv_i^\perp$ is the perpendicular part of $\vecv_i$ with respect to $V=\Span\qty(\vecv_1,\cdots,\vecv_{i-1}).$
\end{thm}
\begin{eg}
	Let $\vecv_1=\mqty[{7\\0\\0}],\vecv_2=\mqty[{1\\1\\1}],\vecv_3=\mqty[{2\\1\\1}]$. Find the volume of the k-parallelepiped spanned by $\vecv_1,\ \vecv_2,\ \vecv_3.$
	\begin{sol}
		\[\vol\qty(\vecv_1,\vecv_2,\vecv_3)=\|\vecv_1\|\cdot\|\vecv_2^\perp\|\cdot\|\vecv_3^\perp\|\]
		\begin{enumerate}
			\item Since $\|\vecv_1\|=7,$ \[\vecu_1=\dfrac{\vecv_1}{\|\vecv_1\|}=\dfrac{1}{7}\mqty[{7\\0\\0}]=\vece_1\]
			\item \[\begin{aligned}\vecv_2^\perp&=\vecv_2-\vecv_2^\parallel\\&=\vecv_2-\qty(\vecv_2\cdot\vecu_1)\vecu_1\\&=\vecv_2-\qty(\vece_1\cdot\vecv_2)\vece_1\\&=\mqty[{1\\1\\1}]-\mqty[{1\\0\\0}]=\mqty[{0\\1\\1}]\\\|\vecv_2^\perp\|&=\sqrt{2}\\\vecu_2&=\dfrac{1}{\sqrt{2}}\mqty[0\\1\\1]\end{aligned}\]
			\item \[\begin{aligned}\vecv_3^\perp&=\vecv_3-\vecv_3^\parallel\\&=\vecv_3-\qty(\vecv_3\cdot\vece_1)\vece_1-\qty(\vecv_3\cdot\vecu_2)\vecu_2\\&=\mqty[{2\\1\\1}]-\mqty[{2\\0\\0}]-\mqty[{0\\1\\1}]=\mqty[{0\\0\\0}]\\\|\vecv_3^\perp\|&=0\longrightarrow\vecv_3\in\Span\qty(\vecv_1,\vecv_2)\end{aligned}\]
		\end{enumerate}
		\[\therefore\vol\qty(\vecv_1,\vecv_2,\vecv_3)=0\]
	\end{sol}
\end{eg}
\begin{thm}
	Let $\matrixA$ be an $n\times n$ matrix, then $\matrixA$ is invertible $\iff$ the volume of the parallelepiped spanned by the columns of $\matrixA$ is not $0.$
	\begin{prf}
		\[\begin{aligned}\matrixA\text{ is invertible }&\iff\ \rank(\matrixA)=n\\&\Rightarrow\vecv_i\notin\Span(\vecv_1,\cdots\vecv_{i-1})\\&\Rightarrow\|\vecv_i^\perp\|\neq0\\&\Rightarrow\vol\qty(\vecv_1,\cdots,\vecv_k)\neq0,		
		\end{aligned}\]\par $\vecv_1,\cdots,\vecv_k$ are columns of $\matrixA.$
	\end{prf}
\end{thm}
\begin{df}[Formal Definition of Determinant]
	There is a unique function from the set of $n\times n$ matrices to real numbers called the \textbf{determinant} and denoted as \[\det:\ \qty{n\times n\text{ matrices}}\to\R\] satisfying the following conditions: 
	\begin{enumerate}
		\item $\qty|\det(\matrixA)|=$volume of the parallelepiped spanned by the columns of $\matrixA.$
		\item 
		\begin{enumerate}
			\item $\det(\matrixI)=1.$
			\begin{eg}$\det\qty(\mqty[{1&0\\0&1}])=1\time1-0=1.$\end{eg}
			\item The determinant is a linear function in each column of $\matrixA$: \[\det\qty(\mqty[{|&&|&|\\\vecv_1&\cdots&\vecv_{n-1}&\vecv_n+k\vecv_n'\\|&&|&|}])=\det\qty(\mqty[{|&&|\\\vecv_1&\cdots&\vecv_n\\|&&|}])+k\det\qty(\mqty[|&&|\\\vecv_1&\cdots&\vecv_n'\\|&&|])\]
			\begin{eg}\[\det\qty(\mqty[{7&c\\3&d}])=7d-3c\text{ is a linear function}.\]\end{eg}
		\end{enumerate}
	\end{enumerate}
\end{df}

\subsection{Computing the Determinant}

\begin{thm}[Computing the Determinant via Row Reduction] 
	Elementary row operations change the determinant in prescribed ways. 
	\begin{enumerate}
		\item Switch rows of a matrix, the determinant changes the sign.
		\begin{prf}
			Wants to show: $\det\qty(\mqty[{&|&&|&\\\cdots&\vecv_i&\cdots&\vecv_j&\cdots\\&|&&|&}])=-\det\qty(\mqty[{&|&&|&\\\cdots&\vecv_j&\cdots&\vecv_i&\cdots\\&|&&|&}]).$\par Consider: $\det\qty(\mqty[&|&&|&\\\cdots&\vecv_i+\vecv_j&\cdots&\vecv_i+\vecv_j&\cdots\\&|&&|&])=0$ because it has repeated columns: \[\begin{aligned}&\det\qty(\mqty[&|&&|&\\\cdots&\vecv_i+\vecv_j&\cdots&\vecv_i+\vecv_j&\cdots\\&|&&|&])\\&=\det\qty(\mqty[&|&&|&\\\cdots&\vecv_i&\cdots&\vecv_i+\vecv_j&\cdots\\&|&&|&])+\det\qty(\mqty[&|&&|&\\\cdots&\vecv_j&\cdots&\vecv_i+\vecv_j&\cdots\\&|&&|&])\\&=\underbrace{\det\qty(\mqty[&|&&|&\\\cdots&\vecv_i&\cdots&\vecv_i&\cdots\\&|&&|&])}_{0}+\det\qty(\mqty[&|&&|&\\\cdots&\vecv_i&\cdots&\vecv_j&\cdots\\&|&&|&])\\&+\det\qty(\mqty[&|&&|&\\\cdots&\vecv_j&\cdots&\vecv_i&\cdots\\&|&&|&])+\overbrace{\det\qty(\mqty[&|&&|&\\\cdots&\vecv_j&\cdots&\vecv_j&\cdots\\&|&&|&])}^{0}=0\end{aligned}\]\[\therefore\det\qty(\mqty[&|&&|&\\\cdots&\vecv_i&\cdots&\vecv_j&\cdots\\&|&&|&])+\det\qty(\mqty[&|&&|&\\\cdots&\vecv_j&\cdots&\vecv_i&\cdots\\&|&&|&])=0\]\[\therefore\det\qty(\mqty[&|&&|&\\\cdots&\vecv_i&\cdots&\vecv_j&\cdots\\&|&&|&])=-\det\qty(\mqty[&|&&|&\\\cdots&\vecv_j&\cdots&\vecv_i&\cdots\\&|&&|&]).\]
		\end{prf}
		\item Adding a multiple of $j^\text{th}$ row to $i^\text{th}$ row with $i\neq j,$ the determinant stays constant. 
		\begin{prf}
			\[\det\qty(\mqty[{&|&\\\cdots&\vecv_i+k\vecv_j&\cdots\\&|&}])=\det\qty(\mqty[{&|&\\\cdots&\vecv_i&\cdots\\&|&}])+k\cdot\det\qty(\mqty[{&|&\\\cdots&\vecv_j&\cdots\\&|&}])\]\par Note that $\det\qty(\mqty[{&|&\\\cdots&\vecv_j&\cdots\\&|&}])=0$ because it has $\vecv_j$ at both the $i^\text{th}$ and $j^\text{th}$ column, and thus the columns are not linearly independent. \[\therefore\det\qty(\mqty[{&|&\\\cdots&\vecv_i+k\vecv_j&\cdots\\&|&}])=\det\qty(\mqty[{&|&\\\cdots&\vecv_i&\cdots\\&|&}])\]
		\end{prf}
		\item Scale a row by $k\neq0,$ the determinant scales by $k$.
		\begin{prf}
			Note that determinant is a linear function in each column of $\matrixA.$
		\end{prf}
	\end{enumerate}
\end{thm}
\begin{eg}
	Compute the determinant of $\matrixA=\mqty[{2&1&1\\1&2&1\\1&1&2}]$
	\begin{sol}
		Row reduce $\matrixA,$ keeping track of how the determinant changes.\par  Note that $\det(\matrixI)=1.$	
		\[\mqty[{2&1&1\\1&2&1\\1&1&2}]\xrightarrow[\text{Reduction}]{\text{Row}}\mqty[{1&0&0\\0&1&0\\0&0&1}]\]\par In this process of row reduction, we know $\dfrac{D}{4}=\det(\matrixI)=1,$ so $D=4.$
	\end{sol}
\end{eg}
\begin{thm}
	Let $\matrixA=\mqty[{a_{11}&*&\cdots&*\\0&a_{22}&&\vdots\\\vdots&&\ddots&\vdots\\0&\cdots&\cdots&a_{nn}}]$ be an upper triangular matrix. The determinant of $\matrixA$ is $a_{11}, a_{22},\cdots,a_{nn},$ the product of the diagonal entries. 
\end{thm}
\begin{prf}
	$\boxed{\text{Case }1}$ All $a_{ii}\neq0.$\par Row reduce $\matrixA$ to compute $\det(\matrixA)$ by dividing each row by $a_{ii}$ to get the identity matrix $\matrixI.$\par So, $\dfrac{1}{a_{11}\cdots a_{nn}}D=\det(\matrixI)=1,$ amd we get $D=a_{11}\cdots a_{nn}$\par 
	$\boxed{\text{Case }2}$ Some $a_{ii}=0.\Rightarrow$ Show $\det(\matrixA)=a_{11}\cdots a_{nn}=0\Rightarrow$ Show $\matrixA$ is not invertible.\par Look at the first $a_{ii}=0,$ we know the $i^\text{th}$ column in row reduction does not contain a pivot.\par $\Rightarrow\matrixA$ is not invertible. 
 \end{prf}
\begin{framed}
\textbf{Computing the Determinant}
\begin{enumerate}
	\item Input: $n\times n$ matrix $\matrixA$
	\item Output: $\det(\matrixA)$
	\item Procedure: Row reduce $\matrixA,$ keeping track of the elementary row operations until an upper triangular matrix is obtained. 
\end{enumerate}	
	Let $a_{11},\cdots,a_{nn}$ be the diagonal entires of this matrix, $k_1,\cdots,k_m$ be the constants multiplied by in row reduction, and $s$ be the number of switches: \[\begin{aligned}(-1)^s(k_1\cdots k_m)\det(\matrixA)&=a_{11}\cdots a_{nn}\\\det(\matrixA)&=\dfrac{a_{11}\cdots a_{nn}}{(k_1\cdots k_m)}(-1)^s\end{aligned}\]
\end{framed}
\begin{thm}
	\[\det(\matrixA)=\det(\matrixA^\T)\]	
\end{thm}
\begin{thm}[Computing Determinant via the Laplace Expansion] 
	To find the formula for an $n\times n$ matrix determinant in terms of an $(n-1)\times(n-1)$ determinants: 
	\[\left[\begin{tabular}{c|ccc}
		1&*&$\cdots$&*\\
		\hline
		0&&&\\
		$\vdots$&&$\matrixA_{n-1}$&\\
		0&&&
	\end{tabular}\right]=\det(\matrixA_{n-1})\]
\end{thm}
\begin{prf}
	To row reduce the $n\times n$ matrix, we row reduce the $(n-1)\times(n-1)$ matrix.\par $\Rightarrow\det(\matrixA_{n-1})=\det(\matrixA_n).$	
\end{prf}
\begin{eg}
	\[\begin{aligned}\det\qty(\mqty[{2&1&1\\1&2&1\\1&1&2}])&=2\mqty|{1&1&1\\0&2&1\\0&1&2}|+\mqty|0&1&1\\1&2&1\\0&1&2|+\mqty|0&1&1\\0&2&1\\1&1&2|\\&=2\mqty|2&1\\1&2|-\mqty|1&1\\1&2|+(-1)^2\mqty|1&1\\2&1|\\&=6-1-1=4.\end{aligned}\]	
\end{eg}
\begin{rmk}
	Note that the vertical bars denote determinant.
\end{rmk}
\begin{df}[$ij$-Cofactor]
	Let $\matrixA$ be an $n\times n$ matrix. The \textbf{$ij-$cofactor} of $\matrixA$ is the $(n-1)\times(n-1)$ obtained by deleting the $i^\text{th}$ row and $j^\text{th}$ column. We denote this matrix as $\matrixA_{ij}.$
\end{df}
\begin{thm}[Laplace Formula]
	Consider the $i^\text{th}$ column of a matrix $\matrixA$ (wasp $i^\text{th}$ row), then, \[\det(\matrixA)=\sum_{j=1}^n(-1)^{i+j}\cdot a_{ij}\cdot\det(\matrixA_{ji})\]	
\end{thm}

\subsection{The Multiplicativity of the Determinant and Other Properties}
\begin{thm}[Multiplicativity of the Determinant]
	Let $\matrixA$ and $\matrixB$ be $n\times n$ matrices, \[\det(\matrixA\matrixB)=\det(\matrixA)\times\det(\matrixB).\]	
\end{thm}
\begin{cor}
	\[\det(\matrixA^k)=\det(\matrixA)^k\]
\end{cor}
\begin{cor}
	If $\matrixA$ is invertible, \[\det(\matrixA^{-1})=\det(\matrixA)^{-1}.\]
	\begin{prf}
		Note that $\matrixA\matrixA^{-1}=\matrixI.$\[\begin{aligned}\therefore\det(\matrixA\cdot\matrixA^{-1})&=\det(\matrixI)=1\\&=\det(\matrixA)\times\det(\matrixA^{-1})\\\therefore\det(\matrixA^{-1})&=\det(\matrixA)^{-1}\end{aligned}\]
	\end{prf}
\end{cor}
\begin{cor}
	If $\matrixA$ is invertible, $\det(\matrixA)=\neq0.$	
\end{cor}
\begin{thm}
	If $\vb Q$ is an orthogonal transformation, then, $\det(\vb Q)=\pm1,$ or $\qty|\det(\vb Q)|=1.$	
\end{thm}
\begin{prf}
	$\qty|\det(\vb Q)|$ is the volume of the unit cube under $\vb Q.$\par The unit cube has a shape of volume $1$, which means it reserves volumes.\par Also that since $\vb Q$ is orthogonal, meaning this transformation preserves lengths and angles.\par Wants to show that preserving lengths and angles means preserving volumes.\par \[\vol(\vecv_1,\cdots,\vecv_n)=\|\vecv_1\|\|\vecv_2^\perp\|\|\vecv_3^\perp\|\cdots\|\vecv_n^\perp\|,\ \vecv_i^\perp\in\Span(\vecv_1,\cdots,\vecv_{i-1})\]\par Since $\vb Q=\mqty[{|&&|\\\vecu_1&\cdots&\vecu_n\\|&&|}],$ \[\qty|\det(\vb Q)|=\vol(\vecu_1,\cdots,\vecu_n)=\|\vecu_1\|\|\vecu_2^\perp\|\cdots\|\vecu_n^\perp\|\]\par Since $\vb Q$ is orthogonal, $\vecu_1,\cdots,\vecu_n$ are perpendicular to each other and have lengths of $1$, \[\qty|\det(\vb Q)|=\|\vecu_1\|\|\vecu_2\|\cdots\|\vecu_n\|=1\time1\times\cdots\times1=1.\]
\end{prf}
\begin{lem}
	When $\matrixA$ is invertible, $\det(\matrixA)=\det(\matrixA^\T)$
\end{lem}
\begin{prf}
	Use QR decomposition, we know that \[\matrixA^\T=(\vb Q\vb R)^\T=\vb R^\T\vb Q^\T\] \[\begin{aligned}\det(\matrixA)&=\det(\vb Q)\cdot\det(\vb R)\\\det(\matrixA^\T)&=\det(\vb R^\T)\cdot\det(\vb Q^\T)\\&=\det(\vb Q^\T)\cdot\det(\vb R^\T).\end{aligned}\]\par Wants to show: $\det(\vb Q)=\det(\vb Q^\T)$ and $\det(\vb R)=\det(\vb R^\T).$
	\begin{enumerate}
		\item Since $\vb Q$ is orthogonal, $\det(\vb Q)=\pm 1$.\par Also note that since $\vb Q$ is orthogonal, $\vb Q^\T=\vb Q^{-1}$.\[\det(\vb Q^\T)=\det(\vb Q^{-1})=\det(\vb Q)^{-1}=\det(\vb Q).\]
		\item Note that $\vb R$ is an upper triangular matrix, and thus its determinant is the product of the entries on diagonal: $\det(\vb R)=a_{11}\cdot a_{22}\cdots a_{nn}.$\par Also note that the transpose of $\vb R,$ $\vb R^\T$ is a lower triangular matrix, and thus we know that $\det(\vb R^\T)=a_{11}\cdot a_{22}\cdots a_{nn}.$\[\therefore \det(\vb R)=\det(\vb R^\T).\]
	\end{enumerate}
	\[\begin{aligned}\det(\matrixA)&=\det(\vb Q)\cdot\det(\vb R)\\&=\det(\vb Q^\T)\cdot\det(\vb R^\T)=\det(\matrixA^\T).\end{aligned}\]
\end{prf}
\begin{lem}
	If $\matrixA$ is not invertible, then $\matrixA^\T$ is also not invertible. 
\end{lem}
\begin{prf}
	$\matrixA$ is invertible exactly when $\rref(\matrixA)=\matrixI,$\par That is, $\rank(\matrixA)=n\implies\rank(\matrixA^\T)=n$ and thus, $\matrixA^\T$ is also invertible,\par If $\matrixA$ is not invertible, $\rank(\matrixA)<n.$\par Thus, $\rank(\matrixA^\T)=\rank(\matrixA)<n,$ indicating $\matrixA^\T$ is also not invertible. 
\end{prf}
\begin{lem}
	For an $n\times n$ matrix $\matrixA,$ $\rank(\matrixA)=\rank(\matrixA^\T).$
\end{lem}
\begin{prf}
	\[\begin{aligned}\dim(\Im(\matrixA)^\perp)&=n-\rank(\matrixA)\\\therefore\nullity(\matrixA^\T)=\dim(\ker(\matrixA^\T))&=n-\rank(\matrixA)\\\because\rank(\matrixA^\T)+\nullity(\matrixA^\T)&=n\\\therefore\rank(\matrixA^\T)+n-\rank(\matrixA)&=n\\\rank(\matrixA^\T)&=\rank(\matrixA)\end{aligned}\]
\end{prf}
\begin{prop}
	When $\matrixA$ is not invertible, then $\det(\matrixA)=\det(\matrixA^\T).$
\end{prop}
\begin{thm}
	\[\det(\matrixA)=\det(\matrixA^\T)\]	
\end{thm}
\begin{thm}[Cramer's Rule]
	Let $\matrixA$ be an invertible $n\times n$ matrix and $\vecb\in\R^n.$ The unique solution to the system $\matrixA\vecx=\vecb$ is the following vector \[\vecx=\dfrac{1}{\det(\matrixA)}\mqty[{\det(\matrixA_{1,\vecb})\\\vdots\\\det(\matrixA_{n,\vecb})}],\] where $\matrixA_{i,\vecb}$ is the $n\times n$ matrix obtained from $\matrixA$ by replacing the $i^\text{th}$ column with $\vecb.$
\end{thm}
\begin{prf}
	Since \[\mqty[|&&|\\\vecv_1&\cdots&\vecv_n\\|&&|]\mqty[x_1\\\vdots\\x_n]=x_1\vecv_1+\cdots+x_n\vecv_n=\vecb,\]\par so we have \[\det(\matrixA_{1,\vecb})=\mqty|{|&|&&|\\\vecb&\vecv_2&\cdots&\vecv_n\\|&|&&|}|=\mqty|{|&|&&|\\x_1\vecv_1+\cdots+x_n\vecv_n&\vecv_2&\cdots&\vecv_n\\|&|&&|}|.\]\par By the linearity of determinant, then \[\begin{aligned}\det(\matrixA_{1,\vecb})&=x_1\mqty|{|&|&&|\\\vecv_1&\vecv_2&\cdots&\vecv_n\\|&|&&|}|+x_2\mqty|{|&|&&|\\\vecv_2&\vecv_2&\cdots&\vecv_n\\|&|&&|}|+\cdots+x_n\mqty|{|&|&&|\\\vecv_n&\vecv_2&\cdots&\vecv_n\\|&|&&|}|\\&=x_1\mqty|{|&|&&|\\\vecv_1&\vecv_2&\cdots&\vecv_n\\|&|&&|}|=x_1\det(\matrixA).\end{aligned}\]\[\therefore x_1=\dfrac{\det(\matrixA_{1,\vecb})}{\det(\matrixA)}.\]\par Similarly, we can extend this proof to an arbitrary $x_i,$ \[\begin{aligned}\det(\matrixA_{i,\vecb})&=x_i\det(\matrixA)\\x_i&=\dfrac{\det(\matrixA_{i,\vecb})}{\det(\matrixA)}\end{aligned}\]
\end{prf}
\begin{eg}
	Suppose $\matrixA=\mqty[1&2\\3&4],$ then $\det(\matrixA)=-2.$ Let $\vecb=\mqty[1\\1].$\par Therefore, $\matrixA_{1,\vecb}=\mqty[1&2\\1&4],$ and $\det(\matrixA_{1,\vecb})=2.$ $\matrixA_{2,\vecb}=\mqty[1&1\\3&1],$ so $\det(\matrixA_{2,\vecb})=-2.$\par Then, \[\vecx=\dfrac{1}{-2}\mqty[2\\-2]=\mqty[-1\\1].\]
\end{eg}
\begin{rmk}
	For an arbitrary $2\times2$ matrix $\matrixA=\mqty[a&b\\c&d],\ a,b,c,d\in\R.$ Suppose $\vecb=\mqty[1\\0],$ then applying Cramer's Rule, we know \[\vecx=\dfrac{1}{ad-bc}\mqty[d\\-c].\]
\end{rmk}
\begin{thm}[Application of Cramer's Rule]
	Cramer's Rule can give formulas for $\matrixA^{-1}$ in general: We saw $\matrixA\vecx_1=\vece_1$ and $\matrixA\vecx_2=\vece_2,$ then \[\matrixA^{-1}=\mqty[|&|\\\vecx_1&\vecx_2\\|&|].\]	To be more specific, for a $2\times2$ matrix $\matrixA=\mqty[a&b\\c&d],$ we have \[\matrixA^{-1}=\dfrac{1}{\det(\matrixA)}\mqty[d&-b\\-c&a].\]
\end{thm}
\begin{thm}
	Give a matrix $\matrixA$ with integer entries and $\det(\matrixA)=\pm1,$ the matrix $\matrixA{-1}$ has integer entries. 
\end{thm}


\newpage
\section{Eigenvalues and Eigenvectors}
\subsection{Computing $\matrixA^k\vecx$}
\begin{df}[Eigenvector and Eigenvalue]
	Let $\matrixA$ be an $n\times n$ matrix, and \textbf{eigenvector} for $\matrixA$ is any non-zero vector $\vecx,$ such that $\matrixA\vecx=\lambda\vecx,$ for some $\lambda\in\R.$ The number $\lambda$ is called the \textbf{eigenvalue} for $\vecx.$ For an eigenvector $\vecx,$ $\matrixA^k\vecx=\lambda^k\vecx.$
\end{df}
\begin{df}[Eigenbasis]
	Let $\matrixA$ be an $nt\times n$ matrix, an \textbf{eigenbasis} for $\matrixA$ is a basis for $\R^n$ consisting of eigenvectors for $\matrixA.$	
\end{df}
\begin{thm}
	Let $\matrixA$ be an $n\times n$ matrix with an eigenbasis of $\vecv_1,\cdots\vecv_n.$ The eigenvalues for $\vecv_1,\cdots,\vecv_n$ are $\lambda_1,\cdots,\lambda_n,$ respectively. To compute $\matrixA^k\vecx,$ we could write $\vecx=c_1\vecv_1+\cdots+c_n\vecv_n,$ and then use linearity, we have \[\begin{aligned}\matrixA^k\vecx&=\matrixA\qty(c_1\vecv_1+\cdots+c_n\vecv_n)\\&=c_1\matrixA^k\vecv_1+\cdots+c_n\matrixA^{k}\vecv_n\\&=c_1\lambda_1^k\vecv_1+\cdots+c_n\lambda_n^k\vecv_n.\end{aligned}\]	
\end{thm}
\begin{thm}
	Consider $f_{\matrixA}:\R\to\R$ defined by \[f_{\matrixA}(t)=\det(\matrixA-t\matrixI).\] The zeros of $f_{\matrixA}(t)$ are exactly the eigenvalues of $\matrixA.$ That is, $f_{\matrixA}(\lambda)=0.$
\end{thm}
\begin{df}[Characteristic Polynomial]
	The \textbf{characteristic polynomial} of $\matrixA$ is the function \[f_{\matrixA}(t)=\det(\matrixA-t\matrixI).\]	
\end{df}
\begin{df}[Modified Definition of Eigenvectors]
	Let $\matrixA$ be an $n\times n$ matrix, and $\lambda$ be an eigenvalue for $\matrixA,$ i.e., a root of the characteristic polynomial of $\matrixA$ defined by $f_{\matrixA}(t)=\det(\matrixA-t\matrixI).$ An \textbf{eigenvector} with eigenvalue $\lambda$ for $\matrixA$ is any non-zero solution to $\matrixA\vecx=\lambda\vecx.$ i.e., non solution to \[(\matrixA-\lambda\matrixI)\vecx=0.\]
\end{df}
\begin{thm}
	The non-zero elements for $\ker(\matrixA-\lambda\matrixI)$ are exactly the eigenvectors with eigenvalue $\lambda.$	
\end{thm}
\begin{df}[$\lambda$-Eigenspace]
	Let $\lambda$ be an eigenvalue for $\matrixA,$ the \textbf{$\lambda$-eigenspace} is \[E_{\matrixA,\lambda}=\ker(\matrixA-\lambda\matrixI).\]
\end{df}
\begin{eg}
	Compute all eigenvectors for $\matrixA=\mqty[2&2\\1&3].$
	\begin{sol}
		\[f_{\matrixA}=(t-4)(t-1)\implies\lambda=1\quad\text{and}\quad4.\]
		\[E_{\matrixA,4}=\ker(\matrixA-4\matrixI)=\ker\qty(\mqty[-2&2\\1&-1])=\ker\qty(\mqty[1&-1\\0&0])=\Span\qty(\mqty[1\\1])\]
		\[E_{\matrixA,1}=\ker(\matrixA-\matrixI)=\ker\qty(\mqty[1&2\\1&2])=\ker\qty(\mqty[1&2\\0&0])=\Span\qty(\mqty[-2\\1])\]
	\end{sol}
\end{eg}
\begin{framed}
\textbf{Computing $\matrixA^{k}\vecx$}:
\begin{enumerate}
	\item Compute $f_{\matrixA}(t)=\det(\matrixA-t\matrixI)$
	\item Find the roots of $f_{\matrixA}(t)$; those are eigenvalues.
	\item Compute eigenspaces
	\item Ask: Is there an eigenbasis?
	\item Write $\vecx$ in form of $c_1\vecv_1+\cdots+c_n\vecv_n$
	\item Find the formula
\end{enumerate}	
\end{framed}
\begin{eg}
	Let $\matrixA=\mqty[0&6\\1&-1].$ Find a formula for $\matrixA^k\mqty[1\\1]$ for all $k$.
	\begin{sol}
	Find eigenvectors $\vecv_1$ and $\vecv_2$ for $\matrixA$ and express $\mqty[1\\1]=c_1\vecv_1+c_2\vecv_2.$ Then, \[\matrixA^k\mqty[1\\1]=c_1\matrixA^k\vecv_1+c_2\matrixA^k\vecv_2=c_1\lambda_1^k\vecv_1+c_2\lambda_2^k\vecv_2.\]
	\begin{enumerate}
		\item Compute $f_{\matrixA}(t)$: \[f_{\matrixA}(t)=\det(\matrixA-t\matrixI)=\mqty|{-t&6\\1&-1-t}|=t(1+t)-6=t^2=t-6.\]
		\item Find roots to the polynomial: \[f_{\matrixA}(t)=(t+3)(t-2)=0\implies\lambda_1=t_1=2\quad\lambda_2=t_2=-3.\]
		\item Compute eigenspaces: \[E_{\matrixA,2}=\ker\qty(\matrixA-2\matrixI)=\ker\qty(\mqty[-2&6\\1&-3])=\ker\qty(\mqty[1&-3\\0&0])=\Span\qty(\mqty[3\\1]);\]\[E_{\matrixA,-3}=\ker\qty(\matrixA+3\matrixI)=\ker\qty(\mqty[3&6\\1&2])=\ker\qty(\mqty[1&2\\0&0])=\Span\qty(\mqty[-2\\1]).\]
		\item Ask: IS there an eigenbasis for $\matrixA$? \par YES! \[\vecv_1=\mqty[3\\1];\qquad\vecv_2=\mqty[-2\\1];\qquad\lambda_1=2;\qquad\lambda_2=-3.\]
		\item Solve $\mqty[3&-2\\1&1]\mqty[c_1\\c_2]=\mqty[1\\1]: $\[\matrixS=\mqty[3&-2\\1&1]\implies\matrixS^{-1}=\dfrac{1}{5}\mqty[1&2\\-1&3]\]\[\therefore\mqty[c_1\\c_2]=\dfrac{1}{5}\mqty[1&2\\-1&3]\mqty[1\\1]=\dfrac{1}{5}\mqty[3\\2]\implies\mqty[1\\1]=\dfrac{3}{5}\mqty[3\\1]+\dfrac{2}{5}\mqty[-2\\1]\]
		\item Find the formula: \[\matrixA^k\mqty[1\\1]=\dfrac{3}{5}(2)^k\mqty[3\\1]+\dfrac{2}{5}\qty(-3)^k\mqty[-2\\1].\]
	\end{enumerate}
	\end{sol}
\end{eg}
\begin{rmk} The matrix $\matrixA=\mqty[0&-1\\1&0]$ has no eigenvectors.\end{rmk}
\begin{prf}
	$\boxed{\text{Algebraic}}$ \[f_{\matrixA}(t)=\mqty|{-t&-1\\1&-t}|=t^2+1>0\forall t\in\R.\]\par So, $f_{\matrixA}(t)$ has no zeros $\implies$ no eigenvalues $\implies$ no eigenvectors.\par $\boxed{\text{Geometric}}$ $\matrixA$ encodes rotation counterclockwise by $90^\circ.$\par  The condition $\matrixA\vecx=\lambda\vecx$ implies that $\matrixA\vecx$ and $\vecx$ have to be on the same line.\par  Yet, rotation by $90^\circ$ preserves no lines.
\end{prf}

\subsection{Diagonalization}
\begin{df}[Diagonal Matrix]
	Let $\matrixD=\mqty[a_{11}&\cdots&a_{1n}\\\vdots&\ddots&\vdots\\a_{n1}&\cdots&a_{nn}],$ we say $\matrixD$ is a \textbf{diagonal matrix} if $a_{ij}=0$ for all $i\neq j.$	
\end{df}
\begin{thm}
	$\matrixD$ is diagonal if and only if $\matrixD\vece_i=\lambda_i\vece_i,\quad\lambda_i\in\R.$ i.e., $e_1,\cdots,e_n$ are eigenvectors for $\matrixD.$ That is, eigenvalues are the diagonal entries: $\mqty[\lambda_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\lambda_n].$	
\end{thm}
\begin{thm}Properties of Diagonal Matrices
\begin{itemize}
	\item Computing $\matrixD^k$ \[\begin{aligned}\matrixD^k&=\mqty[|&|&&|\\\matrixD^k\vece_1&\matrixD^k\vece_2&\cdots&\matrixD^k\vece_n\\|&|&&|]=\mqty[|&|&&|\\\lambda_1^k\vece^1&\lambda_2^k\vece^2&\cdots&\lambda_n^k\vece^n\\|&|&&|]\\&=\mqty[\lambda_1^k&0&\cdots&0\\0&\lambda_2^k&&\vdots\\\vdots&&\ddots&0\\0&\cdots&0&\lambda_n^k]\end{aligned}\]
	\item Computing $\matrixD^{-1}$ \[\matrixD^{-1}=\mqty[\lambda_1^{-1}&0&\cdots&0\\0&\lambda_2^{-1}&&\vdots\\\vdots&&\ddots&0\\0&\cdots&0&\lambda_n^{-1}]\]
	\item Rank of $\matrixD$ \[\rank(\matrixD)=\text{number of non-zero diagonal entries.}\]
	\item Nullity of $\matrixD$ \[\nullity(\matrixD)=\text{number of zeros along the diagonal}.\]
	\item Determinant of $\matrixD$ \[\det(\matrixD)=\lambda_1\lambda_2\cdots\lambda_n\]
\end{itemize}
\end{thm}
\begin{df}[Diagonalizable]
	Let $\matrixA$ be an $n\times n$ matrix. $\matrixA$ is said to be \textbf{diagonalizable} if there is an eigenbasis for $\matrixA.$ i.e., there is a basis $\vecv_1,\cdots,\vecv_n$ of $\R^n$ such that $\vecv_1,\cdots,\vecv_n$ are eigenvectors of $\matrixA$.	
\end{df}
\begin{thm}
	$\matrixA$ is diagonalizable if and only if \[\matrixA=\matrixS\matrixD\matrixS^{-1},\] where $\matrixD$ is diagonal with diagonal entries the eigenvalues of $\matrixA$ ($\lambda_1\cdots\lambda_n$), and $\matrixS$ is invertible with column vectors the eigenvectors of $\matrixA$ ($\vecv_1,\cdots,\vecv_n$). \textbf{Diagonalizing} a matrix means to find an invertible matrix $\matrixS$ and a diagonal matrix $\matrixD$ such that $\matrixA=\matrixS\matrixD\matrixS^{-1}.$
\end{thm}
\begin{eg}
	Let $\matrixA=\mqty[1&1&1\\1&1&1\\1&1&1].$ Diagonalize $\matrixA.$
	\begin{sol}
		By definition, we know \[f_{\matrixA}(t)=\det\qty(\mqty[1-t&1&1\\1&1-t&1\\1&1&1-t])=3t^2-t^3\implies t_1=t_2=0,\quad t_3=3.\]\par Therefore, we know \[E_{\matrixA,0}=\ker(\matrixA)=\ker\qty(\mqty[1&1&1\\1&1&1\\1&1&1])=\Span\qty(\mqty[1\\-1\\0],\mqty[1\\0\\-1]);\] \[E_{\matrixA,3}=\ker(\matrixA-3\matrixI)=\ker\qty(\mqty[-2&1&1\\1&-2&1\\1&1&-2])=\Span\qty(\mqty[1\\1\\1])\]\par Note that since $\vecv_1=\mqty[1\\-1\\0],\vecv_2=\mqty[1\\0\\-1],\vecv_3=\mqty[1\\1\\1]$ span $\R^3,$ they are eigenbasis of $\matrixA.$ \[\begin{aligned}\therefore\matrixA=\matrixS\matrixD\matrixS^{-1}&=\mqty[1&1&1\\-1&0&1\\0&-1&1]\mqty[0&0&0\\0&0&0\\0&0&3]\mqty[1&1&1\\-1&0&1\\0&-1&1]^{-1}\\&=\mqty[1&1&1\\-1&0&1\\0&-1&1]\mqty[0&0&0\\0&0&0\\0&0&3]\mqty[1/3&-2/3&1/3\\1/3&1/3&-2/3\\1/3&1/3&1/3]^{-1}.\end{aligned}\]
	\end{sol}
\end{eg}
\begin{cor}Linear Algebra Becomes Easy for Diagonalized Matrices
	\begin{enumerate}
		\item $\matrixA^{k}=\matrixS\matrixD^{k}\matrixS^{-1}$
		\begin{prf}
			\[\matrixA^{k}=\underbrace{\matrixS\matrixD\matrixS^{-1}\matrixS\matrixD\matrixS^{-1}\cdots\matrixS\matrixD\matrixS^{-1}}_{k\text{ times}}=\matrixS\matrixD^{k}\matrixS^{-1}\]
		\end{prf}
		\item $\matrixA^{-1}=\matrixS\matrixD^{-1}\matrixS^{-1}$
		\begin{prf}
			Since \[\matrixA(\matrixS\matrixD^{-1}\matrixS^{-1})=\matrixS\matrixD\matrixS^{-1}\matrixS\matrixD^{-1}\matrixS^{-1}=\matrixS\matrixD\matrixD^{-1}\matrixS^{-1}=\matrixS\matrixS^{-1}=\matrixI,\]\par so we know that $\matrixA^{-1}=\matrixS\matrixD^{-1}\matrixS^{-1}.$
		\end{prf}
		\item $\det(\matrixA)=\det(\matrixD)=\lambda_1\cdots\lambda_n$
		\begin{prf}
			\[\det(\matrixA)=\det(\matrixS\matrixD\matrixS^{-1})=\det(\matrixS)\det(\matrixD)\det(\matrixS^{-1})=\det(\matrixS)\det(\matrixD)\det(\matrixS)^{-1}=\det(\matrixD).\]
		\end{prf}
		\item $f_{\matrixA}(t)=\det(\matrixA-t\matrixI)=\det(\matrixD-t\matrixI)=f_{\matrixD}(t)=\dsst\prod_{i=1}^n\qty(\lambda_i-t)$
		\begin{prf}
			Let's fix $t\in\R.$ then \[\matrixS(\matrixD-t\matrixI)\matrixS^{-1}=\matrixS\matrixD\matrixS^{-1}-t\matrixS\matrixI\matrixS^{-1}=\matrixA-t\matrixI.\]
		\end{prf}
		\item $\rank(\matrixA)=\rank(\matrixD)=$ number of non-zero $\lambda_i.$
		\item $\nullity(\matrixA)=\nullity(\matrixD)=$ number of zero $\lambda_i.$
		\item If $f_{\matrixA}(t)$ is not a polynomial with all real roots, then $\matrixA$ is not diagonalizable. 
	\end{enumerate}
\end{cor}

\subsection{Procedure of Finding an Eigenbasis}
\begin{df}[Revisit Definition of Characteristic Polynomials]
	For an $n\times n$ matrix $\matrixA$, its characteristic polynomial is a function $f_{\matrixA}:\R\to\R$ defined by $f_{\matrixA}(t)=\det(\matrixA-t\matrixI).$
\end{df}
\begin{thm}
	$f_{\matrixA}(\lambda)=0$ if and only if $\lambda$ is an eigenvalue of $\matrixA.$
\end{thm}
\begin{thm}
	$f_{\matrixA}(t)$ is a polynomial. i.e., $f_{\matrixA}(t)=a_dt^d+a{d-1}t^{d-1}+\cdots+a_0,$ where $a_d\neq0$ and $a_i\in\R.$ We say $d$ is the degree of $f_{\matrixA}(t).$
\end{thm}
\begin{rmk}
	Determinant can be calculated without division: Laplace Expansion
\end{rmk}
\begin{prf}
	Note that $\matrixA-t\matrixI$ is an $n\times n$ matrix with polynomial entries. \par By Laplace expansion, $\det(\matrixA-t\matrixI)$ is the sums and products of those polynomial entries, and thus it is a polynomial. 
\end{prf}
\begin{prop}
	If $k\in\R$ and $\matrixB$ is an $n\times n$ matrix, then \[\det(k\matrixB)=k^n\det(\matrixB).\]	
\end{prop}
\begin{prf}
	\[\det\qty(\mqty[-&k\vecr_1&-\\&\vdots&\\-&k\vecr_n&-])=k^n\cdot\det\qty(\mqty[-&\vecr_1&-\\&\vdots&\\-&\vecr_n&-])\]	
\end{prf}
\begin{thm}
	If $\matrixA$ is an $n\times n$ matrix, then the degree of $f_{\matrixA}$ is $n$.
\end{thm}
\begin{prf}
	Since $f_{\matrixA}(t)$ is a polynomial, $f_{\matrixA}(t)=a_dt^d+a{d-1}t^{d-1}+\cdots+a_0,$ where $a_d\neq0$ and $a_i\in\R.$\par \textit{If we can prove $f_{\matrixA}(t)$ and $\det(-t\matrixI)$ has the same growth rate, and since $\det(-t\matrixI)=(-t)^n\det(\matrixI)=(-t)^n,$ we can say $f_{\matrixA}(t)$ has a degree of $n$. Therefore, we want to show $\dsst\lim_{t\to\infty}\dfrac{f_{\matrixA}(t)}{t^n}$ is finite and non-zero.}	\[\lim_{t\to\infty}\dfrac{f_{\matrixA}(t)}{t^n}=\lim_{t\to\infty}\dfrac{\det(\matrixA-t\matrixI)}{t^n}=\lim_{t\to\infty}\det(\dfrac{\matrixA}{t}-\matrixI)=\lim_{t\to0}\det(t\matrixA-\matrixI)=\det(-\matrixI)=(-1)^n\]\par $\therefore a_d=(-1)^n,$ and $f_{\matrixA}(t)$ has a degree of $n$.
\end{prf}
\begin{rmk}
	If $\lambda_1,\cdots\lambda_k$ are roots of $f_{\matrixA}(t),$ them \[f_{\matrixA}(t)=(t-\lambda_1)^{M_1}(t-\lambda_2)^{M_2}\cdots(t-\lambda_k)^{M_k}g(x),\] where $g(x)$ has no real roots. Then, \[n=M_1+M_2+\cdots+M_k+\text{degree}(g(x)).\] Counted with multiplicity (this power $M_k$), $\matrixA$ has \underline{at most} $n$ eigenvalues.
\end{rmk}
\begin{thm}
	$\matrixA$ has exactly $n$ roots (counted with multiplicity) when $\matrixA$ is diagonalizable.	
\end{thm}
\begin{df}[Algebraic Multiplicity]
	The \textbf{algebraic multiplicity} of a matrix $\matrixA$ is the multiplicity of an eigenvalue $\lambda$ in the characteristic polynomial of $\matrixA.$
\end{df}
\begin{thm}
	If we write $f_{\matrixA}(t)$ as $f_{\matrixA}(t)=a_dt^d+a{d-1}t^{d-1}+\cdots+a_0,$ where $a_d\neq0$ and $a_i\in\R,$  then $a_0=\det(\matrixA)$
\end{thm}
\begin{prf}
	\[f_{\matrixA}(0)=a_0=\det(\matrixA-0\cdot\matrixI)=\det(\matrixA).\]	
\end{prf}
\begin{eg}
	Prove that matrix $\matrixB=\mqty[0&1\\0&0]$ is not diagonalizable. 	
\end{eg}
\begin{prf}
	Note that $f_{\matrixB}(t)=\mqty|{-t&1\\0&-t}|=t^2\implies\lambda_1=0,\ \text{multiplicity}=2.$	\par $\boxed{\text{Method }1}$ Assume for the sake of contradiction that $\matrixB$ is diagonalizable.\par Therefore, $\matrixB=\matrixS\matrixD\matrixS^{-1},$ where $\matrixS=\mqty[|&|\\\vecv_1&\vecv_2\\|&|]$ and $\matrixD=\mqty[\lambda_1&0\\0&\lambda_2]$ for eigenvectors $\vecv_1$ and $\vecv_2$ with eigenvalues $\lambda_1$ and $\lambda_2,$ respectively. \par Then, \[\matrixD=\mqty[0&0\\0&0]=0\cdot\matrixI\]\[\matrixB=\matrixS\matrixD\matrixS^{-1}=\matrixS(0\cdot\matrixI)\matrixS^{-1}=0\cdot\matrixS\cdot\matrixI\cdot\matrixS^{-1}=0\cdot\matrixS\cdot\matrixS^{-1}=0\]\par$\divideontimes$ This is a contradiction that $\matrixB=\mqty[0&1\\0&0]=\neq0.$\par Therefore, $\matrixB$ cannot be diagonalizable.\par $\boxed{\text{Method }2}$ From Method 1, we know that \[E_{\matrixB,0}=\ker(\matrixB-0\cdot\matrixI)=\ker\qty(\mqty[0&1\\0&0])=\Span\qty(\mqty[1\\0])\]\par Since $\ker(\matrixB)$ is 1-dimensional, it doesn't contain a basis for $\R^2.$\par So, $\matrixB$ doesn't have an eigenbasis.
\end{prf}
\begin{thm}
	A matrix $\matrixC=\mqty[\lambda&*&\cdots&\cdots\\0&\lambda&&\vdots\\\vdots&&\ddots&*\\\cdots&\cdots&0&\lambda]$ is diagonalizable if and only if $\matrixC$ is diagonal.
\end{thm}
\begin{prf}
	Since $f_{\matrixC}(t)=\mqty|\lambda-t&*&\cdots&\cdots\\0&\lambda-t&&\vdots\\\vdots&&\ddots&*\\\cdots&\cdots&0&\lambda-t|=(\lambda-t)^n,$ we know $\matrixC$ has $\lambda$ as the only eigenvalue with algebraic multiplicity of $n.$ \[\therefore\matrixD=\mqty[\lambda&0&\cdots&\cdots\\0&\lambda&&\vdots\\\vdots&&\ddots&0\\\cdots&\cdots&0&\lambda]=\lambda\cdot\matrixI.\]\[\therefore\matrixC=\matrixS\matrixD\matrixS^{-1}=\matrixS(\lambda\matrixI)\matrixS^{-1}=\lambda\matrixS\matrixI\matrixS^{-1}=\lambda\matrixI.\]
\end{prf}
\begin{thm}
	For matrix $\matrixC=\mqty[\lambda&*&\cdots&\cdots\\0&\lambda&&\vdots\\\vdots&&\ddots&*\\\cdots&\cdots&0&\lambda],$ $\lambda$ is the only eigenvalue. Also, $E_{\matrixC,\lambda}=\ker(\matrixC-\lambda\matrixI)$ contains a basis if and only if $\ker(\matrixC-\lambda\matrixI)$ is the entire space. i.e., $\matrixC-\lambda\matrixI=0,$ or $\matrixC=\lambda\matrixI.$
\end{thm}
\begin{thm}
	Let $\matrixA$ be an $n\times n$ matrix, and $\vecv_1,\cdots,\vecv_k$ be eigenvectors of $\matrixA.$ The vectors $\vecv_1,\cdots,\vecv_k$ are linearly independent if for every eigenvalue $\lambda$ of $\matrixA,$ the set of these vectors with eigenvalue $\lambda.$ i.e., $\qty{\vecv_i\mid\matrixA\vecv_i=\lambda\vecv_i}$ is linearly independent. 
\end{thm}
\begin{framed}
\textbf{Finding an eigenbasis/diagonalizing $\matrixA$ as an $n\times n$ matrix.}
\begin{enumerate}
	\item Find eigenvalues of $\matrixA$
	\begin{enumerate}
		\item Compute $f_{\matrixA}(t)$
		\item Find the roots of $f_{\matrixA}(t)$ and the multiplicity $M_1,\cdots,M_k$
		\begin{rmk}
			If $M_1+M_2+\cdots+M_k\neq n,$ then STOP. $\matrixA$ is not diagonalizable.
		\end{rmk}
		\item Form matrix $\matrixD.$
	\end{enumerate}
	\item Find basis for eigenspaces: 
	\begin{enumerate}
		\item Form $\matrixS=\mqty[|&&|\\\vecv_1&\cdots&\vecv_n\\|&&|],$ where $\vecv_1,\cdots,\vecv_n$ are linearly independent. 
		\item For each $\lambda_i,$ compute a basis for $\ker(\matrixA-\lambda_i\matrixI).$
		\begin{rmk}
			If 	$\dim(\ker(\matrixA-\lambda_i\matrixI))<M_i,$ then STOP. There is no enough eigenvectors and $\matrixA$ is not diagonalizable.
		\end{rmk}
	\end{enumerate}
	\item By theorem, the concatenation of the lists of bases is an eigenbasis. 
\end{enumerate}
\end{framed}
\begin{df}[Geometric Multiplicity]
	Let $\lambda$ be an eigenvalue of $\matrixA.$. The \textbf{geometric multiplicity} of $\lambda$ is $\dim(\ker(\matrixA-\lambda\matrixI)),$ the number of linearly independent vector in an eigenspace.
\end{df}
\begin{thm}
	For a matrix to be diagonalizable, \[\text{geometric multiplicity}=\text{algebraic multiplicity}.\]	
\end{thm}

\subsection{Multiplicity}
\begin{df}[Multiplicity]
	Let $\matrixA$ be an $n\times n$ matrix and $\lambda$ be an eigenvalue of $\matrixA:$
	\begin{enumerate}
		\item The \textbf{algebraic multiplicity} of $\lambda$ is the largest $k$ such that $f_{\matrixA}(t)=(t-\lambda)^kg(t),$ where $g(t)$ is a polynomial. We denote the algebraic multiplicity of $\lambda$ as $\almu(\lambda)=k.$ $\almu(\lambda)$ is the multiplicity of $\lambda$ as a root of $f_{\matrixA}(t).$
		\item The \textbf{geometric multiplicity} of $\lambda$ is $\gemu(\lambda)=\dim(\ker(\matrixA-\lambda\matrixI)).$ $\gemu(\lambda)$ is the maximum number of linearly independent eigenvectors with eigenvalue $\lambda.$
	\end{enumerate}	
\end{df}
\begin{thm}
$\gemu(\lambda)\leq\almu(\lambda).$	
\end{thm}
\begin{rmk}
	Note that in $\matrixA-\lambda\matrixI,$ every non-zero diagonal entry contributes a pivot to $\rref(\matrixA-\lambda\matrixI).$ Then, $\rank(\matrixA-\lambda\matrixI)\geq$ the number of diagonal entries that is not $\lambda.$\par  Therefore, $\nullity(\matrixA-\lambda\matrixI)\leq$ the number of diagonal entries that equals $\lambda.$\par Hence, $\gemu(\lambda)\leq\almu(\lambda).$
\end{rmk}
\begin{prf}
	Assume $\vecv_1,\cdots,\vecv_g$ is a basis of $E_{\matrixA,\lambda}.$ Then, $\gemu(\lambda)=g.$\par Choose $\vecv_{g+1},\cdots,\vecv_n$ such that $\vecv_1,\cdots,\vecv_g,\vecv_{g+1},\cdots,\vecv_n$ is a basis for $\R^n.$ Then, \[\matrixS=\mqty[|&&|&&|\\\vecv_1&\cdots&\vecv_g&\cdots&\vecv_n\\|&&|&&|]\]\par \textit{$\matrixS$ is invertible since $\vecv_1,\cdots\vecv_n$ is a basis.}\par \begin{clm}$\matrixB=\matrixS^{-1}\matrixA\matrixS,$ where $\matrixB=\left[\begin{array}{c|c}\mqty{\lambda&&\\&\ddots&\\&&\lambda}&*\\\hline\underbrace{0}_{g-\text{columns}}&\matrixC\end{array}\right]$\end{clm}\[\begin{aligned}f_{\matrixB}(t)=\det(\matrixS\matrixA\matrixS^{-1}-t\matrixI)=\det(\matrixS\matrixA\matrixS^{-1}-t\matrixS^{-1}\matrixI\matrixS)&=\det(\matrixS^{-1}(\matrixA-t\matrixI)\matrixS)\\&=\det(\matrixA-t\matrixI)=f_{\matrixA}(t).\end{aligned}\]\par Since $f_{\matrixB}(t)=(\lambda-t)^gf_{\matrixC}(t)=f_{\matrixA}(t),$ we know that $\gemu\leq\almu.$
\end{prf}
\begin{thm}
	For a matrix to be diagonalizable, it is necessary that $\almu(\lambda)=\gemu(\lambda)$ for all $\lambda.$
\end{thm}
\begin{thm}
	Let $\matrixA$ be an $n\times n$ matrix. If $f_{\matrixA}(t)$ has $n$ distinct real roots, then $\matrixA$ is diagonalizable.
\end{thm}
\begin{prf}
	Every eigenvalue has an eigenvector: $\det(\matrixA-\lambda\matrixI)=0\implies(\matrixA-\lambda\matrixI)$ is not invertible. \[\therefore\ker(\matrixA-\lambda\matrixI)=\neq0.\]\par Therefore, there are eigenvectors $\vecv_1,\cdots\vecv_n$ for eigenvalues $\lambda_1,\cdots,\lambda_n,$ respectively.\par Since eigenvectors with distinct eigenvalues are linearly independent, $\vecv_1,\cdots,\vecv_n$ is an eigenbasis. 
\end{prf}


\newpage
\section{Singular Value Decomposition}
\subsection{The Spectral Theorem}
\begin{df}[Symmetry]
	A matrix $\matrixA$ is called \textbf{symmetric} if $\matrixA=\matrixA^\T.$ A symmetric matrix is symmetric across the diagonal. That is, $a_{ij}=a_{ji}.$	
\end{df}
\begin{thm}[Spectral Theorem]
	$\matrixA$ is symmetric if and only if $\matrixA$ has an orthogonal eigenbasis. Equivalently, $\matrixA=\matrixS\matrixD\matrixS^{-1},$ where $\matrixD$ is diagonal and $\matrixS$ is orthogonal (having orthogonal columns). That is, $\matrixA=\matrixS\matrixD\matrixS^\T$ because $\matrixS^{-1}=\matrixS^\T$ if $\matrixS$ is orthogonal. 	
\end{thm}
\begin{prf}
	\textit{Given $\matrixA=\matrixS\matrixD\matrixS^\T,$ we want to show that $\matrixA=\matrixA^\T.$}\par Note that since $\matrixD$ is diagonal, we have $\matrixD=\matrixD^\T.$ Then
	\[\matrixA^\T=(\matrixS\matrixD\matrixS^\T)^\T=(\matrixS^\T)^\T(\matrixD)^\T\matrixS^\T=\matrixS\matrixD^\R\matrixS^\T=\matrixS\matrixD\matrixS^\T=\matrixA.\]
\end{prf}
\begin{thm}
	Orthogonal projection is symmetric. 	
\end{thm}
\begin{prf}
	Let $V$ to be a subspace of $\R^n$. Define $\Proj_{\vecv}:\R^n\to\R^n$ as $\vecx\longmapsto\vecx^\parallel\in V,$ where $\vecx=\vecx^\parallel+\vecx^\perp$ and $\vecx^\perp\in V^\perp.$ Finding eigenspaces of $\Proj_{\vecv},$ we get \[E_{\Proj_{\vecv},1}=V,\qquad\text{and}\qquad E_{\Proj_{\vecv},0}=V^\perp.\]\par Since eigenspaces are perpendicular, $\Proj_{\vecv}$ is symmetric. 
\end{prf}
\begin{cor}
	Let \[V=\Span\qty(\mqty[\vecv_1\\\vdots\\\vecv_n])=\Span(\vecv),\] then $\vecx\longmapsto V$ is $\dfrac{\vecx\cdot\vecv}{\vecv\cdot\vecv}\cdot\vecv.$ That is, $\dfrac{1}{\vecv\cdot\vecv}\mqty[\vecv\\\vdots\\vecv_n]\qty(\mqty[\vecv_1&\cdots&\vecv_n]\vecx).$ \[\therefore\Proj_{\vecv}\qty(\vecx)=\dfrac{1}{\vecv\cdot\vecv}\mqty[v_1v_1&v_1v_2&\cdots&\cdots\\v_1v_2&v_2v_2&&\vdots\\\cdots&&\ddots&\vdots\\\cdots&\cdots&\cdots&v_nv_n]\]
\end{cor}
\begin{thm}[Adjoint Property of Transpose]
	If $\matrixA$ is an $n\times n$ matrix, $\vecv_1,\vecv_2\in\R^n.$ Then, $\qty(\matrixA\vecv_1)\cdot\vecv_2=\vecv_1\cdot\qty(\matrixA^\T\vecv_2).$ That is, bringing a matrix through a dot product, transpose it. Specially, if $\matrixA$ is symmetric, $\matrixA=\matrixA^\T,$ and $\qty(\matrixA\vecv_1)\cdot\vecv_2=\vecv_1\cdot\qty(\matrixA\vecv_2).$
\end{thm}
\begin{prf}
	\[\qty(\matrixA\vecv_1)\cdot\vecv_2=\qty(\matrixA\vecv_1)\vecv_2=(\vecv_1)^\T\qty(\matrixA^\T\vecv_2)=\vecv_1\cdot\qty(\matrixA^\T\vecv_2).\]	
\end{prf}
\begin{thm}[Spectral Theorem -- Continued 1]
	If $\vecv_1,\vecv_2$ are eigenvectors for $\matrixA$ with eigenvalues $\lambda_1\neq\lambda_2,$ then $\vecv_1\perp\vecv_2.$	
\end{thm}
\begin{prf}
	Note that \[\lambda_1\vecv_1\cdot\vecv_2=\matrixA\vecv_1\cdot\vecv_2=\vecv_1\cdot\qty(\matrixA\vecv_2)=\lambda_2\vecv_1\cdot\vecv_2,\]\par  However, by our assumption we have $\lambda_1\neq\lambda_2.$ So it must be $\vecv_1\cdot\vecv_2=0.$\par That is, exactly, $\vecv_1\perp\vecv_2.$
\end{prf}
\begin{cor}
	Distinct eigenspaces are perpendicular. 	
\end{cor}
\begin{thm}[Spectral Theorem -- Continued 2]
	If $\matrixA$ is a symmetric matrix, then $\matrixA$ has an orthogonal eigenbasis.
	\begin{prf}
		\begin{clm}
			$f_{\matrixA}(t)$ has all real roots. 
		\end{clm}\par 
		\textit{If $\lambda=x+\i y$ is a root of $f_{\matrixA}(t)$ , show $y=0$.}\par Let $\vecv+\i\vecw\in\C^n$ be an eigenvector for $\matrixA$ with eigenvalue $\lambda.$ Then, $\vecv-\i\vecw\in\C^n$ is also an eigenvector for $\matrixA$ with eigenvalue $\lambda^*=x-\i y.$ \[\therefore\begin{cases}\matrixA(\vecv-\i\vecw)=\lambda^*(\vecv-\i\vecw)\\\matrixA(\vecv+\i\vecw)=\lambda(\vecv+\i\vecw)\end{cases}\]\[\begin{aligned}\therefore(\vecv-\i\vecw)\cdot\matrixA(\vecv+\i\vecw)&=\lambda(\vecv-\i\vecw)\cdot(\vecv+\i\vecw)\\&=\lambda(\vecv\cdot\vecv+\vecw\cdot\vecw)\\&=\lambda(\underbrace{\|\vecv\|^2+\|\vecw\|^2}_{\text{greater than }0})\\\matrixA(\vecv-\i\vecw)\cdot(\vecv+\i\vecw)&=\lambda^*(\vecv-\i\vecw)\cdot(\vecv+\i\vecw)\\&=\lambda^*(\underbrace{\|\vecv\|^2+\|\vecw^2\|}_{\text{greater than }0})\\\therefore\lambda(\|\vecv\|^2+\|\vecw\|^2)&=\lambda^*(\|\vecv\|^2+\|\vecw\|^2)\end{aligned}\]\par Since $\|\vecv\|^2+\|\vecw\|^2>0,$ it must be $\lambda=\lambda^*.$ That is, $x+\i y=x-\i y.$ So, $y=0.$\par Thus, all the roots are real. 
		\begin{clm} $\matrixA$ has an eigenbasis. That is, $\gemu(\lambda)=\almu(\lambda)\forall\lambda.$\end{clm}\par 
		We can write $\matrixA=\matrixS_1\underbrace{\left[\begin{array}{c|c}*&\mqty{0&\cdots&0}\\\hline\mqty{0\\\vdots\\0}&\matrixA_{n-1}\end{array}\right]}_{\text{symmetric matrix}}\matrixS_1^{-1}$\par We can do the same thing over and over again, and eventually, we will get a diagonal matrix. So we know $\gemu(\lambda)=\almu(\lambda).$
	\end{prf}
\end{thm}
\begin{cor}
	We can always diagonalize $\matrixA.$
\end{cor}
\begin{framed}
\textbf{Find orthogonal eigenbasis of a symmetric matrix $\matrixA$}
\begin{enumerate}
	\item Find an eigenbasis for $\matrixA.$
	\item Run Gram-Schudt on eigenbasis. The result is orthogonal eigenbasis.
\end{enumerate}	
\end{framed}

\subsection{Quadratic Form, Principal Axis Theorem}
\begin{df}[Quadratic Form]
	A \textbf{quadratic form} is a function $f:\R^n\to\R$ of the form \[f(x_1,\cdots,x_n)=\sum_{1\leq i,j\leq n}a_{ij}x_{i}x_{j}\]	for some constants $a_{ij}\in\R.$
	\begin{eg}
		$7x^2+3xy+4y^2$ and $7x^2+3xy+4xz+2y^2+3yz+7z^2$ are quadratic forms.
	\end{eg}
\end{df}
\begin{df}[Diagonal form]
	A quadratic form is called a \textbf{diagonal form} if \[f(x_1,\cdots,x_n)=\lambda_1x_1^2+\lambda_2x_2^2+\cdots+\lambda_nx_n^2,\]	where $\lambda_i\in\R.$
	\begin{eg}
		$x^2+y^2$, $x^2-y^2,$ and $-x^2-y^2$ are examples of quadratic forms in the diagonal form. But $xy$ or $x^2+7xy+3y^2$ are not examples of diagonal forms.
	\end{eg}
\end{df}
\begin{df}[Degenerate]
	A diagonal form is called \textbf{degenerate} if $\lambda_i=0$ for some $i.$ If $\lambda_i\neq0\forall i,$ then the diagonal form is called \textbf{non-degenerate}.	
\end{df}
\begin{thm}
	Let $f$ be a non-degenerate diagonal form: 
	\begin{enumerate}
		\item If all $\lambda_i>0,$ then $f(\vecx)\geq0$ and $f(\vecx)=0$ if and only if $\vecx=0.$ That is, $\va 0$ is a global minimum, and $f$ is positive definite. 
		\item If all $\lambda_i<0,$ then $f(\vecx)\leq0$ and $f(\vecx)=0$ if and only if $\vecx=0.$ That is, $\va 0$ is a global maximum, and $f$ is negative definite. 
		\item If some $\lambda_i$ are positive and some are negative, there is no local maxima or minima, and we say $f$ is indefinite. 
	\end{enumerate}	
\end{thm}
\begin{rmk}
	\[\vecx^\T\vecx=\mqty[x_1&\cdots&x_n]\mqty[x_1\\\vdots\\x_n]=x_1^2+\cdots+x_n^2\]	
\end{rmk}
\begin{rmk}
	Let $\matrixD=\mqty[\lambda_1&0&\cdots&0\\0&\ddots&&\vdots\\\vdots&&\ddots&\vdots\\0&\cdots&\cdots&\lambda_n],$ then we have \[\vecx^\T\matrixD\vecx=\mqty[x_1&\cdots&x_n]\mqty[\lambda_1x_1\\\vdots\\\lambda_nx_n]=\lambda_1x_1^2+\cdots+\lambda_nx_n^2.\]	
\end{rmk}
\begin{thm}
	Let $\matrixA$ be an $n\times n$ matrix, then $f(\vecx)=\vecx^\T\matrixA\vecx$ is a quadratic form. In general, \[\mqty[x_1&\cdots&x_n]\mqty[a_{11}&\cdots&a_{1n}\\\vdots&\ddots&\vdots\\a_{n1}&\cdots&a_{nn}]\mqty[x_1\\\vdots\\x_n]=\sum_{1\leq i, j\leq n}a_{ij}x_ix_j.\]	
\end{thm}
\begin{thm}
	Let $f(x_1,\cdots,x_n)=\dsst\sum_{i\leq j}c_{ij}x_ix_j,$ then there is a unique symmetric matrix such that $f=\vecx^\T\matrixA\vecx,$ where the $ij-$th entry of $\matrixA=\begin{cases}c_{ii},\quad& i=j\\c_{ij}\quad& i\neq j\end{cases}$
	\begin{eg}
		$7x^2+11xy+y^2$ can be written as $\mqty[7&11/2\\11/2&1].$
	\end{eg}	
\end{thm}
\begin{thm}[Principal Axes Theorem]
	Let $f:\R^n\to\R$ be a quadratic form, then there exists an orthogonal matrix $\matrixS$ and a diagonal quadratic form $d:R^n\to\R$ such that \[f(\vecx)=d\circ\matrixS^\T\vecx=d\qty(\matrixS^\T\vecx).\] Any quadratic form looks diagonal in some coordinate $\lambda_1x_1^2$ system. 
\end{thm}
\begin{prf}
	$f(\vecx)=\vecx^\T\matrixA\vecx,$ where $\matrixA$ is symmetric.\par Note that $\matrixA=\matrixS\matrixD\matrixS^{-1},$ where $\matrixS$ is orthogonal and $\matrixD$ is diagonal. \par So, $f(\vecx)=\vecx^\T\qty(\matrixS\matrixD\matrixS^{-1})\vecx=\qty(\matrixS^\T\vecx)^\T\matrixD\qty(\matrixS^\T\vecx).$\par Since $d(\vecx)=\vecx^\T\matrixD\vecx,$ we know $f(\vecx)=d\qty(\matrixS^\T)\vecx$.
\end{prf}
\begin{cor}
	$f$ is positive definite if $\lambda_i>0$ and $f$ is negative definite if $\lambda_i<0.$
\end{cor}

\subsection{Singular Value Decomposition}
\begin{df}[Sigular Value Decomposition]
	The \textbf{singular value decomposition (SVD)} is a recipe to write a general matrix $\matrixA$ as a product of matrices which is easy to understand geometrically. 
\end{df}
\begin{thm}
	Let $\matrixA$ to be an $n\times m$ matrix, then $\matrixA=\matrixU\matrixSig\matrixV^\T,$ where $\matrixV$ is an orthogonal matrix (so $\matrixV^\T=\matrixV^{-1}$), $\matrixSig$ is an $n\times m$ matrix, whose $ij-$elements are all zero, and whose $ii$-entries	satisfy $a_{11}\geq a_{22}\geq a_{33}\geq\cdots\geq0,$ and $\matrixU$ is an orthogonal matrix. 
\end{thm}
\begin{eg}
	Suppose $\matrixA=\mqty[|&|\\\vecu_1&\vecu_2\\|&|]\mqty[\sigma_1&0\\0&\sigma_2]\mqty[-&\vecv_1&-\\-&\vecv_2&-]:$
	\begin{center}
	\tikzset{every picture/.style={line width=0.75pt}}
	\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
	\draw  (57.73,119.84) -- (220.1,119.84)(141.14,35.56) -- (141.14,199.26) (213.1,114.84) -- (220.1,119.84) -- (213.1,124.84) (136.14,42.56) -- (141.14,35.56) -- (146.14,42.56)  ;
	\draw    (140.69,120.28) -- (92.23,101.01) ;
	\draw [shift={(90.37,100.27)}, rotate = 21.68] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (140.69,120.28) -- (159.64,72.13) ;
	\draw [shift={(160.37,70.27)}, rotate = 111.47] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw   (136.82,111.31) -- (143.24,113.86) -- (140.69,120.28) -- (134.27,117.73) -- cycle ;
	\draw    (219.74,241.72) -- (219.86,189.56) ;
	\draw [shift={(219.86,187.56)}, rotate = 90.12] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (219.74,241.72) -- (271.49,241.64) ;
	\draw [shift={(273.49,241.64)}, rotate = 179.91] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw   (226.67,234.82) -- (226.65,241.73) -- (219.74,241.72) -- (219.76,234.81) -- cycle ;
	\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.31 ] (87.85,119.84) .. controls (87.85,90.41) and (111.71,66.55) .. (141.14,66.55) .. controls (170.57,66.55) and (194.43,90.41) .. (194.43,119.84) .. controls (194.43,149.27) and (170.57,173.13) .. (141.14,173.13) .. controls (111.71,173.13) and (87.85,149.27) .. (87.85,119.84) -- cycle ;
	\draw  (136.34,241.72) -- (298.71,241.72)(219.74,157.45) -- (219.74,321.15) (291.71,236.72) -- (298.71,241.72) -- (291.71,246.72) (214.74,164.45) -- (219.74,157.45) -- (224.74,164.45)  ;
	\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.31 ] (166.45,241.72) .. controls (166.45,212.29) and (190.31,188.43) .. (219.74,188.43) .. controls (249.18,188.43) and (273.04,212.29) .. (273.04,241.72) .. controls (273.04,271.15) and (249.18,295.01) .. (219.74,295.01) .. controls (190.31,295.01) and (166.45,271.15) .. (166.45,241.72) -- cycle ;
	\draw    (410.74,241.39) -- (410.74,210.73) ;
	\draw [shift={(410.74,208.73)}, rotate = 90] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (410.74,241.39) -- (473.37,241.39) ;
	\draw [shift={(475.37,241.39)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw   (417.67,234.49) -- (417.65,241.4) -- (410.74,241.39) -- (410.76,234.48) -- cycle ;
	\draw  (327.34,241.39) -- (489.71,241.39)(410.74,157.12) -- (410.74,320.82) (482.71,236.39) -- (489.71,241.39) -- (482.71,246.39) (405.74,164.12) -- (410.74,157.12) -- (415.74,164.12)  ;
	\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.22 ] (347.24,241.39) .. controls (347.24,223.72) and (375.67,209.39) .. (410.74,209.39) .. controls (445.81,209.39) and (474.24,223.72) .. (474.24,241.39) .. controls (474.24,259.07) and (445.81,273.39) .. (410.74,273.39) .. controls (375.67,273.39) and (347.24,259.07) .. (347.24,241.39) -- cycle ;
	\draw    (533.09,118.36) -- (520.29,146.22) ;
	\draw [shift={(519.45,148.04)}, rotate = 294.68] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (533.09,118.36) -- (476.19,92.21) ;
	\draw [shift={(474.37,91.37)}, rotate = 24.68] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw   (523.92,121.74) -- (526.82,115.46) -- (533.09,118.36) -- (530.2,124.63) -- cycle ;
	\draw  (449.41,118.96) -- (611.78,118.96)(532.82,34.69) -- (532.82,198.39) (604.78,113.96) -- (611.78,118.96) -- (604.78,123.96) (527.82,41.69) -- (532.82,34.69) -- (537.82,41.69)  ;
	\draw  [fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.22 ] (590.52,145.48) .. controls (583.14,161.54) and (551.32,162.68) .. (519.45,148.04) .. controls (487.59,133.4) and (467.74,108.51) .. (475.12,92.45) .. controls (482.5,76.39) and (514.31,75.24) .. (546.18,89.89) .. controls (578.04,104.53) and (597.9,129.42) .. (590.52,145.48) -- cycle ;
	\draw    (182.37,65.27) .. controls (231.12,14.53) and (409.57,-1.57) .. (488.19,64.27) ;
	\draw [shift={(489.37,65.27)}, rotate = 220.66] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (90,181) .. controls (78.48,196.12) and (104.83,265.86) .. (150.96,274.04) ;
	\draw [shift={(152.37,274.27)}, rotate = 188.47] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (274,289) .. controls (313.76,326.7) and (349.29,321.44) .. (372.93,290.43) ;
	\draw [shift={(374,289)}, rotate = 126.21] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw    (477,289) .. controls (536.76,300.79) and (596.17,232.3) .. (581.83,184.36) ;
	\draw [shift={(581.37,182.91)}, rotate = 71.57] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	\draw (161.37,73.67) node [anchor=north west][inner sep=0.75pt]    {$\overrightarrow{v_{1}}$};
	\draw (68.37,82.67) node [anchor=north west][inner sep=0.75pt]    {$\overrightarrow{v_{2}}$};
	\draw (262.37,244.67) node [anchor=north west][inner sep=0.75pt]    {$\overrightarrow{e_{1}}$};
	\draw (223.37,176.67) node [anchor=north west][inner sep=0.75pt]    {$\overrightarrow{e_{2}}$};
	\draw (453.37,244.34) node [anchor=north west][inner sep=0.75pt]    {$\sigma _{1}\overrightarrow{e_{1}}$};
	\draw (414.37,176.34) node [anchor=north west][inner sep=0.75pt]    {$\sigma _{2}\overrightarrow{e_{2}}$};
	\draw (386.37,62.34) node [anchor=north west][inner sep=0.75pt]    {$\mathbf{A}\overrightarrow{v_{1}} =\sigma _{1}\overrightarrow{u_{1}}$};
	\draw (492.37,150.34) node [anchor=north west][inner sep=0.75pt]    {$\mathbf{A}\overrightarrow{v_{2}} =\sigma _{2}\overrightarrow{u_{2}}$};
	\draw (317,24.4) node [anchor=north west][inner sep=0.75pt]    {$\mathbf{A}$};
	\draw (73,232.4) node [anchor=north west][inner sep=0.75pt]    {$\mathbf{V}^{\mathbf{T}}$};
	\draw (318,325.4) node [anchor=north west][inner sep=0.75pt]    {$\mathbf{\Sigma }$};
	\draw (571,261.4) node [anchor=north west][inner sep=0.75pt]    {$\mathbf{U}$};
	\draw (342,188) node [anchor=north west][inner sep=0.75pt]   [align=left] {ellipse};
	\end{tikzpicture}
	\end{center}
	\begin{enumerate}
		\item $\sigma\vecu_1$ is a longest vector on the ellipse (image of the unit circle under $\matrixA$).
		\begin{enumerate}
			\item $\sigma_1$ is its length
			\item $\vecu_1$ is a unit vector pointing in direction.
			\item $\vecv_1$ is a vector on unit circle such $\|\matrixA\vecv_1\|$ is maximized. 
		\end{enumerate}
		\item $\matrixA\vecv_2=\sigma_2\vecu_2$ is a shortest vector on the ellipse (image of unit circle under $\matrixA$).
		\begin{enumerate}
			\item $\sigma_2$ is its length
			\item $\vecu_2$ is a vector on unit circle such that $\|\matrixA\vecv_2\|$ is minimized. 
		\end{enumerate}
	\end{enumerate}
\end{eg}
\begin{rmk}
	SVD of $\matrixA$ encodes information about lengths change under $\matrixA.$ Let $\vecx\in\R^m$ and consider $\|\matrixA\vecx\|=\sqrt{\matrixA\vecx\cdot\matrixA\vecx},$ then \[\begin{aligned}\matrixA\vecx\cdot\matrixA\vecx=\vecx\cdot\matrixA^\T\matrixA\vecx&=\qty(\sum c_i\vecv_i)\cdot\qty(\matrixA^\T\matrixA)\qty(\sum c_i\vecv_i)=\qty(\sum_{i=1}^mc_ik\vecv_i)\cdot\qty(\sum_{j=1}^mc_j\lambda_j\vecv_j)\\&=\sum_{i,j}c_ic_j\qty(\vecv_i\cdot\vecv_j)\lambda_j=\sum c_i^2\lambda_i.\end{aligned}\]\par So, $\|\matrixA\vecx\|=\sqrt{\sum c_i^2\lambda_i}$ and $\|\vecx\|=\sqrt{\sum c_i^2}$. Therefore, $\boxed{\|\matrixA\vecv_i\|=\sqrt{\lambda_i}=\sigma_i}$\par Since $\matrixV$ is orthogonal, $\matrixV^\T\vecx=\mqty[c_1\\\vdots\\c_m].$ Since $\matrixSig=\mqty[\sqrt{\lambda_1}&&\\&\ddots&\\&&\sqrt{\lambda_m}],$ then \[\matrixSig\matrixV^\T\vecx=\mqty[\sqrt{\lambda_1}c_1\\\vdots\\\sqrt{\lambda_m}c_m].\]
\end{rmk}
\begin{df}[Singular Value]
	Let $\matrixA$ be a matrix, the \textbf{singular value} of $\matrixA$ are the square roots of the positive eigenvalues of $\matrixA^\T\matrixA.$ i.e., $\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_m$ are eigenvalues of $\matrixA^\T\matrixA,$ and $\sigma_i=\sqrt{\lambda_i}$ whenever $\lambda_i>0.$
\end{df}
\begin{rmk}
	To find $\matrixU,$ we can consider the following: \[\vecu_i=\mqty[|&|&&|&|&&|\\\dfrac{\matrixA\vecv_1}{\sigma_1}&\dfrac{\matrixA\vecv_2}{\sigma_2}&\cdots&\dfrac{\matrixA\vecv_r}{\sigma_r}&\vecx_1&\cdots&\vecx_k],\]	where $\vecx_1,\cdots,\vecx_k$ are orthogonal and span orthogonal component to the space spanned by first $r$ columns.\par Also, note that $\dsst\dfrac{\matrixA\vecv_1}{\sigma_1},\dfrac{\matrixA\vecv_2}{\sigma_2},\cdots,\dfrac{\matrixA\vecv_r}{\sigma_r}$ are image of $\matrixA$ with \[\dfrac{\matrixA\vecv_i}{\sigma_i}\cdot\dfrac{\matrixA\vecv_j}{\sigma_j}=\dfrac{\vecv_i\cdot\matrixA^\T\matrixA\vecv_j}{\sigma_i\sigma_j}=\dfrac{\vecv_i\cdot\lambda_j\vecv_i}{\sigma_i\sigma_j}=0.\]
\end{rmk}
\begin{framed}
\textbf{Procedure to find the SVD for an $n\times m$ matrix $\matrixA$.}
\begin{enumerate}
	\item Compute $\matrixA^\T\matrixA$ and find orthogonal eigenbasis $\vecv_1,\cdots,\vecv_m$ such that the eigenvalues satisfy $\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_m$. \[\matrixV=\mqty[|&&|\\\vecv_1&\cdots&\vecv_m\\|&&|];\qquad\matrixSig=\mqty[\sqrt{\lambda_1}&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\sqrt{\lambda_m}].\]
	\item Define \[\matrixU=\left[\begin{array}{cc}\mqty{\dfrac{\matrixA\vecv_1}{\sigma_1}&\dfrac{\matrixA\vecv_2}{\sigma_2}&\cdots&\dfrac{\matrixA\vecv_r}{\sigma_r}}&\mqty{\vecx_1&\cdots&\vecx_k}\end{array}\right],\] where $\vecx_1,\cdots,\vecx_k$ are choices of orthogonal basis of $\Im(\matrixA)^\perp.$
	\item $\matrixA=\matrixU\matrixSig\matrixV^\T.$
\end{enumerate}	
\end{framed}
\begin{eg}
	Compute the SVD of $\matrixA=\mqty[0&1\\1&1\\1&0].$	
\end{eg}
\begin{sol}
\begin{enumerate}
	\item Compute $\matrixA^\T\matrixA,$ and find orthogonal eigenbasis: \[\matrixA^\T\matrixA=\mqty[0&1&1\\1&1&0]\mqty[0&1\\1&1\\1&0]=\mqty[2&1\\1&2]\implies f_{\matrixA}(t)=(2-t)^2-1=(t-1)(t-3).\]\par Therefore, $\lambda_1=3,\quad\sigma_1=\sqrt{3},\quad\lambda_2=1,\quad\sigma_2=\sqrt{1}=1.$ \[E_{\matrixA^\T\matrixA,3}=\Span\qty(\mqty[1\\1])\implies\vecv_1=\mqty[1/\sqrt{2}\\1/\sqrt{2}]\] \[E_{\matrixA^\T\matrixA,1}=\Span\qty(\mqty[1\\-1])\implies\vecv_2=\mqty[1/\sqrt{2}\\-1/\sqrt{2}]\] \[\therefore\matrixV^\T=\dfrac{1}{\sqrt{2}}\mqty[1&1\\1&-1];\qquad\matrixSig=\mqty[3&0\\0&1\\0&0].\]
	\item Find $\matrixU=\mqty[\dfrac{\matrixA\vecv_1}{\sigma_1}&\dfrac{\matrixA\vecv_2}{\sigma_2}&\vecx_1]$\[\dfrac{\matrixA\vecv_1}{\sigma_1}=\dfrac{1}{\sqrt{3}}=\mqty[0&1\\1&1\\1&0]\mqty[1/\sqrt{2}&1/\sqrt{2}]=\dfrac{1}{6}\mqty[1\\2\\1]\]\[\dfrac{\matrixA\vecv_2}{\sigma_2}=\mqty[0&1\\1&1\\1&0]\mqty[1/\sqrt{2}&-1/\sqrt{2}]=\dfrac{1}{2}\mqty[-1\\0\\1]\] \[\ker(\matrixA^\T)=\ker\qty(\mqty[0&1&1\\1&1&0])=\Span\qty(\mqty[1\\-1\\1])\implies\vecx_1=\dfrac{1}{\sqrt{3}}\mqty[1\\-1\\1]\]\[\therefore\matrixU=\mqty[1/\sqrt{6}&-1/\sqrt{2}&1/\sqrt{3}\\2/\sqrt{6}&0&-1\sqrt{3}\\1/\sqrt{6}&1/\sqrt{2}&1/\sqrt{3}]\]
	\item $\matrixA=\matrixU\matrixSig\matrixV^\T$\[\matrixA=\mqty[1/\sqrt{6}&-1/\sqrt{2}&1/\sqrt{3}\\2/\sqrt{6}&0&-1\sqrt{3}\\1/\sqrt{6}&1/\sqrt{2}&1/\sqrt{3}]\mqty[3&0\\0&1\\0&0]\mqty[1&1\\1&-1]\dfrac{1}{\sqrt{2}}\]
\end{enumerate}	
\end{sol}


\end{document}
\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{ntheorem}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{amsmath}
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}
\usepackage[most]{tcolorbox}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{float} 
\usepackage{subfigure} 
\usepackage{arydshln}
\usepackage{multicol}
\usepackage{url}
\usepackage{setspace}
\usepackage[T1]{fontenc}
\usepackage{mathptmx}
\usepackage{framed}
\usepackage{xcolor}
\usepackage{listings}

\geometry{a4paper, left=2cm, right=2cm, bottom=2cm, top=2cm}

\definecolor{darkblue}{HTML}{003472}
\definecolor{lightblue}{HTML}{70f3ff}
\definecolor{red}{HTML}{dc3023}
\definecolor{lightred}{HTML}{f47983}
\definecolor{green}{HTML}{057748}
\definecolor{lightgreen}{HTML}{a4e2c6}
\definecolor{gray}{HTML}{758a99}
\definecolor{lightgray}{HTML}{41555d}
\definecolor{lightblack}{HTML}{424c50}
\definecolor{orange}{HTML}{9b4400}
\definecolor{yellow}{HTML}{ffa631}
\definecolor{white}{HTML}{f0fcff}

\tcbuselibrary{skins, breakable, theorems}
\newtcolorbox{defbox}[2][]{
	colback=lightgray!5, colframe=lightblack,colbacktitle=lightblack, coltitle=white,title={#2},fonttitle=\bfseries,breakable
}

\newtcolorbox{thmbox}[2][]{
	colback=lightblue!5, colframe=darkblue,colbacktitle=darkblue, coltitle=white,title={#2},fonttitle=\bfseries,breakable
}

\newtcolorbox{egbox}[2][]{
	colback=lightgreen!5, colframe=green,colbacktitle=green, coltitle=white,title={#2},fonttitle=\bfseries,breakable
}

\newtcolorbox{prfbox}[2][]{
	colback=orange!5, colframe=orange,colbacktitle=orange, coltitle=white,title={#2},fonttitle=\bfseries,breakable
}

\newtcolorbox{extbox}[2][]{
	colback=yellow!5, colframe=yellow,colbacktitle=yellow, coltitle=white,title={#2},fonttitle=\bfseries,breakable
}

\newtcolorbox{rmkbox}[2][]{
	colback=lightred!5, colframe=red,colbacktitle=red, coltitle=white,title={#2},fonttitle=\bfseries,breakable
}


\lstset{language=Python}
\lstset{
	numbers=left,
	numberstyle=\tiny,
	keywordstyle=\color{green},
	commentstyle=\color{lightgray},
	frame=shadownbox,
	rulesepcolor=\color{lightblack},
	escapeinside='',
	xleftmargin=2em,aboveskip=1em,
	framexleftmargin=2em
	}

\title{Supervised Machine Learning}
\author{Jiuru Lyu}
\date{ }

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Linear Regression Model}
Basic notations: 
\begin{itemize}
	\item $x$ is the input variable, or \textbf{feature};
	\item $y$ is the output variable, or \textbf{target};
	\item $m$ is the number of training examples;
	\item $(x,y)$ is a single training example;
	\item $(x^{(i)},y^{(i)})$ is the i-th training example;
	\begin{itemize}
  		\item Note: the “i” here is an index, rather than exponent.
	\end{itemize}
	\item $\hat{y}$ is the prediction, or the estimated $y$;
	\item $f$ is the model, or hypothesis.
	\end{itemize}
	
\subsection{Representing the Model}
\begin{thmbox}{Univariate Linear Regression}
	$$f_{w,b}(x)=wx+b,$$ where $w$ and $b$ are parameters/coefficients/weights
\end{thmbox}

\subsection{Cost Function}
\begin{thmbox}{The Error}
$$\hat{y}^{(i)}=f_{w,b}\left(x^{(i)}\right)$$
$$f_{w,b}\left(x^{(i)}\right)=wx^{(i)}+b$$
$$\text{Error}=\hat{y}^{(i)}-y^{(i)}$$
\end{thmbox}
\begin{thmbox}{The Squared Error Cost Function ($J$)}
	$$\begin{aligned}
		J(w,b)&=\frac{1}{2m}\sum_{i=1}^m\left(\hat{y}^{(i)}-y^{(i)}\right)^2\\
		&=\frac{1}{2m}\sum_{i=1}^m\left(f_{w,b}\left(x^{(i)}\right)-y^{(i)}\right)^2
	\end{aligned}$$
\end{thmbox}
The job is the find $w$ and $b$ such that $\hat{y}^{(i)}$ is close to $y^{(i)}$ for all $(x^{(i)},y^{(i)})$, i.e., $$\text{minimize}_{w,b}J(w,b)$$

\subsection{Visualizing the Cost Function}
\quad In linear regression, the cost function is always a bowl-shaped function. At the bottom of the “bowl”, the cost function is minimized. 

\subsection{Gradient Descent}
\begin{rmkbox}{Outlined Procedure of Gradient Descent}
	\begin{itemize}
		\item Start with some $w$ and $b$
		\item Keep changing $w$ and $b$ to reduce $J(w,b)$
		\item Until we settle at or near a minimum.
	\end{itemize}
\end{rmkbox}
*Refer to the concept of Gradient $\nabla f$.

\subsubsection{Formula}
\begin{rmkbox}{Gradient Descent Algorithm}
	- Repeat until convergence: 
	$$w=w-\alpha\frac{\partial}{\partial w}J(w,b)$$
	$$b=b-\alpha\frac{\partial}{\partial b}J(w,b)$$
	where $\alpha$ is the learning rate. \\
	Key: Simultaneously update $w$ and $b$.
\end{rmkbox}

\subsubsection{The Learning Rate}
\quad The learning rate cannot be too big or too small.

\subsubsection{More on the Formula}
\quad Recall:
\begin{enumerate}
	\item $f_{w,b}(x)=wx+b$
	\item $J(w,b)=\frac{1}{2m}\sum_{i=1}^m\left(f_{w,b}\left(x^{(i)}\right)-y^{(i)}\right)^2$.
\end{enumerate}

Hence, 
	$$\begin{aligned}
		\frac{\partial}{\partial w}J(w,b)&=\frac{\partial}{\partial w}\frac{1}{2m}\sum_{i=1}^m\left(f_{w,b}(x^{(i)})-y^{(i)}\right)^2\\
		&=\frac{\partial}{\partial w}\frac{1}{2m}\sum_{i=1}^m\left(wx^{(i)}+b-y^{(i)}\right)^2\\
		&=\frac{1}{2m}\sum_{i=1}^m\left(wx^{(i)}+b-y^{(i)}\right)2x^{(i)}\\
		&=\frac{1}{m}\sum_{i=1}^m\left(f_{w,b}(x^{(i)})-y^{(i)}\right)x^{(i)}
	\end{aligned}$$
	$$\begin{aligned}
		\frac{\partial}{\partial b}J(w,b)&=\frac{\partial}{\partial b}\frac{1}{2m}\sum_{i=1}^m\left(f_{w,b}(x^{(i)})-y^{(i)}\right)^2\\
		&=\frac{\partial}{\partial b}\frac{1}{2m}\sum_{i=1}^m\left(wx^{(i)}+b-y^{(i)}\right)^2\\
		&=\frac{1}{2m}\sum_{i=1}^m\left(wx^{(i)}+b-y^{(i)}\right)2\\
		&=\frac{1}{m}\sum_{i=1}^m\left(f_{w,b}(x^{(i)})-y^{(i)}\right)
		\end{aligned}$$

Therefore, the gradient descent algorithm becomes: 
\begin{rmkbox}{Gradient Descent Algorithm}
	Repeat until convergence: 
	$$w=w-\alpha\frac{1}{m}\sum_{i=1}^m\left(f_{w,b}(x^{(i)})-y^{(i)}\right)x^{(i)}$$
	$$b=b-\alpha\frac{1}{m}\sum_{i=1}^m\left(f_{w,b}(x^{(i)})-y^{(i)}\right)$$
	Update $w$ and $b$ simultaneously.
\end{rmkbox}

\subsubsection{Gradient Descent in Action}
\quad Sometimes, we might encounter cost functions with more than one local minima, indicated by the diagram above. However, in linear regression, we always have a cost function with one and only one local minimum, and that minima is called the global minimum. 

\section{Linear Model with Multiple Features}
\subsection{Notation}
\begin{itemize}
	\item $x_j$ is the j-th feature
	\item $n$ is the number of features
	\item $\vec{x}^{(i)}$ is a vector representing features of the i-th training example
	\item $x_j^{(i)}$ is the value of feature j in the i-th training example
\end{itemize}
\subsection{Model}
\begin{thmbox}{Linear Model with Multiple Features}
	$$f_{\vec{w},b}\left(\vec{x}\right)=w_1x_1+w_2x_2+\cdots+w_nx_n+b$$
\end{thmbox}
If we let $\vec{x}=\begin{bmatrix}x_1&x_2&\cdots&x_n\end{bmatrix}$ and $\vec{w}=\begin{bmatrix}w_1&w_2&\cdots&w_n\end{bmatrix}$ be two column vectors, the model can be written using the \textbf{dot product}, as shown below: 
\begin{thmbox}{Linear Model with Multiple Features using Dot Product}
	$$f_{\vec{w},b}\left(\vec{x}\right)=\vec{w}\cdot\vec{x}+b$$
	This is called the multiple linear regression. (We do not call it multivariate regression. )	
\end{thmbox}

\subsection{Vectorization}
\quad By using NumPy, a package in Python, we can easily write parameters and features in an array. 
\begin{lstlisting}
	w = np.array([1.0,2.5,-3.3])
	b=4
	x = np.array([10,20,30])
\end{lstlisting}

There are multiple ways to write the multiple linear regression algorithm in Python, but the most efficient way is to use vectorization. 

\subsubsection{The least effective way}
\begin{lstlisting}
	f = w[0] * x[0] + 
		w[1] * x[1] + 
		w[2] * x[2] + b
\end{lstlisting}
Note: the code count from `0`, so the first index should be `0` instead of `1`. 

This way is the least effective way because when we have a large amount of features, we need to write very long and complicated codes to conduct the regression.

\subsubsection{A more effective way}
\begin{lstlisting}
	f = 0
	for j in range(0,n): 
		f = f + w[j] * x[j]
	f = f + b
\end{lstlisting}
Note: `range(0,n)` can also be written as `range(n)`, which indicates a range from `0` to `n`, including `0` but excluding `n`. 

In this way, we use the for loop to conduct the regression. This will be faster than the previous way, but less efficient than the vectorization method because when we have a large amount of features, the computer will go through this loop for many times, which is very time-consuming. 

\subsubsection{The most effective way - Vectorization}
\begin{lstlisting}
	f = np.dot(w,x) + b
\end{lstlisting}

In this way, the NumPy directly computes the dot product between the two arrays, w and x. Because it computes the dot product by first multiplying each elements in the array and then adding all the product results together, this method will save time when we are dealing with very large datasets. 

\subsection{Gradient Descent for Multiple Regression}
\begin{thmbox}{Cost Function for Multiple Linear Regression}
	$$J(\vec{w},b)=\frac{1}{2m}\sum_{i=1}^{m}\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right)$$
\end{thmbox}
\begin{thmbox}{The Gradient Descent}
	$$w_j=w_j-\alpha\frac{\partial}{\partial w_j}J(\vec{w},b)$$
	$$b=b-\alpha\frac{\partial}{\partial b}J(\vec{w},b)$$
\end{thmbox}
After computing the partial derivative, it becomes
\begin{rmkbox}{Gradient Descent Algorithm}
	Repeat until convergence: 
	$$w_1=w_1-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right)x_1^{(i)}$$
	$$\vdots$$
	$$w_n=w_n-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right)x_n^{(i)}$$
	$$b=b-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(f_{\vec{w},b}(\vec{x}^{(i)}-y^{(i)}\right)$$
	Simultaneously update $w_j$ (for $j=1,\cdots,n$) and $b$.
\end{rmkbox}

\section{Practical Tips for Linear Regression}
\subsection{Feature Scaling}
\quad The size of the feature and parameter will influence the effectiveness of gradient descent. When the range of the feature or the parameter is too large or too small, we should consider to rescale the range so that we can fit them in acceptable ranges. We have several ways to do the feature scaling, including mean normalization and Z-score normalization. 
\subsubsection{Mean Normalization}
\begin{thmbox}{Mean Normalizaiton}
	$$x_n=\frac{x_n-\mu_n}{\text{max }x_n-\text{min }x_n},$$
	where $\mu_n$ is the mean of $x_n$.
\end{thmbox}
\subsubsection{Z-Score Normalization}
\begin{thmbox}{Z-Score Normalization}
	$$x_n=\frac{x_n-\mu_n}{\sigma_n},$$
	where $\mu_n$ is the mean of $x_n$, and $\sigma_n$ is the standard deviation of $x_n$.
\end{thmbox}

\subsection{Checking Gradient Descent for Convergence}
\quad We have to ways to make sure the gradient descent is working correctly. 
\subsubsection{Drawing the diagram of the cost function}
\quad The value of our cost function should decrease after ever iteration. We can simply plot the cost function versus number of iterations. If the cost function does decrease as number of iterations increasing, our gradient descent is working correctly. 
\subsubsection{The Automatics Convergence Test}
\quad We can also use the automatic convergence test to tell if our gradient descent is working properly. 

Firstly, we need to select an epsilon $\varepsilon$, which is a very small number. For example, we can set $\varepsilon=10^{-3}$.

If our cost function $J(\vec{w},b)$ decreases by  $\leq\varepsilon$ in one iteration, we declare convergence. 

\subsection{Choosing the Learning Rate}
\quad When we choose the learning rate, we could choose the values from the following array: 0.001, 0.003, 0.01, 0.03, 0.1, 1,… Basically, the next try is three times larger than the previous try. 

\subsection{Feature Engineering}
\quad \textbf{Feature engineering}: Using intuition to design new features by transforming or combining original features.
\begin{egbox}{Example}
	\quad We have our original model: 
	$$f_{\vec{w},b}(\vec{x})=w_1x_1+w_2x_2+b,$$
	where $x_1$ represents frontage, and $x_2$ represents depth. 
	
	\quad Now, we can use feature engineering to design a new feature, called area, because area equals to the product between frontage and depth. 
	$$x_3(\text{area})=x_1x_2$$
	
	\quad Then, our model is turned into $$f_{\vec{w},b}(\vec{x})=w_1x_1+w_2x_2+w_3x_3+b.$$
\end{egbox}


\subsection{Polynomial Regression}
\quad We can also have a model with the feature in its different orders. 
\begin{egbox}{Examples}
	$$f_{\vec{w},b}(x)=w_1x+w_2x^2+w_3x^3+b;$$
	$$f_{\vec{w},b}(x)=w_1x+w_2\sqrt{x}+b$$
\end{egbox}


\section{Classification - Logistic Regression}
\subsection{Motivations}
\quad \textbf{Binary classification}: $y$ can only be one of two values: false (no) or true (yes). 

Normally, we record false as 0 and true as 1. 0 is also called the negative class, representing absence; 1 is also called the positive class, indicating presence.
 
\subsection{Logistic Regression}
\quad Sigmoid function, or the logistic function, only gives outputs between 0 and 1: 
\begin{thmbox}{Sigmoid Function}
	$$g(z)=\frac{1}{1+e^{-z}},\text{ where }0<g(z)<1.$$
\end{thmbox}

Recall our linear regression function. We can set a function $z$, which is exactly the linear regression model. 
\begin{thmbox}{Logistic Model}
	$$z=\vec{w}\cdot\vec{x}+b$$
	Substituting $z$ into our sigmoid function $g(z)$: 
	$$\begin{aligned}
		f_{\vec{w},b}(\vec{x})=g(z)&=g(\vec{w}\cdot\vec{x}+b)\\
		&=\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}
		\end{aligned}.$$
	This is called the \textbf{logistic regression}. 
\end{thmbox}

The logistic regression gives the probability that the class is `1`.  So, we also denote it as 
\begin{thmbox}{Logistic Regression}
	$$f_{\vec{w},b}(\vec{x})=P(y=1|\vec{x};\vec{w},b)$$
	$P(y=1|\vec{x};\vec{w},b)$ represents the probability that $y$ is `1`, given input $\vec{x}$ and parameters $\vec{w}$ and $b$. 
\end{thmbox}
\begin{thmbox}{Property of Logistic Regreesion}
	$$P(y=0)+P(y=1)=1.$$
\end{thmbox}

\subsection{Decision Boundary}
\quad \textbf{Decision boundary} is a boundary that classify data into different classes. We can also regard them as the threshold of transferring from one category to another. 
\begin{egbox}{Example}
	Is $f_{\vec{w},b}(\vec{x})\geq0.5$? \\
	Yes: $\hat{y}=1$;\quad\quad\quad\quad No: $\hat{y}=0$. 
\end{egbox}
\begin{rmkbox}{Decision Boundary in Algorithm}
	When $g(z)\geq0.5$, $z\geq0$, $\vec{w}\cdot\vec{x}+b\geq0$, the regression returns $\hat{y}=1$.\\
	When $g(z)<0.5$, $z<0$, $\vec{w}\cdot\vec{x}+b<0$, the regression returns $\hat{y}=0$.
\end{rmkbox}

\subsection{Cost Function for Logistic Regression}
\begin{thmbox}{The Cost Function}
	For logistic regression, 
	$$f_{\vec{w},b}(\vec{x})=\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}$$
	The cost function is 
	$$J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^mL\left(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}\right),$$
	where $L\left(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}\right)$ is the loss function. 
\end{thmbox}

Hence, to define a cost function for logistic regression, we need to define the loss function first. 
\begin{thmbox}{The Loss Function by Definition}
$$L\left(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}\right)=\frac{1}{2}\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right)^2$$	
\end{thmbox}

\begin{thmbox}{Standard Representation of the Loss function}
	$$L\left(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}\right)=
	\begin{cases}
	-\log\left(f_{\vec{w},b}(\vec{x}^{(i)})\right)\text{ if }y^{(i)}=1;\\
	-\log\left(1-f_{\vec{w},b}(\vec{x}^{(i)})\right)\text{ if }y^{(i)}=0.
	\end{cases}$$
\end{thmbox}

The property of standard representation of the loss function: 
\begin{itemize}
	\item If $y^{(i)}=1$: 
		\begin{itemize}
			\item As $f_{\vec{w},b}(\vec{x}^{(i)})\to1$, then $\text{loss}\to0$; 
			\item As $f_{\vec{w},b}(\vec{x}^{(i)})\to0$, then $\text{loss}\to\infty$. 
		\end{itemize}
	\item If $y^{(i)}=0$: 
		\begin{itemize}
			\item As $f_{\vec{w},b}(\vec{x}^{(i)})\to1$, then $\text{loss}\to\infty$; 
			\item As $f_{\vec{w},b}(\vec{x}^{(i)})\to0$, then $\text{loss}\to0$. 
		\end{itemize}
\end{itemize}

We can write the loss function in one single function: 
\begin{thmbox}{Simplied Loss Function}
	$$L\left(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}\right)\\=-y^{(i)}\log\left(f_{\vec{w},b}(\vec{x}^{(i)})\right)-(1-y^{(i)})\log\left(1-f_{\vec{w},b}(\vec{x}^{(i)})\right)$$
\end{thmbox}

In this way, when $y^{(i)}=1$, 
$$\begin{aligned}
	L\left(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}\right)&\\
	&=-\log\left(f_{\vec{w},b}(\vec{x}^{(i)})\right)-(1-1)\log\left(1-f_{\vec{w},b}(\vec{x}^{(i)})\right)\\
	&=-\log\left(f_{\vec{w},b}(\vec{x}^{(i)})\right)
	\end{aligned}$$

When $y^{(i)}=0,$ 
$$\begin{aligned}
	L\left(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}\right)&\\
	&=-0\times\log\left(f_{\vec{w},b}(\vec{x}^{(i)})\right)-(1-0)\log\left(1-f_{\vec{w},b}(\vec{x}^{(i)})\right)\\
	&=-\log\left(1-f_{\vec{w},b}(\vec{x}^{(i)})\right)
	\end{aligned}$$

Hence, our cost function is written as
\begin{thmbox}{Cost Function}
	$$\begin{aligned}
		J(\vec{w},b)&=\frac{1}{m}\sum_{i=1}^m\left[L\left(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}\right)\right]\\
		&=\frac{1}{m}\sum_{i=1}^m\left[-y^{(i)}\log\left(f_{\vec{w},b}(\vec{x}^{(i)})\right)-(1-y^{(i)})\log\left(1-f_{\vec{w},b}(\vec{x}^{(i)})\right)\right]\\
		&=-\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}\log\left(f_{\vec{w},b}(\vec{x}^{(i)})\right)+(1-y^{(i)})\log\left(1-f_{\vec{w},b}(\vec{x}^{(i)})\right)\right]
		\end{aligned}$$
\end{thmbox}

\subsection{Gradient Descent for Logistic Regression}
\quad The gradient descent for logistic regression is similar to that of regression model, expect for different expressions for $f_{\vec{w},b}(\vec{x})$.
\begin{rmkbox}{Gradient Descent for Logistic Regression}
	Repeat until convergence: 
	$$w_j=w_j-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right)x_j^{(i)}$$
	$$b=b-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(f_{\vec{w},b}(\vec{x}^{(i)}-y^{(i)}\right)$$
	Simultaneously update $w_j$ (for $j=1,\cdots,n$) and $b$.
\end{rmkbox}

\section{Overfitting and Regularization}
\subsection{The Problem of Overfitting}
\begin{itemize}
	\item \textbf{Underfit}: The model does not fit the training set well, also known as high bias. 
	\item \textbf{Generalization}: The model fits the training set pretty well. 
	\item \textbf{Overfit}: The model fits the training set extremely well, also known as high variance. 
\end{itemize}

\subsection{Addressing Overfitting}
\begin{rmkbox}{Addressing Overfitting}
	\begin{enumerate}
		\item Collect more training examples
		\item Select features to include/exclude: Feature selection
		\item Regularization: Reduce the size of parameters $w_j$.
	\end{enumerate}	
\end{rmkbox}

\subsection{Regularization}
\quad To find small values of $w_j$, we introduce a regularization term to our cost function. 
\begin{thmbox}{Regularized Cost Function}
	$$J(\vec{w},b)=\frac{1}{2m}\sum_{i=1}^m\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right)^2+\frac{\lambda}{2m}\sum_{j=1}^nw_j^2+\frac{\lambda}{2m}\sum_{j=1}^nb^2,$$
	where $\lambda>0$ is called the regularization parameter. 
\end{thmbox}

Normally, we exclude the term relating to the parameter $b$ becuase we do not want be to be extremely small. Hence, the most frequently used regularized cost function is 
\begin{thmbox}{Regularized Cost Function}
	$$J(\vec{w},b)=\frac{1}{2m}\sum_{i=1}^m\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right)^2+\frac{\lambda}{2m}\sum_{j=1}^nw_j^2,$$
	where $$\frac{1}{2m}\sum_{i=1}^m(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2$$ is called the mean squared error, and $$\frac{\lambda}{2m}\sum_{j=1}^nw_j^2$$ is called the regularization term. 
\end{thmbox}

To select an appropriate $\lambda$, we find the following properties: 
\begin{itemize}
	\item When $\lambda$ is very big, to minimize $J(\vec{w},b)$, the result will yield very small $w_j$.
	\item When $\lambda$ is relatively small, to minimize $J(\vec{w},b)$, the result will yield larger $w_j$.
\end{itemize}

\subsection{Regularized Linear regression}
\quad Recall: The gradient descent algorithm for linear regression is
\begin{rmkbox}{The Gradient Descent Algorithm for Linear Regression}
	Repeat until convergence: 
	$$w_j=w_j-\alpha\frac{\partial}{\partial w_j}J(\vec{w},b)$$
	$$b=b-\alpha\frac{\partial}{\partial b}J(\vec{w},b)$$
	Simultaneously update $w_j$ (for $j=1,\cdots,n$) and $b$.
\end{rmkbox}

Substitute the regularized cost function, we get: 
\begin{rmkbox}{The Regularized Gradient Descent Algorithm for Linear Regression}
	Repeat until convergence: 
	$$w_j=w_j-\alpha\frac{\partial}{\partial w_j}\left[\frac{1}{2m}\sum_{i=1}^m\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right)^2+\frac{\lambda}{2m}\sum_{j=1}^nw_j^2\right]$$
	$$w_j=w_j-\alpha\frac{\partial}{\partial w_j}\left[\frac{1}{2m}\sum_{i=1}^m\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right)^2+\frac{\lambda}{2m}\sum_{j=1}^nw_j^2\right]$$
	Simultaneously update $w_j$ (for $j=1,\cdots,n$) and $b$.
\end{rmkbox}

Computing the partial derivatives, we get: 
$$\begin{aligned}
	\frac{\partial}{\partial w_j}J(\vec{w},b)&=\frac{\partial}{\partial w_j}\left[\frac{1}{2m}\sum_{i=1}^m\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right)^2+\frac{\lambda}{2m}\sum_{j=1}^nw_j^2\right]\\
	&=\frac{1}{2m}\sum_{i=1}^m\left[(\vec{w}\cdot\vec{x}^{(i)}+b-y^{(i)})2x_j^{(i)}\right]+\frac{\lambda}{2m}2w_j\\
	&=\frac{1}{m}\sum_{i=1}^m\left[(\vec{w}\cdot\vec{x}^{(i)}+b-y^{(i)})x_j^{(i)}\right]+\frac{\lambda}{m}w_j\\
	&=\frac{1}{m}\sum_{i=1}^m\left[(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}\right]+\frac{\lambda}{m}w_j
\end{aligned}$$
$$\begin{aligned}
	\frac{\partial}{\partial b}J(\vec{w},b)
	&=\frac{\partial}{\partial b}\left[\frac{1}{2m}\sum_{i=1}^m\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right)^2+\frac{\lambda}{2m}\sum_{j=1}^nw_j^2\right]\\
	&=\frac{1}{m}\sum_{i=1}^m\left(f_{\vec{w},b}(\vec{x}^{(i)}-y^{(i)}\right)
\end{aligned}$$

Substituting the partial derivatives to the gradient descent algorithm, we get: 
\begin{rmkbox}{The regularized gradient descent algorithm for linear regression}
	Repeat until convergence: 
	$$w_j=w_j-\alpha\left[\frac{1}{m}\sum_{i=1}^m\left[(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}\right]+\frac{\lambda}{m}w_j\right];$$
	$$b=b-\alpha\frac{1}{m}\sum_{i=1}^m\left(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}\right).$$
	Simultaneously update $w_j$ (for $j=1,\cdots,n$) and $b$.
\end{rmkbox}

\subsection{Regularized Logistic Regression}
\quad The regularized cost function for logistic regression is given by 
\begin{thmbox}{The Regularized Cost Function for Logistic Regression}
	$$J(\vec{w},b)=-\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}\log\left(f_{\vec{w},b}(\vec{x}^{(i)})\right)+(1-y^{(i)})\log\left(1-f_{\vec{w},b}(\vec{x}^{(i)})\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^nw_j^2$$
\end{thmbox}

The gradient descent algorithm for regularized logistic regression looks the same, except for $f_{\vec{w},b}(\vec{x}^{(i)})$ representing the logistic regression model. 
\end{document}
